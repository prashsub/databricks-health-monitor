---
description: Comprehensive guide for Databricks SQL Alerts V2 - config-driven alerting framework with SDK deployment
globs: **/*alert*.py,**/*alert*.yml
alwaysApply: false
---

# SQL Alerting: Config-Driven Framework for Databricks (V2 API)

## ðŸ“‹ Table of Contents

1. [Core Principles](#core-principles)
2. [âš ï¸ CRITICAL: V2 API Reference](#critical-v2-api-reference)
3. [Alert ID Convention](#alert-id-convention)
4. [Alert Rules Configuration Table](#alert-rules-configuration-table)
5. [SQL Query Patterns](#sql-query-patterns)
6. [Databricks SDK Integration (Recommended)](#databricks-sdk-integration-recommended)
7. [REST API Integration (Alternative)](#rest-api-integration-alternative)
8. [DAB Job Configuration](#dab-job-configuration)
9. [Complete Examples](#complete-examples)
10. [Troubleshooting](#troubleshooting)
11. [Unity Catalog Table Limitations](#unity-catalog-table-limitations)
12. [References](#references)

---

## Core Principles

### Principle 1: Config-Driven Alerting
Alert rules are stored in a Delta configuration table, not hardcoded. This enables:
- Runtime updates without code changes
- Centralized alert management
- Version history via Delta time travel
- Easy enable/disable without deployment

### Principle 2: Two-Job Pattern
**âš ï¸ CRITICAL:** Alerting uses a two-job separation of concerns:

1. **Setup Job** (`alert_rules_setup_job`): Creates/updates the `alert_rules` config table
2. **Deploy Job** (`alert_deploy_job`): Reads config table and creates/updates SQL Alerts via SDK

**Why This Matters:**
- Rules can be modified in Delta without redeploying alerts
- Dry-run capability for validation
- Clear separation between configuration and deployment

### Principle 3: Fully Qualified Table Names
**âš ï¸ CRITICAL:** Databricks SQL Alerts (Public Preview) do NOT support parameters in queries.

```sql
-- âŒ WRONG: Parameterized query (NOT SUPPORTED)
SELECT * FROM ${catalog}.${schema}.fact_booking_daily

-- âœ… CORRECT: Fully qualified table names embedded in query
SELECT * FROM wanderbricks_dev.gold.fact_booking_daily
```

**Pattern:** Use f-strings at rule creation time to embed catalog/schema:

```python
rev_001_query = f"""
SELECT ...
FROM {catalog}.{gold_schema}.fact_booking_daily
WHERE ...
"""
```

### Principle 4: Severity-Based Notification Routing
Alerts are categorized by severity with different notification strategies:

| Severity | Icon | Action Required | Notification Speed |
|----------|------|-----------------|-------------------|
| CRITICAL | ðŸ”´ | Immediate | Real-time (email + Slack) |
| WARNING | ðŸŸ¡ | Investigate soon | Batched (email) |
| INFO | ðŸŸ¢ | Informational | Daily digest |

---

## âš ï¸ CRITICAL: V2 API Reference

### API Endpoints

**âš ï¸ CRITICAL:** The correct V2 endpoint is `/api/2.0/alerts` (NOT `/api/2.0/sql/alerts` or `/api/2.0/sql/alerts-v2`)

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/api/2.0/alerts` | Create a new alert |
| `GET` | `/api/2.0/alerts` | List all alerts (paginated) |
| `GET` | `/api/2.0/alerts/{id}` | Get alert by ID |
| `PATCH` | `/api/2.0/alerts/{id}?update_mask=...` | Update alert (**requires update_mask**) |
| `DELETE` | `/api/2.0/alerts/{id}` | Trash alert (soft delete, 30-day recovery) |

**Reference:** [Databricks Alerts V2 API](https://docs.databricks.com/api/workspace/alertsv2/createalert)

### V2 API Payload Structure

```json
{
  "display_name": "[CRITICAL] Alert Name",
  "query_text": "SELECT column_name FROM catalog.schema.table WHERE condition",
  "warehouse_id": "warehouse-id-here",
  "schedule": {
    "quartz_cron_schedule": "0 0 8 * * ?",
    "timezone_id": "America/Los_Angeles",
    "pause_status": "UNPAUSED"
  },
  "evaluation": {
    "source": {
      "name": "column_name",
      "aggregation": "SUM"
    },
    "comparison_operator": "GREATER_THAN",
    "threshold": {
      "value": {
        "double_value": 100.0
      }
    },
    "empty_result_state": "OK",
    "notification": {
      "subscriptions": [
        {"user_email": "user@example.com"},
        {"destination_id": "uuid-here"}
      ],
      "notify_on_ok": false,
      "retrigger_seconds": 3600
    }
  },
  "custom_summary": "Custom email subject",
  "custom_description": "Custom notification body with {{column_name}} mustache templates"
}
```

### Comparison Operators (V2 API)

| Operator | API Value | Symbol |
|----------|-----------|--------|
| Greater than | `GREATER_THAN` | `>` |
| Greater or equal | `GREATER_THAN_OR_EQUAL` | `>=` |
| Less than | `LESS_THAN` | `<` |
| Less or equal | `LESS_THAN_OR_EQUAL` | `<=` |
| Equal | `EQUAL` | `=` |
| Not equal | `NOT_EQUAL` | `!=` |
| Is null | `IS_NULL` | - |
| Is not null | `IS_NOT_NULL` | - |

### Aggregation Types (V2 API)

| Aggregation | API Value | Notes |
|-------------|-----------|-------|
| Sum | `SUM` | Numeric columns |
| Count | `COUNT` | Any column |
| Count distinct | `COUNT_DISTINCT` | Any column |
| Average | `AVG` | Numeric columns |
| Median | `MEDIAN` | Numeric columns |
| Minimum | `MIN` | Numeric columns |
| Maximum | `MAX` | Numeric columns |
| Std deviation | `STDDEV` | Numeric columns |
| First value | - (null/omit) | **FIRST maps to null in API** |

### Threshold Value Types (V2 API)

```json
// For numeric comparisons
{"double_value": 100.0}

// For string comparisons
{"string_value": "expected_value"}

// For column-to-column comparisons (advanced)
{"column": {"name": "other_column"}}
```

### âš ï¸ Update Mask Required for PATCH

When updating an alert, you **MUST** specify which fields to update:

```bash
PATCH /api/2.0/alerts/{id}?update_mask=display_name,query_text,schedule,evaluation
```

Valid `update_mask` fields:
- `display_name`
- `query_text`
- `warehouse_id`
- `schedule`
- `evaluation`
- `custom_summary`
- `custom_description`

### Key Differences: V2 vs Legacy API

| Feature | Legacy API | V2 API |
|---------|------------|--------|
| **Endpoint** | `/api/2.0/sql/alerts` | `/api/2.0/alerts` |
| **Query** | References saved query via `query_id` | Embeds SQL directly via `query_text` |
| **Condition** | `options.column`, `options.op`, `options.value` | `evaluation.source.name`, `evaluation.comparison_operator`, `evaluation.threshold` |
| **Updates** | Full replacement | Partial updates with `update_mask` |
| **Delete** | Permanent | Soft delete (trash, 30-day recovery) |

---

## Alert ID Convention

### Format: `<DOMAIN>-<NUMBER>-<SEVERITY>`

**Components:**
- `DOMAIN`: Business domain (3-4 chars)
- `NUMBER`: Sequential within domain (3 digits, zero-padded)
- `SEVERITY`: CRIT, WARN, or INFO

### Domain Prefixes

| Domain | Prefix | Description | Example |
|--------|--------|-------------|---------|
| Revenue | REV | Bookings, payments, cancellations | REV-001-CRIT |
| Engagement | ENG | Traffic, conversion, views | ENG-002-WARN |
| Property | PROP | Inventory, pricing, listings | PROP-003-INFO |
| Host | HOST | Ratings, verification, activity | HOST-001-CRIT |
| Customer | CUST | Signups, churn, segments | CUST-004-INFO |

### Examples

```
REV-001-CRIT  â†’ Revenue domain, alert #1, critical severity
ENG-003-WARN  â†’ Engagement domain, alert #3, warning severity
PROP-004-INFO â†’ Property domain, alert #4, informational
```

### Naming in Databricks UI

Alert display names follow pattern: `[SEVERITY] Alert Name`

```python
alert_name = f"[{rule['severity']}] {rule['alert_name']}"
# Result: "[CRITICAL] Revenue Drop Alert"
```

---

## Alert Rules Configuration Table

### Schema Definition

```sql
CREATE TABLE IF NOT EXISTS {catalog}.{gold_schema}.alert_rules (
    -- Primary Key
    alert_id STRING NOT NULL
        COMMENT 'Unique identifier for the alert rule (format: DOMAIN-NUMBER-SEVERITY)',
    
    -- Alert Identity
    alert_name STRING NOT NULL
        COMMENT 'Human-readable display name for the alert',
    domain STRING NOT NULL
        COMMENT 'Business domain: revenue, engagement, property, host, customer',
    severity STRING NOT NULL
        COMMENT 'Alert severity: CRITICAL, WARNING, INFO',
    alert_description STRING NOT NULL
        COMMENT 'Detailed description of what this alert monitors',
    
    -- Query Configuration
    alert_query STRING NOT NULL
        COMMENT 'SQL query that returns data when condition is met',
    condition_column STRING NOT NULL
        COMMENT 'Column name to evaluate in alert condition',
    condition_operator STRING NOT NULL
        COMMENT 'Comparison operator: >, <, >=, <=, =, !=',
    condition_threshold STRING NOT NULL
        COMMENT 'Threshold value for condition comparison',
    aggregation_type STRING
        COMMENT 'Optional aggregation: SUM, AVG, COUNT, MIN, MAX, FIRST',
    
    -- Schedule Configuration
    schedule_cron STRING NOT NULL
        COMMENT 'Quartz cron expression for schedule',
    schedule_timezone STRING NOT NULL
        COMMENT 'IANA timezone identifier',
    
    -- Notification Configuration
    notification_emails STRING
        COMMENT 'Comma-separated email addresses',
    notification_slack_channel STRING
        COMMENT 'Slack channel for notifications',
    custom_subject_template STRING
        COMMENT 'Custom email subject template',
    custom_body_template STRING
        COMMENT 'Custom notification body template',
    notify_on_ok BOOLEAN NOT NULL
        COMMENT 'Send notification when alert returns to OK status',
    rearm_seconds INT
        COMMENT 'Cooldown period in seconds before re-triggering',
    
    -- Control
    is_enabled BOOLEAN NOT NULL
        COMMENT 'Whether this alert is active',
    tags STRING
        COMMENT 'JSON-formatted tags for categorization',
    owner STRING NOT NULL
        COMMENT 'Email of alert owner/maintainer',
    
    -- Audit
    record_created_timestamp TIMESTAMP NOT NULL,
    record_updated_timestamp TIMESTAMP NOT NULL,
    
    CONSTRAINT pk_alert_rules PRIMARY KEY (alert_id) NOT ENFORCED
)
USING DELTA
CLUSTER BY AUTO
TBLPROPERTIES (
    'delta.enableChangeDataFeed' = 'true',
    'layer' = 'gold',
    'domain' = 'alerting',
    'entity_type' = 'config',
    'config_table' = 'true'
)
```

### Required Columns for SDK Deployment

| Column | SDK Field | Required | Notes |
|--------|-----------|----------|-------|
| `alert_query` | `query_text` | âœ… | Full SQL query |
| `condition_column` | `AlertOperandColumn.name` | âœ… | Column to check |
| `condition_operator` | `AlertConditionOperator` | âœ… | >, <, =, etc. |
| `condition_threshold` | `AlertOperandValue.string_value` | âœ… | Threshold value |
| `schedule_cron` | `cron_schedule` | âœ… | Quartz format |
| `schedule_timezone` | `cron_timezone` | âœ… | IANA timezone |

---

## SQL Query Patterns

### Pattern 1: Threshold Comparison (Most Common)

**Use Case:** Alert when metric crosses a threshold

```sql
-- REV-002-CRIT: High Cancellation Rate (>15%)
SELECT 
    DATE_ADD(CURRENT_DATE(), -1) as check_in_date,
    SUM(cancellation_count) as cancellations,
    SUM(booking_count) as bookings,
    ROUND(SUM(cancellation_count) / NULLIF(SUM(booking_count), 0) * 100, 1) as cancellation_rate,
    'CRITICAL: Cancellation rate at ' || 
        ROUND(SUM(cancellation_count) / NULLIF(SUM(booking_count), 0) * 100, 1) || 
        '% (' || SUM(cancellation_count) || ' of ' || SUM(booking_count) || ' bookings)' as alert_message
FROM {catalog}.{gold_schema}.fact_booking_daily
WHERE check_in_date = DATE_ADD(CURRENT_DATE(), -1)
HAVING SUM(cancellation_count) / NULLIF(SUM(booking_count), 0) > 0.15
```

**Key Elements:**
- `condition_column`: `cancellation_rate`
- `condition_operator`: `>`
- `condition_threshold`: `15`
- Returns rows ONLY when condition is met (via HAVING clause)
- Includes `alert_message` column for notification content

### Pattern 2: Percentage Change from Baseline

**Use Case:** Alert when metric deviates from historical average

```sql
-- REV-001-CRIT: Revenue Drop (>20% below 7-day average)
SELECT 
    CURRENT_DATE() as alert_date,
    yesterday_revenue,
    avg_7d_revenue,
    ROUND((yesterday_revenue - avg_7d_revenue) / avg_7d_revenue * 100, 1) as pct_change,
    'CRITICAL: Revenue dropped ' || 
        ROUND((avg_7d_revenue - yesterday_revenue) / avg_7d_revenue * 100, 1) || 
        '% below 7-day average ($' || FORMAT_NUMBER(yesterday_revenue, 2) || 
        ' vs avg $' || FORMAT_NUMBER(avg_7d_revenue, 2) || ')' as alert_message
FROM (
    SELECT
        SUM(CASE WHEN check_in_date = DATE_ADD(CURRENT_DATE(), -1) 
            THEN total_booking_value ELSE 0 END) as yesterday_revenue,
        AVG(CASE WHEN check_in_date BETWEEN DATE_ADD(CURRENT_DATE(), -8) 
            AND DATE_ADD(CURRENT_DATE(), -2) 
            THEN daily_total ELSE NULL END) as avg_7d_revenue
    FROM (
        SELECT check_in_date, SUM(total_booking_value) as daily_total
        FROM {catalog}.{gold_schema}.fact_booking_daily
        WHERE check_in_date >= DATE_ADD(CURRENT_DATE(), -8)
        GROUP BY 1
    )
)
WHERE yesterday_revenue < avg_7d_revenue * 0.8
```

### Pattern 3: Statistical Anomaly Detection (Z-Score)

**Use Case:** Alert when metric is statistically unusual

```sql
-- REV-003-WARN: Booking Volume Anomaly (>2 std from mean)
SELECT 
    CURRENT_TIMESTAMP() as alert_time,
    today_bookings,
    ROUND(avg_bookings, 0) as avg_bookings,
    ROUND(stddev_bookings, 0) as stddev_bookings,
    ROUND((today_bookings - avg_bookings) / NULLIF(stddev_bookings, 0), 1) as z_score,
    'WARNING: Booking volume anomaly - ' || today_bookings || ' bookings (' ||
        ROUND((today_bookings - avg_bookings) / NULLIF(stddev_bookings, 0), 1) || 
        ' std deviations from 30-day mean of ' || ROUND(avg_bookings, 0) || ')' as alert_message
FROM (
    SELECT
        SUM(CASE WHEN check_in_date = CURRENT_DATE() 
            THEN booking_count ELSE 0 END) as today_bookings,
        AVG(daily_bookings) as avg_bookings,
        STDDEV(daily_bookings) as stddev_bookings
    FROM (
        SELECT check_in_date, SUM(booking_count) as daily_bookings
        FROM {catalog}.{gold_schema}.fact_booking_daily
        WHERE check_in_date BETWEEN DATE_ADD(CURRENT_DATE(), -30) AND CURRENT_DATE()
        GROUP BY 1
    )
)
WHERE ABS(today_bookings - avg_bookings) > 2 * stddev_bookings
  AND stddev_bookings > 0  -- Prevent division by zero
```

### Pattern 4: Count-Based Alert (Low Activity)

**Use Case:** Alert when count is below threshold

```sql
-- ENG-003-WARN: Low Engagement Properties (<10 views in 7 days)
SELECT 
    COUNT(*) as low_engagement_count,
    CONCAT_WS(', ', COLLECT_LIST(CAST(property_id AS STRING))) as property_ids,
    'WARNING: ' || COUNT(*) || ' properties have <10 views in past 7 days' as alert_message
FROM (
    SELECT 
        p.property_id,
        COALESCE(SUM(e.view_count), 0) as weekly_views
    FROM {catalog}.{gold_schema}.dim_property p
    LEFT JOIN {catalog}.{gold_schema}.fact_property_engagement e
        ON p.property_id = e.property_id
        AND e.engagement_date >= DATE_ADD(CURRENT_DATE(), -7)
    WHERE p.is_current = true
    GROUP BY p.property_id
    HAVING COALESCE(SUM(e.view_count), 0) < 10
)
HAVING COUNT(*) > 0  -- Only alert if there are affected properties
```

### Pattern 5: Informational Summary (Always Triggers)

**Use Case:** Daily/weekly summary reports

```sql
-- REV-005-INFO: Daily Revenue Summary
SELECT 
    DATE_ADD(CURRENT_DATE(), -1) as date,
    SUM(total_booking_value) as total_revenue,
    SUM(booking_count) as total_bookings,
    ROUND(AVG(avg_booking_value), 2) as avg_booking_value,
    1 as always_trigger,  -- âœ… Always returns 1 for INFO alerts
    'Daily Summary: $' || FORMAT_NUMBER(SUM(total_booking_value), 2) || 
        ' revenue from ' || SUM(booking_count) || ' bookings' as alert_message
FROM {catalog}.{gold_schema}.fact_booking_daily
WHERE check_in_date = DATE_ADD(CURRENT_DATE(), -1)
```

**Key:** For INFO alerts that should always trigger, include a column like `always_trigger` set to `1`, with condition `condition_column='always_trigger', condition_operator='=', condition_threshold='1'`.

### Query Design Rules

1. **Always include `alert_message`**: Human-readable notification content
2. **Use `NULLIF()` for division**: Prevent division by zero errors
3. **Filter with WHERE + HAVING**: WHERE for time ranges, HAVING for threshold filtering
4. **Include context in message**: Actual values, thresholds, and percentages
5. **Use `FORMAT_NUMBER()` for currency**: Proper formatting in messages

---

## Databricks SDK Integration (Recommended)

**SDK Reference:** https://databricks-sdk-py.readthedocs.io/en/latest/workspace/sql/alerts_v2.html

**Why SDK over REST API?**
- Automatic authentication via `WorkspaceClient()` (no token handling)
- Typed objects (`AlertV2`) with IDE autocompletion
- Pagination handled automatically
- Built-in error types (`ResourceAlreadyExists`)
- Cleaner, more maintainable code

### âš ï¸ CRITICAL: SDK Version Requirement

**The `AlertV2` type requires SDK version >= 0.40.0.** Databricks serverless environments have an older pre-installed SDK that takes precedence over job dependencies.

**Solution:** Force pip upgrade at the start of your notebook:

```python
# Databricks notebook source
# MAGIC %pip install --upgrade databricks-sdk>=0.40.0 --quiet

# COMMAND ----------

# MAGIC %restart_python

# COMMAND ----------

# Now imports will work
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.sql import AlertV2
```

**Why this is necessary:**
- Serverless environments have a pre-installed SDK at `/databricks/python/lib/python3.12/site-packages/`
- Job dependencies (`environments.spec.dependencies`) install packages but don't override system packages
- Python imports the system SDK first (earlier in `sys.path`)
- `%pip install --upgrade` + `%restart_python` forces the new version to be used

### SDK Methods

| Method | Description |
|--------|-------------|
| `ws.alerts_v2.create_alert(alert)` | Create a new alert |
| `ws.alerts_v2.get_alert(id)` | Get alert by ID |
| `ws.alerts_v2.list_alerts()` | List all alerts (paginated iterator) |
| `ws.alerts_v2.update_alert(id, alert, update_mask)` | Update an alert |
| `ws.alerts_v2.trash_alert(id)` | Soft delete (30-day recovery) |

### Import Pattern

```python
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.sql import AlertV2
```

**Note:** The full set of typed classes (`AlertV2Condition`, etc.) may not be needed - SDK accepts dicts via `AlertV2.from_dict()`.

### Build AlertV2 Object

```python
def build_alert_v2(cfg, warehouse_id: str, subscriptions: list) -> AlertV2:
    """Build an AlertV2 object from config row."""
    
    display_name = f"[{cfg.severity}] {cfg.alert_name}"
    
    # Build operand (source column with optional aggregation)
    operand = AlertV2ConditionOperand(name=cfg.threshold_column)
    if cfg.aggregation_type and cfg.aggregation_type.upper() != "FIRST":
        operand.aggregation = cfg.aggregation_type.upper()
    
    # Build threshold value
    try:
        threshold_value = AlertV2ConditionThresholdValue(
            double_value=float(cfg.threshold_value)
        )
    except (ValueError, TypeError):
        threshold_value = AlertV2ConditionThresholdValue(
            string_value=str(cfg.threshold_value)
        )
    
    # Build notification subscriptions
    notification_subs = []
    for sub in subscriptions:
        if "user_email" in sub:
            notification_subs.append(
                AlertV2NotificationSubscription(user_email=sub["user_email"])
            )
        elif "destination_id" in sub:
            notification_subs.append(
                AlertV2NotificationSubscription(destination_id=sub["destination_id"])
            )
    
    # Build notification
    notification = AlertV2Notification(
        notify_on_ok=cfg.notify_on_ok,
        subscriptions=notification_subs if notification_subs else None,
        retrigger_seconds=cfg.rearm_seconds,
    )
    
    # Build condition
    condition = AlertV2Condition(
        comparison_operator=cfg.comparison_operator,  # e.g., "GREATER_THAN"
        empty_result_state=cfg.empty_result_state or "OK",
        operand=operand,
        threshold=AlertV2ConditionThreshold(value=threshold_value),
        notification=notification,
    )
    
    # Build schedule
    schedule = AlertV2Schedule(
        quartz_cron_schedule=cfg.schedule_cron,
        timezone_id=cfg.schedule_timezone,
        pause_status="UNPAUSED" if cfg.is_enabled else "PAUSED",
    )
    
    # Build AlertV2 object
    alert = AlertV2(
        display_name=display_name,
        warehouse_id=warehouse_id,
        query_text=cfg.alert_query,
        schedule=schedule,
        condition=condition,
    )
    
    # Add custom templates if present
    if cfg.custom_subject_template:
        alert.custom_summary = cfg.custom_subject_template
    if cfg.custom_body_template:
        alert.custom_description = cfg.custom_body_template
    
    return alert
```

### List Existing Alerts (SDK handles pagination)

```python
def list_existing_alerts(ws: WorkspaceClient) -> dict:
    """List all existing alerts, keyed by display_name.
    
    SDK handles pagination automatically via iterator.
    """
    existing = {}
    for alert in ws.alerts_v2.list_alerts():
        if alert.display_name:
            existing[alert.display_name] = alert
    return existing
```

### Create Alert

```python
def create_alert(ws: WorkspaceClient, alert: AlertV2) -> AlertV2:
    """Create a new SQL alert using SDK."""
    return ws.alerts_v2.create_alert(alert)
```

### Update Alert (requires update_mask)

```python
def update_alert(ws: WorkspaceClient, alert_id: str, alert: AlertV2) -> AlertV2:
    """Update an existing SQL alert.
    
    âš ï¸ CRITICAL: update_mask is required to specify which fields to update.
    """
    return ws.alerts_v2.update_alert(
        id=alert_id,
        alert=alert,
        update_mask="display_name,query_text,warehouse_id,schedule,condition,custom_summary,custom_description"
    )
```

### Delete Alert (Trash)

```python
def trash_alert(ws: WorkspaceClient, alert_id: str) -> None:
    """Soft delete an alert (can be restored within 30 days)."""
    ws.alerts_v2.trash_alert(alert_id)
```

### Handle ResourceAlreadyExists Error

**âš ï¸ CRITICAL PATTERN:** SDK raises `ResourceAlreadyExists` when alert exists. Catch and update instead.

```python
from databricks.sdk.errors import ResourceAlreadyExists

def sync_alert(ws: WorkspaceClient, cfg, warehouse_id: str, existing_alerts: dict):
    """Sync a single alert with ResourceAlreadyExists handling."""
    
    alert = build_alert_v2(cfg, warehouse_id, subscriptions=[])
    display_name = alert.display_name
    existing = existing_alerts.get(display_name)
    
    if not existing:
        # Create new alert
        try:
            created = ws.alerts_v2.create_alert(alert)
            return {"status": "CREATED", "id": created.id}
        except ResourceAlreadyExists:
            # Refresh and update instead
            print(f"  Alert already exists, updating...")
            existing_alerts = list_existing_alerts(ws)
            existing = existing_alerts.get(display_name)
            if existing:
                updated = update_alert(ws, existing.id, alert)
                return {"status": "UPDATED", "id": updated.id}
            raise RuntimeError("Alert exists but could not be found")
    else:
        # Update existing alert
        updated = update_alert(ws, existing.id, alert)
        return {"status": "UPDATED", "id": updated.id}
```

---

## REST API Integration (Alternative)

**Use REST API when:**
- SDK version constraints are problematic
- You need more control over request/response handling
- SDK doesn't support a specific feature yet
- You prefer not to add startup overhead from `%pip install`

**Trade-off:** REST API approach is faster (no pip install overhead) but requires manual token handling.

### API Endpoint

```
/api/2.0/alerts
```

**âš ï¸ NOT:** `/api/2.0/sql/alerts` or `/api/2.0/sql/alerts-v2`

---

## DAB Job Configuration

### Setup Job (Create/Update Config Table)

```yaml
# resources/gold/alert_rules_setup_job.yml
resources:
  jobs:
    alert_rules_setup_job:
      name: "[${bundle.target}] Wanderbricks Alert Rules - Setup"
      description: "Creates alert_rules configuration table with alert definitions"
      
      environments:
        - environment_key: default
          spec:
            environment_version: "4"
      
      tasks:
        - task_key: setup_alert_rules
          environment_key: default
          notebook_task:
            notebook_path: ../../src/wanderbricks_gold/alerting/setup_alert_rules.py
            base_parameters:
              catalog: ${var.catalog}
              gold_schema: ${var.gold_schema}
          timeout_seconds: 1800
      
      email_notifications:
        on_failure:
          - data-engineering@company.com
      
      tags:
        environment: ${bundle.target}
        project: wanderbricks
        layer: gold
        job_type: setup
        component: alerting
        compute_type: serverless
```

### Deploy Job (Create SQL Alerts from Config)

```yaml
# resources/alerting/sql_alert_deployment_job.yml
resources:
  jobs:
    sql_alert_deployment_job:
      name: "[${bundle.target}] SQL Alert - Deploy"
      description: "Deploys SQL alerts from alert_configurations table using V2 API"
      
      environments:
        - environment_key: default
          spec:
            environment_version: "4"
            dependencies:
              - "requests>=2.28.0"  # For REST API calls
      
      tasks:
        - task_key: sync_sql_alerts
          environment_key: default
          notebook_task:
            notebook_path: ../../src/alerting/sync_sql_alerts.py
            base_parameters:
              catalog: ${var.catalog}
              gold_schema: ${var.gold_schema}
              warehouse_id: ${var.warehouse_id}
              dry_run: "false"
              delete_disabled: "false"  # Set to "true" to auto-delete disabled alerts
              api_base: "/api/2.0/alerts"  # âš ï¸ V2 API endpoint (NOT /api/2.0/sql/alerts!)
              enable_parallel: "false"
              parallel_workers: "5"
          timeout_seconds: 1800
      
      email_notifications:
        on_failure:
          - data-engineering@company.com
        on_success:
          - data-engineering@company.com
      
      tags:
        environment: ${bundle.target}
        project: health_monitor
        layer: gold
        job_type: deployment
        component: alerting
        compute_type: serverless
```

### Key Configuration Points

1. **API Endpoint**: Must be `/api/2.0/alerts` (V2 API) - NOT `/api/2.0/sql/alerts`
2. **Warehouse ID**: Required for SQL Alert execution
3. **Dry Run**: Enable for validation without creating alerts
4. **Delete Disabled**: Controls whether disabled alerts are removed from Databricks
5. **Serverless**: Both jobs use serverless compute
6. **Parallel Sync**: Optional parallel processing for large alert sets

---

## Complete Examples

### Example 1: Critical Revenue Alert

```python
{
    "alert_id": "REV-001-CRIT",
    "alert_name": "Revenue Drop Alert",
    "domain": "revenue",
    "severity": "CRITICAL",
    "alert_description": "Triggers when daily revenue drops below 80% of 7-day average.",
    "alert_query": """
        SELECT 
            CURRENT_DATE() as alert_date,
            yesterday_revenue,
            avg_7d_revenue,
            ROUND((yesterday_revenue - avg_7d_revenue) / avg_7d_revenue * 100, 1) as pct_change,
            'CRITICAL: Revenue dropped ' || 
                ROUND((avg_7d_revenue - yesterday_revenue) / avg_7d_revenue * 100, 1) || 
                '% below 7-day average' as alert_message
        FROM (
            SELECT
                SUM(CASE WHEN check_in_date = DATE_ADD(CURRENT_DATE(), -1) 
                    THEN total_booking_value ELSE 0 END) as yesterday_revenue,
                AVG(CASE WHEN check_in_date BETWEEN DATE_ADD(CURRENT_DATE(), -8) 
                    AND DATE_ADD(CURRENT_DATE(), -2) 
                    THEN daily_total ELSE NULL END) as avg_7d_revenue
            FROM (
                SELECT check_in_date, SUM(total_booking_value) as daily_total
                FROM catalog.gold.fact_booking_daily
                WHERE check_in_date >= DATE_ADD(CURRENT_DATE(), -8)
                GROUP BY 1
            )
        )
        WHERE yesterday_revenue < avg_7d_revenue * 0.8
    """,
    "condition_column": "pct_change",
    "condition_operator": "<",
    "condition_threshold": "-20",
    "aggregation_type": "FIRST",
    "schedule_cron": "0 0 6 * * ?",  # Daily at 6 AM
    "schedule_timezone": "America/Los_Angeles",
    "notification_emails": "finance@company.com,revenue@company.com",
    "notification_slack_channel": "#revenue-alerts",
    "custom_subject_template": "[{{ALERT_STATUS}}] CRITICAL: {{ALERT_NAME}}",
    "custom_body_template": """Alert: {{ALERT_NAME}}
Time: {{ALERT_TIME}}
Status: {{ALERT_STATUS}}

{{QUERY_RESULT_VALUE}}

Action Required: Investigate immediately.

View Alert: {{ALERT_URL}}""",
    "notify_on_ok": True,
    "rearm_seconds": 1800,  # 30 min cooldown
    "is_enabled": True,
    "tags": '{"team": "finance", "priority": "p1"}',
    "owner": "data-engineering@company.com"
}
```

### Example 2: Warning Engagement Alert

```python
{
    "alert_id": "ENG-002-WARN",
    "alert_name": "Conversion Rate Drop",
    "domain": "engagement",
    "severity": "WARNING",
    "alert_description": "Triggers when average conversion rate falls below 2%.",
    "alert_query": """
        SELECT 
            DATE_ADD(CURRENT_DATE(), -1) as date,
            ROUND(AVG(conversion_rate), 2) as avg_conversion,
            COUNT(*) as property_count,
            'WARNING: Conversion rate at ' || 
                ROUND(AVG(conversion_rate), 2) || '% (threshold: 2%)' as alert_message
        FROM catalog.gold.fact_property_engagement
        WHERE engagement_date = DATE_ADD(CURRENT_DATE(), -1)
        HAVING AVG(conversion_rate) < 2
    """,
    "condition_column": "avg_conversion",
    "condition_operator": "<",
    "condition_threshold": "2",
    "aggregation_type": "FIRST",
    "schedule_cron": "0 0 8 * * ?",  # Daily at 8 AM
    "schedule_timezone": "America/Los_Angeles",
    "notification_emails": "growth@company.com",
    "notification_slack_channel": "#marketing-alerts",
    "custom_subject_template": "[{{ALERT_STATUS}}] Warning: {{ALERT_NAME}}",
    "custom_body_template": """Alert: {{ALERT_NAME}}
Time: {{ALERT_TIME}}
Status: {{ALERT_STATUS}}

{{QUERY_RESULT_VALUE}}

Please investigate at your earliest convenience.

View Alert: {{ALERT_URL}}""",
    "notify_on_ok": True,
    "rearm_seconds": 3600,
    "is_enabled": True,
    "tags": '{"team": "growth", "priority": "p2"}',
    "owner": "data-engineering@company.com"
}
```

### Example 3: Info Summary Alert

```python
{
    "alert_id": "REV-005-INFO",
    "alert_name": "Daily Revenue Summary",
    "domain": "revenue",
    "severity": "INFO",
    "alert_description": "Daily informational summary of revenue metrics.",
    "alert_query": """
        SELECT 
            DATE_ADD(CURRENT_DATE(), -1) as date,
            SUM(total_booking_value) as total_revenue,
            SUM(booking_count) as total_bookings,
            ROUND(AVG(avg_booking_value), 2) as avg_booking_value,
            1 as always_trigger,
            'Daily Summary: $' || FORMAT_NUMBER(SUM(total_booking_value), 2) || 
                ' revenue from ' || SUM(booking_count) || ' bookings' as alert_message
        FROM catalog.gold.fact_booking_daily
        WHERE check_in_date = DATE_ADD(CURRENT_DATE(), -1)
    """,
    "condition_column": "always_trigger",
    "condition_operator": "=",
    "condition_threshold": "1",
    "aggregation_type": "FIRST",
    "schedule_cron": "0 0 9 * * ?",  # Daily at 9 AM
    "schedule_timezone": "America/Los_Angeles",
    "notification_emails": "leadership@company.com",
    "notification_slack_channel": "#daily-metrics",
    "custom_subject_template": "[INFO] {{ALERT_NAME}}",
    "custom_body_template": """Daily Summary: {{ALERT_NAME}}
Time: {{ALERT_TIME}}

{{QUERY_RESULT_TABLE}}

View Details: {{ALERT_URL}}""",
    "notify_on_ok": False,
    "rearm_seconds": None,
    "is_enabled": True,
    "tags": '{"team": "leadership", "priority": "p3"}',
    "owner": "data-engineering@company.com"
}
```

---

## Custom Notification Templates

### Available Variables

| Variable | Description |
|----------|-------------|
| `{{ALERT_NAME}}` | Display name of the alert |
| `{{ALERT_STATUS}}` | Current status: TRIGGERED, OK, ERROR |
| `{{ALERT_TIME}}` | Timestamp when alert was evaluated |
| `{{QUERY_RESULT_VALUE}}` | Single value from query result |
| `{{QUERY_RESULT_TABLE}}` | Full table of query results |
| `{{ALERT_URL}}` | Link to alert in Databricks UI |

### Critical Template

```
Alert: {{ALERT_NAME}}
Time: {{ALERT_TIME}}
Status: {{ALERT_STATUS}}

{{QUERY_RESULT_VALUE}}

âš ï¸ Action Required: Investigate immediately.

View Alert: {{ALERT_URL}}
```

### Warning Template

```
Alert: {{ALERT_NAME}}
Time: {{ALERT_TIME}}
Status: {{ALERT_STATUS}}

{{QUERY_RESULT_VALUE}}

Please investigate at your earliest convenience.

View Alert: {{ALERT_URL}}
```

### Info Template

```
Summary: {{ALERT_NAME}}
Time: {{ALERT_TIME}}

{{QUERY_RESULT_TABLE}}

View Details: {{ALERT_URL}}
```

---

## Schedule Patterns (Quartz Cron)

### Common Schedules

| Pattern | Cron Expression | Description |
|---------|-----------------|-------------|
| Daily at 6 AM | `0 0 6 * * ?` | Morning critical alerts |
| Hourly | `0 0 * * * ?` | Frequent monitoring |
| Every 15 min | `0 0/15 * * * ?` | High-frequency alerts |
| Weekly Monday 9 AM | `0 0 9 ? * MON` | Weekly summaries |
| Business hours | `0 0 9-17 * * ?` | 9 AM to 5 PM hourly |

### Timezone Considerations

Always use IANA timezone identifiers:
- `America/Los_Angeles` (Pacific)
- `America/New_York` (Eastern)
- `UTC` (Coordinated Universal Time)
- `America/Chicago` (Central)

---

## Troubleshooting

### Problem: Alert Not Triggering

**Symptoms:** Alert is enabled but never sends notifications

**Diagnosis Steps:**
1. Run the alert query manually - does it return rows?
2. Check condition: does returned value satisfy operator + threshold?
3. Verify warehouse is running when schedule fires
4. Check notification destination configuration

**Common Fixes:**
```sql
-- Debug: Run query and check condition column value
SELECT *, 
    CASE 
        WHEN {condition_column} {operator} {threshold} THEN 'WOULD_TRIGGER'
        ELSE 'NO_TRIGGER'
    END as would_trigger
FROM ({alert_query})
```

### Problem: Alert Always Triggering

**Symptoms:** Getting notifications even when condition shouldn't be met

**Diagnosis:**
- Query returns rows even when condition isn't met
- Missing HAVING clause
- Wrong operator direction

**Fix:** Add HAVING clause to filter results:
```sql
-- âŒ WRONG: Returns rows always
SELECT rate FROM ... WHERE rate IS NOT NULL

-- âœ… CORRECT: Only returns rows when threshold crossed
SELECT rate FROM ... HAVING rate > 15
```

### Problem: "Query Failed" Error Status

**Symptoms:** Alert shows ERROR status

**Common Causes:**
1. Table doesn't exist (wrong catalog/schema)
2. Column doesn't exist
3. SQL syntax error
4. Warehouse unavailable

**Debug:**
```python
# Test query in notebook first
spark.sql(f"""
    {alert_query}
""").display()
```

### Problem: SDK Permission Error

**Symptoms:** `PermissionDenied` when creating alerts

**Fix:** Ensure service principal or user has:
- `CAN_MANAGE` permission on SQL Warehouse
- `CAN_CREATE` permission for SQL Alerts
- `CAN_USE` on catalog and schema

### Problem: V2 API List Response Key

**Symptoms:** `list_existing_alerts()` returns empty dict even though alerts exist

**Root Cause:** V2 API returns `alerts` key, not `results`:
```json
// V2 API Response (CORRECT)
{"alerts": [...], "next_page_token": "..."}

// NOT "results" like other APIs
{"results": [...]}  // WRONG - this is NOT what V2 returns
```

**Fix:** Use `alerts` key when parsing response:
```python
# âœ… CORRECT
for alert in result.get("alerts", []):
    existing[alert.get("display_name")] = alert

# âŒ WRONG
for alert in result.get("results", []):  # Will always be empty!
```

---

### Problem: Duplicate Alerts Created

**Symptoms:** Multiple alerts with same name

**Fix:** Check existing alerts before creating:
```python
existing = get_existing_alerts(ws)
if alert_name not in existing:
    ws.alerts.create(...)
```

### Problem: RESOURCE_ALREADY_EXISTS Error

**Symptoms:** `HTTP 400` with `RESOURCE_ALREADY_EXISTS` when creating alert

**Root Causes:**
1. Alert already exists with same display name
2. Previous deployment created alert but sync status wasn't updated
3. Manual alert creation with same name

**Fix:** Catch the error and update instead of create:
```python
try:
    result = create_alert(session, host, token, api_base, payload)
except Exception as e:
    if "RESOURCE_ALREADY_EXISTS" in str(e):
        # Find existing alert and update
        existing = list_existing_alerts(session, host, token, api_base)
        if display_name in existing:
            alert_id = existing[display_name]["id"]
            update_alert(session, host, token, api_base, alert_id, payload)
    else:
        raise
```

### Problem: Update Fails with 400 Error

**Symptoms:** PATCH request returns 400 error

**Root Cause:** Missing `update_mask` parameter (required for V2 API)

**Fix:** Include update_mask in PATCH URL:
```python
# âŒ WRONG
PATCH /api/2.0/alerts/{id}
Body: {...}

# âœ… CORRECT
PATCH /api/2.0/alerts/{id}?update_mask=display_name,query_text,schedule,evaluation
Body: {...}
```

### Problem: Wrong API Endpoint

**Symptoms:** `HTTP 404` when calling API

**Common Mistakes:**
- `/api/2.0/sql/alerts` (Legacy, wrong for V2)
- `/api/2.0/sql/alerts-v2` (Incorrect)
- `/api/2.0/preview/sql/alerts` (Old preview path)

**Fix:** Use correct V2 endpoint:
```python
# âœ… CORRECT V2 endpoint
api_base = "/api/2.0/alerts"
```

---

## Unity Catalog Table Limitations

**âš ï¸ CRITICAL:** Unity Catalog has different DDL support than traditional Databricks tables.

### CHECK Constraints NOT Supported

```sql
-- âŒ WRONG: Will fail in Unity Catalog
CREATE TABLE alert_configurations (
    severity STRING NOT NULL,
    CONSTRAINT chk_severity CHECK (severity IN ('CRITICAL', 'WARNING', 'INFO'))
)

-- âœ… CORRECT: Validate in application code
CREATE TABLE alert_configurations (
    severity STRING NOT NULL
        COMMENT 'Alert severity: CRITICAL, WARNING, INFO'
)
```

### DEFAULT Values NOT Supported

```sql
-- âŒ WRONG: Will fail in Unity Catalog
CREATE TABLE alert_configurations (
    is_enabled BOOLEAN NOT NULL DEFAULT true,
    record_created_timestamp TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP()
)

-- âœ… CORRECT: Set values explicitly in INSERT statements
CREATE TABLE alert_configurations (
    is_enabled BOOLEAN NOT NULL,
    record_created_timestamp TIMESTAMP NOT NULL
)

-- Then in INSERT:
INSERT INTO alert_configurations VALUES (
    ...,
    true,  -- is_enabled
    CURRENT_TIMESTAMP()  -- record_created_timestamp
)
```

### PARTITIONED BY and CLUSTER BY AUTO Conflict

```sql
-- âŒ WRONG: Cannot use both together
CREATE TABLE alert_history (
    ...
)
PARTITIONED BY (alert_date)
CLUSTER BY AUTO

-- âœ… CORRECT Option 1: Use CLUSTER BY AUTO only (recommended)
CREATE TABLE alert_history (
    ...
)
CLUSTER BY AUTO

-- âœ… CORRECT Option 2: Use explicit PARTITIONED BY only
CREATE TABLE alert_history (
    ...
)
PARTITIONED BY (alert_date)
```

---

## Best Practices

### âœ… DO

1. **Use config table for all rules** - Never hardcode alert configurations
2. **Include `alert_message` column** - Human-readable notification content
3. **Test queries manually first** - Verify in notebook before adding to config
4. **Use NULLIF for division** - Prevent division by zero errors
5. **Set appropriate rearm periods** - Prevent alert fatigue (1800-3600 seconds)
6. **Enable notify_on_ok for critical** - Know when issues are resolved
7. **Use dry_run for validation** - Test deployment without creating alerts

### âŒ DON'T

1. **Don't use parameters in queries** - SQL Alerts don't support `${param}` syntax
2. **Don't skip the HAVING clause** - Queries should only return rows when alerting
3. **Don't set rearm too low** - Causes notification spam
4. **Don't hardcode credentials** - Use WorkspaceClient() for auto-auth
5. **Don't skip schema validation** - Verify alert_rules table exists before deploying

---

## Workflow Summary

### Initial Setup

```bash
# 1. Deploy alert rules table
databricks bundle run alert_rules_setup_job -t dev

# 2. Verify rules
# SELECT * FROM {catalog}.{gold_schema}.alert_rules

# 3. Deploy alerts (dry run first)
# Edit dry_run: "true" in job config
databricks bundle run alert_deploy_job -t dev

# 4. Deploy alerts (for real)
# Edit dry_run: "false" in job config
databricks bundle run alert_deploy_job -t dev
```

### Adding New Alerts

1. Add rule to `get_alert_rules()` function in `setup_alert_rules.py`
2. Run setup job: `databricks bundle run alert_rules_setup_job -t dev`
3. Run deploy job: `databricks bundle run alert_deploy_job -t dev`

### Modifying Existing Alerts

1. Update rule in config table or `setup_alert_rules.py`
2. Run setup job to update config table
3. Delete existing alert in Databricks UI
4. Run deploy job to recreate with new configuration

---

## Validation Checklist

Before deploying alerts:

### Configuration Table
- [ ] Table uses Unity Catalog (no CHECK constraints, no DEFAULT values)
- [ ] All columns have explicit values in INSERT statements
- [ ] `CLUSTER BY AUTO` used (not `PARTITIONED BY`)

### V2 API
- [ ] API endpoint is `/api/2.0/alerts` (NOT `/api/2.0/sql/alerts`)
- [ ] Payload uses `display_name` (not `name`)
- [ ] Payload uses `query_text` (not `query_id`)
- [ ] Payload uses `evaluation` block (not `condition`)
- [ ] PATCH requests include `?update_mask=...` parameter
- [ ] `FIRST` aggregation maps to `null`/omitted in API

### Queries
- [ ] Fully qualified table names (no `${catalog}` parameters)
- [ ] Includes `alert_message` column for notifications
- [ ] Uses `HAVING` clause to filter trigger conditions
- [ ] Uses `NULLIF()` for division operations

### Error Handling
- [ ] Handles `RESOURCE_ALREADY_EXISTS` by updating instead of failing
- [ ] Implements retry logic with exponential backoff
- [ ] Updates sync status in config table

---

## References

### Official Documentation
- [Databricks SQL Alerts V2 API](https://docs.databricks.com/api/workspace/alertsv2/createalert)
- [Databricks SQL Alerts User Guide](https://docs.databricks.com/aws/en/sql/user/alerts)
- [Databricks SDK - Alerts V2](https://databricks-sdk-py.readthedocs.io/en/stable/workspace/sql/alerts_v2.html)
- [Quartz Cron Expression Format](http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html)

### Internal Documentation
- [Alerting Framework Architecture](../../docs/alerting/architecture.md)
- [Alerting API Reference](../../docs/alerting/api-reference.md)
- [Alerting Deployment Guide](../../docs/alerting/deployment-guide.md)
- [Alerting Operations Guide](../../docs/alerting/operations-guide.md)

---

## Version History

- **v2.1** (December 31, 2024) - SDK integration improvements
  - **Changed SDK to Recommended** (was Alternative)
  - Added critical `%pip install --upgrade databricks-sdk>=0.40.0` pattern
  - Added `%restart_python` requirement for SDK to take effect
  - Documented why job dependencies don't override system SDK
  - Added V2 API list response key (`alerts` not `results`)
  - Created `context/prompts/14-sql-alerts-prompt.md` for easy reuse
  - **Key Learning:** Serverless environments have pre-installed SDK; use `%pip install --upgrade` + `%restart_python`

- **v2.0** (December 2024) - Major update based on production implementation learnings
  - Added V2 API reference section with correct endpoint (`/api/2.0/alerts`)
  - Added REST API integration patterns
  - Added `update_mask` requirement for PATCH requests
  - Added `RESOURCE_ALREADY_EXISTS` error handling pattern
  - Added Unity Catalog table limitations (CHECK, DEFAULT, PARTITIONED BY)
  - Added aggregation type mapping (`FIRST` â†’ null)
  - Added validation checklist
  - Updated references to official V2 API documentation
  - **Key Learning:** V2 API endpoint is `/api/2.0/alerts`, NOT `/api/2.0/sql/alerts`

- **v1.0** (October 2024) - Initial config-driven alerting framework
  - Core principles and alert ID convention
  - SQL query patterns
  - SDK integration patterns
  - DAB job configuration

---

**Last Updated:** December 31, 2024  
**Pattern Origin:** Databricks Health Monitor - Config-driven alerting framework  
**Key Lessons:**
1. V2 API endpoint is `/api/2.0/alerts` (NOT `/api/2.0/sql/alerts` or `/api/2.0/sql/alerts-v2`)
2. PATCH requests require `update_mask` parameter
3. Handle `RESOURCE_ALREADY_EXISTS` by updating instead of failing
4. Unity Catalog doesn't support CHECK constraints or DEFAULT values
5. `FIRST` aggregation type maps to null/omitted in V2 API payload
6. Always use fully qualified table names - query parameters are NOT supported
7. **SDK requires `%pip install --upgrade` + `%restart_python`** - job dependencies don't override system packages
8. V2 API list response uses `alerts` key (NOT `results`)
