---
description: MLflow Tracing, ResponsesAgent, Streaming, Multi-Agent Orchestration, and OBO Authentication patterns
globs: src/agents/**/*.py
alwaysApply: false
---
# MLflow 3.0 Tracing & Agent Implementation Patterns

## Pattern Recognition
Comprehensive patterns for production agent implementation covering tracing, ResponsesAgent interface, streaming responses, multi-agent orchestration with Genie, on-behalf-of authentication, and visualization hints. Based on production implementation of Databricks Health Monitor Agent.

---

## üî¥ CRITICAL: ResponsesAgent is MANDATORY for AI Playground

**Without `ResponsesAgent`, your agent will NOT work in AI Playground, Agent Evaluation, or Mosaic AI features.**

Reference: [Microsoft Docs - Model Signatures](https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-framework/create-agent#understand-model-signatures-to-ensure-compatibility-with-azure-databricks-features)

---

## Pattern 1: ResponsesAgent Implementation

### Why ResponsesAgent?

| Feature | ResponsesAgent | Legacy ChatAgent/PythonModel |
|---|---|---|
| Automatic signature inference | ‚úÖ Yes | ‚ùå Manual required |
| AI Playground compatible | ‚úÖ Yes | ‚ö†Ô∏è Depends on signature |
| Streaming support | ‚úÖ Built-in | ‚ùå Manual |
| Tool calling | ‚úÖ Built-in | ‚ùå Manual |
| Multi-agent support | ‚úÖ Built-in | ‚ùå Manual |

### ResponsesAgent Template

```python
import mlflow
import uuid
from mlflow.pyfunc import ResponsesAgent
from mlflow.types.responses import (
    ResponsesAgentRequest,
    ResponsesAgentResponse,
    ResponsesAgentStreamEvent,
    ResponsesAgentStreamEventDelta,
    ResponsesAgentMessageContentDelta
)

class HealthMonitorAgent(ResponsesAgent):
    """
    Production agent implementing ResponsesAgent with streaming.
    
    Reference: https://mlflow.org/docs/latest/genai/serving/responses-agent
    """
    
    def __init__(self):
        super().__init__()
        # Initialize components
        self.llm_endpoint = "databricks-claude-sonnet-4-5"
        self._graph = None  # Lazy-loaded LangGraph
    
    @mlflow.trace(name="health_monitor_agent", span_type="AGENT")
    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:
        """
        Non-streaming prediction.
        
        Input format:  {"input": [{"role": "user", "content": "..."}]}
        Output format: ResponsesAgentResponse
        """
        # Extract query from request.input (NOT messages!)
        input_messages = [msg.model_dump() for msg in request.input]
        query = input_messages[-1].get("content", "")
        
        # Extract custom inputs
        custom_inputs = request.custom_inputs or {}
        user_id = custom_inputs.get("user_id", "unknown")
        
        # Process query
        response_text = self._process(query, custom_inputs)
        
        # Return ResponsesAgentResponse
        return ResponsesAgentResponse(
            output=[self.create_text_output_item(
                text=response_text,
                id=str(uuid.uuid4())
            )],
            custom_outputs={
                "source": "agent",
                "domains_queried": ["cost", "reliability"],
                "user_id": user_id
            }
        )
    
    def _process(self, query: str, custom_inputs: dict) -> str:
        """Core processing logic."""
        # Your agent logic here
        return f"Processed: {query}"


# ============================================================================
# LOGGING (NO Manual Signature!)
# ============================================================================

def log_agent():
    agent = HealthMonitorAgent()
    
    # CRITICAL: Set model for MLflow
    mlflow.models.set_model(agent)
    
    # Input example (ResponsesAgent format - uses 'input', not 'messages')
    input_example = {
        "input": [{"role": "user", "content": "What is the status?"}],
        "custom_inputs": {"user_id": "test_user"}
    }
    
    with mlflow.start_run(run_name="log_agent"):
        # ================================================================
        # CRITICAL: DO NOT pass signature parameter!
        # ResponsesAgent automatically infers the correct signature.
        # Manual signatures WILL BREAK AI Playground compatibility.
        # ================================================================
        logged_model = mlflow.pyfunc.log_model(
            artifact_path="agent",
            python_model=agent,
            input_example=input_example,
            # signature=...  # ‚ùå NEVER include this!
            registered_model_name="health_monitor_agent",
            pip_requirements=[
                "mlflow>=3.0.0",
                "databricks-sdk>=0.28.0",
                "langchain>=0.3.0",
                "langgraph>=0.2.0",
            ],
        )
        
        print(f"‚úì Agent logged: {logged_model.model_uri}")
    
    return logged_model
```

---

## Pattern 2: Streaming Responses

### Streaming Implementation

```python
from typing import Generator

class HealthMonitorAgent(ResponsesAgent):
    """Agent with streaming support."""
    
    def predict_stream(
        self,
        request: ResponsesAgentRequest
    ) -> Generator[ResponsesAgentStreamEvent, None, None]:
        """
        Stream ResponsesAgent events (delta pattern).
        
        Yields ResponsesAgentStreamEvent objects:
        - type="output_item.delta" for text chunks
        - type="output_item.done" for completion
        """
        input_messages = [msg.model_dump() for msg in request.input]
        query = input_messages[-1].get("content", "")
        
        # Generate unique item ID
        item_id = str(uuid.uuid4())
        
        # Stream text chunks
        for chunk in self._process_streaming(query):
            yield ResponsesAgentStreamEvent(
                type="output_item.delta",
                delta=ResponsesAgentStreamEventDelta(
                    type="message_delta",
                    delta=ResponsesAgentMessageContentDelta(
                        type="text",
                        text=chunk
                    )
                ),
                item_id=item_id
            )
        
        # Final done event (REQUIRED)
        yield ResponsesAgentStreamEvent(
            type="output_item.done",
            item_id=item_id
        )
    
    def _process_streaming(self, query: str) -> Generator[str, None, None]:
        """Generate response chunks."""
        # Example: Stream from LLM
        for chunk in self.llm.stream(query):
            if hasattr(chunk, 'content') and chunk.content:
                yield chunk.content
    
    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:
        """
        Non-streaming prediction (delegates to streaming).
        
        Code reuse pattern: predict() collects all chunks from predict_stream().
        """
        chunks = []
        item_id = None
        
        for event in self.predict_stream(request):
            if event.type == "output_item.delta":
                if event.delta and event.delta.delta:
                    chunks.append(event.delta.delta.text)
            elif event.type == "output_item.done":
                item_id = event.item_id
        
        full_text = "".join(chunks)
        
        return ResponsesAgentResponse(
            output=[self.create_text_output_item(
                text=full_text,
                id=item_id or str(uuid.uuid4())
            )]
        )
```

---

## Pattern 3: MLflow Tracing

### Automatic Tracing with Decorators

```python
import mlflow

@mlflow.trace(name="orchestrator_node", span_type="AGENT")
def orchestrator_node(state: dict) -> dict:
    """
    LangGraph node with automatic tracing.
    
    All inputs/outputs automatically logged to trace.
    """
    query = state["messages"][-1].content
    
    # Classify intent
    domains = classify_intent(query)
    
    # Route to domain workers
    results = route_to_workers(query, domains)
    
    return {"messages": state["messages"] + [results]}


@mlflow.trace(name="genie_query", span_type="TOOL")
def query_genie(domain: str, query: str) -> str:
    """
    Tool call with automatic tracing.
    
    Span type TOOL indicates external service call.
    """
    genie = get_genie_space(domain)
    
    try:
        result = genie.invoke(query)
        return result
    except Exception as e:
        # Log error to trace
        mlflow.update_current_trace(
            attributes={"error": str(e), "domain": domain}
        )
        raise
```

### Manual Span Creation (Fine-Grained Control)

```python
def complex_processing(query: str) -> dict:
    """Complex operation with nested spans."""
    
    with mlflow.start_span(name="complex_processing", span_type="AGENT") as span:
        span.set_inputs({"query": query})
        
        # Step 1: Intent classification
        with mlflow.start_span(name="classify_intent", span_type="CLASSIFIER") as intent_span:
            intent_span.set_inputs({"query": query})
            domains = classify(query)
            intent_span.set_outputs({"domains": domains})
            intent_span.set_attributes({"confidence": 0.95})
        
        # Step 2: Query Genie
        results = {}
        for domain in domains:
            with mlflow.start_span(name=f"genie_{domain}", span_type="TOOL") as genie_span:
                genie_span.set_inputs({"domain": domain, "query": query})
                result = query_genie(domain, query)
                genie_span.set_outputs({"result": result})
                results[domain] = result
        
        # Step 3: Synthesize
        with mlflow.start_span(name="synthesize", span_type="AGENT") as synth_span:
            synth_span.set_inputs({"results": results})
            final_response = synthesize(results)
            synth_span.set_outputs({"response": final_response})
        
        span.set_outputs({"response": final_response})
        span.set_attributes({"num_domains": len(domains)})
    
    return final_response
```

### Trace Storage Configuration

**File:** `src/agents/setup/configure_trace_storage.py`

```python
# Databricks notebook source
"""
Configure Unity Catalog trace storage for agent experiment.

Reference: https://docs.databricks.com/aws/en/mlflow3/genai/tracing/storage
"""

import mlflow
from databricks.sdk import WorkspaceClient

# Parameters
catalog = dbutils.widgets.get("catalog")
agent_schema = dbutils.widgets.get("agent_schema")

EXPERIMENT_PATH = "/Shared/health_monitor_agent_evaluation"
TRACE_TABLE_NAME = f"{catalog}.{agent_schema}.agent_traces"

# Set experiment
mlflow.set_experiment(EXPERIMENT_PATH)

# Get experiment ID
client = mlflow.MlflowClient()
experiment = client.get_experiment_by_name(EXPERIMENT_PATH)
experiment_id = experiment.experiment_id

# Configure trace storage
w = WorkspaceClient()

w.experiments.set_experiment_tag(
    experiment_id=experiment_id,
    key="mlflow.tracing.destination.catalog",
    value=catalog
)

w.experiments.set_experiment_tag(
    experiment_id=experiment_id,
    key="mlflow.tracing.destination.schema",
    value=agent_schema
)

w.experiments.set_experiment_tag(
    experiment_id=experiment_id,
    key="mlflow.tracing.destination.table",
    value="agent_traces"
)

print(f"‚úì Trace storage configured: {TRACE_TABLE_NAME}")
```

---

## Pattern 4: Multi-Agent Orchestration with Genie

### Multi-Domain Architecture

```
User Query
    ‚Üì
Orchestrator (Intent Classification)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Cost   ‚îÇSecurity ‚îÇPerf.    ‚îÇReliab.  ‚îÇQuality  ‚îÇ ‚Üê Domain Workers
‚îÇ  Agent  ‚îÇ Agent   ‚îÇAgent    ‚îÇAgent    ‚îÇAgent    ‚îÇ   (parallel)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îÇ         ‚îÇ         ‚îÇ         ‚îÇ         ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
                  Synthesizer (Response Composition)
                        ‚Üì
                  Final Response
```

### LangGraph Implementation

```python
from langgraph.graph import StateGraph, END
from typing import TypedDict, List

class AgentState(TypedDict):
    """State shared across nodes."""
    messages: List[dict]
    domains: List[str]
    domain_results: dict
    final_response: str


def build_multi_agent_graph() -> StateGraph:
    """Build LangGraph workflow for multi-agent orchestration."""
    
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("orchestrator", orchestrator_node)
    workflow.add_node("cost_worker", lambda s: cost_worker(s))
    workflow.add_node("security_worker", lambda s: security_worker(s))
    workflow.add_node("performance_worker", lambda s: performance_worker(s))
    workflow.add_node("reliability_worker", lambda s: reliability_worker(s))
    workflow.add_node("quality_worker", lambda s: quality_worker(s))
    workflow.add_node("synthesizer", synthesizer_node)
    
    # Entry point
    workflow.set_entry_point("orchestrator")
    
    # Conditional routing based on domains
    workflow.add_conditional_edges(
        "orchestrator",
        route_to_workers,  # Returns list of worker nodes to execute
        {
            "cost": "cost_worker",
            "security": "security_worker",
            "performance": "performance_worker",
            "reliability": "reliability_worker",
            "quality": "quality_worker",
        }
    )
    
    # All workers converge to synthesizer
    for worker in ["cost_worker", "security_worker", "performance_worker", 
                   "reliability_worker", "quality_worker"]:
        workflow.add_edge(worker, "synthesizer")
    
    # Synthesizer to end
    workflow.add_edge("synthesizer", END)
    
    return workflow


@mlflow.trace(name="orchestrator_node", span_type="AGENT")
def orchestrator_node(state: AgentState) -> AgentState:
    """Route query to relevant domain workers."""
    query = state["messages"][-1]["content"]
    
    # Classify intent
    domains = classify_intent(query)
    
    state["domains"] = domains
    state["domain_results"] = {}
    
    return state


def route_to_workers(state: AgentState) -> List[str]:
    """Conditional routing logic."""
    return [f"{domain}_worker" for domain in state["domains"]]


@mlflow.trace(name="cost_worker", span_type="AGENT")
def cost_worker(state: AgentState) -> AgentState:
    """Cost domain specialist."""
    query = state["messages"][-1]["content"]
    
    # Query Cost Genie
    genie_result = query_genie("cost", query)
    
    state["domain_results"]["cost"] = genie_result
    return state


@mlflow.trace(name="synthesizer_node", span_type="AGENT")
def synthesizer_node(state: AgentState) -> AgentState:
    """Synthesize multi-domain results."""
    query = state["messages"][-1]["content"]
    domain_results = state["domain_results"]
    
    # Synthesize response
    synthesized = synthesize_response(query, domain_results)
    
    state["final_response"] = synthesized
    state["messages"].append({"role": "assistant", "content": synthesized})
    
    return state
```

### Genie Tool Integration

```python
from databricks.agents import GenieAgent

def query_genie(domain: str, query: str) -> str:
    """
    Query Genie Space for real data.
    
    NO LLM FALLBACK - return explicit error if Genie fails.
    """
    with mlflow.start_span(name=f"genie_{domain}", span_type="TOOL") as span:
        span.set_inputs({"domain": domain, "query": query})
        
        try:
            # Get Genie Space ID
            genie_space_id = GENIE_SPACES.get(domain)
            if not genie_space_id:
                error_msg = f"Genie Space not configured for domain: {domain}"
                span.set_attributes({"error": error_msg})
                return f"## Error\n\n{error_msg}"
            
            # Query Genie
            genie = GenieAgent(space_id=genie_space_id)
            result = genie.invoke(query)
            
            span.set_outputs({"result": result})
            span.set_attributes({"source": "genie", "space_id": genie_space_id})
            
            return result
            
        except Exception as e:
            # ================================================================
            # CRITICAL: NO LLM FALLBACK!
            # Return explicit error instead of hallucinating fake data.
            # ================================================================
            error_msg = f"""## Genie Query Failed

**Domain:** {domain}
**Query:** {query}
**Error:** {str(e)}

I was unable to retrieve real data from the Databricks Genie Space.

**Note:** I will NOT generate fake data. All responses must come from real system tables via Genie."""
            
            span.set_attributes({"error": str(e), "fallback": "none"})
            span.set_outputs({"source": "error"})
            
            return error_msg
```

---

## Pattern 4: Adding Context to Traces (Tags & Metadata)

### Tags vs Metadata

**Reference:** [Microsoft Docs - Attach Custom Tags and Metadata](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/tracing/attach-tags/)

| Property | Mutability | Use For | Example |
|---|---|---|---|
| **Metadata** | Immutable (write-once) | Fixed information captured during execution | User ID, session ID, model version, environment |
| **Tags** | Mutable (can update) | Dynamic information that may change | User feedback, review status, quality assessments |

### Standard Metadata Fields

**Reference:** [Microsoft Docs - Add Context to Traces](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/tracing/add-context-to-traces)

MLflow provides standardized metadata fields that enable automatic filtering and grouping in the UI:

```python
import mlflow

mlflow.update_current_trace(
    metadata={
        # ========== USER & SESSION TRACKING ==========
        "mlflow.trace.user": user_id,           # Associate with specific user
        "mlflow.trace.session": session_id,     # Group multi-turn conversations
        
        # ========== DEPLOYMENT CONTEXT ==========
        "mlflow.source.type": "PRODUCTION",     # Environment (PRODUCTION, STAGING, DEV)
        "mlflow.source.name": "health-monitor-agent",  # Application name
        "mlflow.modelId": "v2.1.0",             # Application version
        
        # ========== GIT CONTEXT (auto-populated if in repo) ==========
        "mlflow.source.git.commit": commit_hash,   # Git commit hash
        "mlflow.source.git.branch": branch_name,   # Git branch
        "mlflow.source.git.repoURL": repo_url,     # Git repository URL
        
        # ========== REQUEST TRACKING ==========
        "client_request_id": request_id,        # Link to client request
        
        # ========== CUSTOM METADATA ==========
        "deployment_region": "us-west-2",       # Custom metadata
        "feature_flags": "new_routing,caching", # Application-specific
    }
)
```

### Tagging Pattern (Mutable Context)

```python
mlflow.update_current_trace(
    tags={
        # ========== QUERY CLASSIFICATION ==========
        "query_category": "cost_analysis",
        "domains": "cost,reliability",
        "confidence": "0.95",
        
        # ========== EXECUTION DETAILS ==========
        "streaming": "true",
        "genie_used": "true",
        "cache_hit": "false",
        
        # ========== QUALITY ASSESSMENT ==========
        "review_status": "approved",
        "data_quality": "high",
        
        # ========== BUSINESS CONTEXT ==========
        "customer_tier": "enterprise",
        "use_case": "cost_optimization",
    }
)
```

### Complete Context Pattern (Production Agent)

```python
@mlflow.trace(name="health_monitor_predict", span_type="AGENT")
def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:
    """Predict with comprehensive trace context."""
    
    # ========== EXTRACT CONTEXT FROM REQUEST ==========
    
    input_messages = [msg.model_dump() for msg in request.input]
    query = input_messages[-1].get("content", "")
    custom_inputs = request.custom_inputs or {}
    
    # User context
    user_id = custom_inputs.get("user_id", "unknown")
    session_id = custom_inputs.get("session_id", str(uuid.uuid4()))
    thread_id = custom_inputs.get("thread_id", str(uuid.uuid4()))
    
    # Request context
    client_request_id = custom_inputs.get("request_id", str(uuid.uuid4()))
    
    # Deployment context
    app_environment = os.environ.get("APP_ENVIRONMENT", "development")
    app_version = os.environ.get("APP_VERSION", "dev")
    deployment_region = os.environ.get("DEPLOYMENT_REGION", "unknown")
    endpoint_name = os.environ.get("ENDPOINT_NAME", "health-monitor-agent")
    
    # ========== UPDATE TRACE WITH METADATA (Immutable) ==========
    
    mlflow.update_current_trace(
        metadata={
            # User & Session (standard fields for UI filtering)
            "mlflow.trace.user": user_id,
            "mlflow.trace.session": session_id,
            
            # Deployment context
            "mlflow.source.type": app_environment.upper(),  # PRODUCTION, STAGING, DEV
            "mlflow.modelId": app_version,
            "client_request_id": client_request_id,
            
            # Custom application metadata
            "deployment_region": deployment_region,
            "endpoint_name": endpoint_name,
            "query_length": str(len(query)),
            "has_custom_inputs": str(bool(custom_inputs)),
        },
        tags={
            # Execution details (mutable - can update later)
            "user_id": user_id,         # Also in metadata, but tags are filterable
            "thread_id": thread_id,     # For conversation tracking
            "session_id": session_id,   # For multi-turn analysis
            "query_length": str(len(query)),
            "streaming": "pending",     # Can update to "true" later
        }
    )
    
    # ========== PROCESS QUERY ==========
    
    response_text, domains = self._process(query, custom_inputs)
    
    # ========== UPDATE TRACE WITH EXECUTION RESULTS (Tags) ==========
    
    mlflow.update_current_trace(tags={
        "domains_queried": ",".join(domains),
        "streaming": "false",  # Updated from "pending"
        "response_generated": "true",
    })
    
    # Return response
    return ResponsesAgentResponse(
        output=[self.create_text_output_item(text=response_text, id=str(uuid.uuid4()))],
        custom_outputs={
            "thread_id": thread_id,         # Return for client tracking
            "session_id": session_id,
            "domains_queried": domains,
        }
    )
```

### Safe Trace Update Helper

**Problem:** `mlflow.update_current_trace()` raises warnings during evaluation when no active trace exists.

**Solution:** Check for active trace before updating.

```python
def _safe_update_trace(metadata: dict = None, tags: dict = None) -> bool:
    """
    Safely update the current MLflow trace without triggering warnings.
    
    During evaluation testing, there may not be an active trace context.
    This helper checks for an active trace before attempting updates.
    
    Returns:
        True if update succeeded, False if no active trace.
    """
    try:
        # Check if there's an active trace
        current_span = mlflow.get_current_active_span()
        if current_span is None:
            return False
        
        # Safe to update
        mlflow.update_current_trace(metadata=metadata, tags=tags)
        return True
    except Exception:
        return False


# Usage in agent
_safe_update_trace(
    metadata={"mlflow.trace.user": user_id},
    tags={"query_category": "cost"}
)
```

### Updating Tags on Finished Traces

**After trace is logged, only tags can be updated (not metadata).**

**Reference:** [Microsoft Docs - Setting Tags on Finished Traces](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/tracing/attach-tags/#setting-tags-on-a-finished-trace)

```python
import mlflow

# Execute traced function
@mlflow.trace
def process_query(query):
    return f"Processed: {query}"

result = process_query("Why did costs spike?")

# ========== GET TRACE ID ==========
trace_id = mlflow.get_last_active_trace_id()

# ========== SET/UPDATE TAGS ON FINISHED TRACE ==========

# Add review status after human review
mlflow.set_trace_tag(
    trace_id=trace_id,
    key="review_status",
    value="approved"
)

# Add quality assessment after evaluation
mlflow.set_trace_tag(
    trace_id=trace_id,
    key="quality_score",
    value="0.85"
)

# ========== DELETE TAG ==========

mlflow.delete_trace_tag(
    trace_id=trace_id,
    key="quality_score"  # Remove tag
)
```

### Context Tracking for Multi-Turn Conversations

```python
class ConversationAgent(ResponsesAgent):
    """Agent with session and conversation tracking."""
    
    @mlflow.trace(name="predict", span_type="AGENT")
    def predict(self, request: ResponsesAgentRequest) -> ResponsesAgentResponse:
        """Predict with conversation context."""
        
        # Extract conversation identifiers
        custom_inputs = request.custom_inputs or {}
        user_id = custom_inputs.get("user_id", "anonymous")
        session_id = custom_inputs.get("session_id")
        thread_id = custom_inputs.get("thread_id")
        
        # Generate IDs if not provided
        if not session_id:
            session_id = str(uuid.uuid4())
        if not thread_id:
            thread_id = str(uuid.uuid4())
        
        # ‚úÖ CRITICAL: Use standard metadata fields for UI filtering
        mlflow.update_current_trace(
            metadata={
                "mlflow.trace.user": user_id,       # Standard user field
                "mlflow.trace.session": session_id, # Standard session field
                "turn_number": str(custom_inputs.get("turn_number", 1)),
            },
            tags={
                "thread_id": thread_id,
                "conversation_type": "multi_turn",
                "user_tier": custom_inputs.get("user_tier", "standard"),
            }
        )
        
        # Load conversation history using thread_id
        history = self._get_conversation_history(thread_id)
        
        # Process with context
        response = self._process_with_history(query, history)
        
        # Return with IDs for client tracking
        return ResponsesAgentResponse(
            output=[self.create_text_output_item(text=response, id=str(uuid.uuid4()))],
            custom_outputs={
                "session_id": session_id,
                "thread_id": thread_id,
                "turn_number": len(history) + 1,
            }
        )
```

### Environment-Specific Context

```python
import os

def add_deployment_context():
    """Add deployment environment context to trace."""
    
    # Extract from environment variables (not hardcoded)
    mlflow.update_current_trace(
        metadata={
            # Override automatic detection
            "mlflow.source.type": os.getenv("APP_ENVIRONMENT", "DEVELOPMENT"),
            "mlflow.source.name": os.getenv("APP_NAME", "health-monitor-agent"),
            "mlflow.modelId": os.getenv("APP_VERSION", "dev"),
            
            # Custom deployment metadata
            "deployment_id": os.getenv("DEPLOYMENT_ID", "unknown"),
            "deployment_region": os.getenv("AWS_REGION", "us-west-2"),
            "k8s_pod_name": os.getenv("HOSTNAME", "unknown"),
        },
        tags={
            # Feature flags (can change without redeployment)
            "feature_new_routing": os.getenv("FEATURE_NEW_ROUTING", "false"),
            "feature_caching": os.getenv("FEATURE_CACHING", "true"),
        }
    )
```

### Query Pattern for Trace Context

```python
import mlflow

# ========== SEARCH TRACES BY USER ==========

user_traces = mlflow.search_traces(
    filter_string="metadata.mlflow.trace.user = 'user@example.com'",
    max_results=100
)

# ========== SEARCH TRACES BY SESSION ==========

session_traces = mlflow.search_traces(
    filter_string="metadata.mlflow.trace.session = 'session_123'",
    order_by=["timestamp ASC"]  # Chronological order
)

# ========== SEARCH TRACES BY ENVIRONMENT ==========

prod_traces = mlflow.search_traces(
    filter_string="metadata.mlflow.source.type = 'PRODUCTION'",
    max_results=1000
)

# ========== SEARCH BY TAG ==========

reviewed_traces = mlflow.search_traces(
    filter_string="tags.review_status = 'approved'",
    max_results=50
)

# ========== COMPLEX FILTER ==========

filtered_traces = mlflow.search_traces(
    filter_string="""
        metadata.mlflow.trace.user = 'user@example.com'
        AND tags.domains LIKE '%cost%'
        AND metadata.mlflow.source.type = 'PRODUCTION'
    """,
    max_results=100
)
```

---

## Pattern 5: On-Behalf-Of (OBO) Authentication

### Model Serving with OBO

**OBO enables agents to query data on behalf of the calling user.**

```python
# ============================================================================
# AGENT IMPLEMENTATION (reads user identity from context)
# ============================================================================

class HealthMonitorAgent(ResponsesAgent):
    """Agent with on-behalf-of authentication support."""
    
    @mlflow.trace(name="predict", span_type="AGENT")
    def predict(self, context, model_input, params=None):
        """
        Execute agent on behalf of calling user.
        
        Context contains:
        - user_id: Calling user's email
        - workspace_id: Workspace where agent is deployed
        """
        # Extract user identity
        user_id = context.get("user_id", "unknown")
        
        # Log user for audit trail
        mlflow.update_current_trace(tags={"user_id": user_id})
        
        # Process query (Genie automatically uses OBO)
        result = self._process(model_input, user_id=user_id)
        
        return result


# ============================================================================
# SERVING ENDPOINT CONFIGURATION (enables OBO)
# ============================================================================

# File: resources/agents/agent_serving_endpoint.yml

resources:
  model_serving_endpoints:
    health_monitor_agent:
      name: health-monitor-agent
      config:
        served_entities:
          - name: current
            entity_name: ${var.catalog}.${var.agent_schema}.health_monitor_agent
            entity_version: "${var.agent_version}"
            workload_size: Small
            scale_to_zero_enabled: true
            
            # ================================================================
            # CRITICAL: Enable on-behalf-of authentication
            # This allows the agent to access data as the calling user.
            # Reference: https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html
            # ================================================================
            environment_vars:
              # Enable OBO for Genie
              DATABRICKS_USE_IDENTITY_PASSTHROUGH: "true"
              
              # Pass Genie Space IDs
              COST_GENIE_SPACE_ID: ${var.cost_genie_space_id}
              SECURITY_GENIE_SPACE_ID: ${var.security_genie_space_id}
              # ... more Genie IDs ...
        
        # Traffic configuration
        traffic_config:
          routes:
            - served_model_name: current
              traffic_percentage: 100
```

### Querying with OBO

```python
from databricks.sdk import WorkspaceClient

# Query agent endpoint (OBO automatically applied)
w = WorkspaceClient()

response = w.serving_endpoints.query(
    name="health-monitor-agent",
    inputs={
        "input": [{"role": "user", "content": "Show me failed jobs"}]
    }
)

# Agent queries Genie on behalf of calling user
# User sees only data they have permission to access
```

---

## Pattern 6: Visualization Hints (AI/BI Integration)

### Generating Visualization Metadata

```python
def generate_visualization_hints(domain: str, data: dict) -> dict:
    """
    Generate visualization hints for AI/BI dashboards.
    
    Returns metadata about chart type, axes, and formatting.
    """
    hints = {
        "visualization_type": None,
        "x_axis": None,
        "y_axis": None,
        "color_by": None,
        "sort_by": None,
        "format": {}
    }
    
    # Domain-specific hints
    if domain == "cost":
        if "time_series" in data:
            hints["visualization_type"] = "line_chart"
            hints["x_axis"] = "date"
            hints["y_axis"] = "cost_usd"
            hints["format"]["y_axis"] = {"type": "currency", "currency": "USD"}
        
        elif "top_n" in data:
            hints["visualization_type"] = "bar_chart"
            hints["x_axis"] = "job_name"
            hints["y_axis"] = "cost_usd"
            hints["sort_by"] = {"field": "cost_usd", "order": "desc"}
            hints["format"]["y_axis"] = {"type": "currency", "currency": "USD"}
    
    elif domain == "reliability":
        hints["visualization_type"] = "table"
        hints["columns"] = ["job_name", "failure_count", "last_failure_time"]
        hints["format"]["failure_count"] = {"type": "number"}
        hints["format"]["last_failure_time"] = {"type": "datetime"}
    
    return hints


# In agent response
response_text, visualization_hints = process_query(query)

return ResponsesAgentResponse(
    output=[self.create_text_output_item(text=response_text, id=str(uuid.uuid4()))],
    custom_outputs={
        "visualization_hints": visualization_hints,  # ‚úÖ For AI/BI dashboards
        "domain": domain,
    }
)
```

---

## Common Mistakes to Avoid

### ‚ùå DON'T: Use `messages` Instead of `input`

```python
# BAD: Legacy ChatAgent format
input_example = {
    "messages": [{"role": "user", "content": "..."}]  # ‚ùå Wrong!
}
```

### ‚úÖ DO: Use ResponsesAgent Format

```python
# GOOD: ResponsesAgent format
input_example = {
    "input": [{"role": "user", "content": "..."}]  # ‚úÖ Correct!
}
```

### ‚ùå DON'T: Return Dict from predict()

```python
# BAD: Dict output
def predict(self, request):
    return {"messages": [...]}  # ‚ùå Wrong!
```

### ‚úÖ DO: Return ResponsesAgentResponse

```python
# GOOD: ResponsesAgentResponse
def predict(self, request):
    return ResponsesAgentResponse(
        output=[self.create_text_output_item(...)]
    )
```

### ‚ùå DON'T: Pass Manual Signature

```python
# BAD: Manual signature breaks AI Playground
mlflow.pyfunc.log_model(
    python_model=agent,
    signature=my_custom_signature,  # ‚ùå Breaks auto-inference!
)
```

### ‚úÖ DO: Let MLflow Infer Signature

```python
# GOOD: Auto-inference
mlflow.pyfunc.log_model(
    python_model=agent,
    # NO signature parameter!
)
```

### ‚ùå DON'T: Hardcode Deployment Context

```python
# BAD: Hardcoded environment
mlflow.update_current_trace(
    metadata={"mlflow.source.type": "PRODUCTION"}  # ‚ùå Hardcoded!
)
```

### ‚úÖ DO: Use Environment Variables

```python
# GOOD: From environment
mlflow.update_current_trace(
    metadata={
        "mlflow.source.type": os.getenv("APP_ENVIRONMENT", "DEV")  # ‚úÖ Dynamic!
    }
)
```

### ‚ùå DON'T: Use Custom Fields for Standard Context

```python
# BAD: Custom field names for standard concepts
mlflow.update_current_trace(
    metadata={
        "user": "user123",          # ‚ùå Wrong field name!
        "conversation": "session1"  # ‚ùå Wrong field name!
    }
)
```

### ‚úÖ DO: Use Standard MLflow Fields

```python
# GOOD: Standard field names (enables UI filtering)
mlflow.update_current_trace(
    metadata={
        "mlflow.trace.user": "user123",       # ‚úÖ Standard!
        "mlflow.trace.session": "session1"    # ‚úÖ Standard!
    }
)
```

### ‚ùå DON'T: Try to Update Metadata on Finished Traces

```python
# BAD: Metadata is immutable after logging
trace_id = mlflow.get_last_active_trace_id()

mlflow.update_current_trace(
    metadata={"new_field": "value"}  # ‚ùå Won't work on finished trace!
)
```

### ‚úÖ DO: Use Tags for Post-Logging Updates

```python
# GOOD: Tags are mutable
trace_id = mlflow.get_last_active_trace_id()

mlflow.set_trace_tag(
    trace_id=trace_id,
    key="review_status",
    value="approved"  # ‚úÖ Works!
)
```

### ‚ùå DON'T: Update Trace Without Checking for Active Trace

```python
# BAD: Causes warnings during evaluation
def my_function():
    mlflow.update_current_trace(metadata={"key": "value"})  # ‚ùå May not have active trace!
```

### ‚úÖ DO: Safe Trace Update

```python
# GOOD: Check for active trace first
def my_function():
    try:
        current_span = mlflow.get_current_active_span()
        if current_span:
            mlflow.update_current_trace(metadata={"key": "value"})  # ‚úÖ Safe!
    except:
        pass  # No active trace, skip
```

---

## Validation Checklist

Before deploying agent:

### ResponsesAgent
- [ ] Agent inherits from `mlflow.pyfunc.ResponsesAgent`
- [ ] `predict()` accepts `request` parameter
- [ ] `predict()` returns `ResponsesAgentResponse`
- [ ] Input example uses `input` key (not `messages`)
- [ ] No `signature` parameter in `log_model()`

### Streaming
- [ ] `predict_stream()` implemented
- [ ] Yields `ResponsesAgentStreamEvent` objects
- [ ] Final `output_item.done` event sent
- [ ] `predict()` delegates to `predict_stream()` (code reuse)

### Tracing
- [ ] `@mlflow.trace` decorator on key functions
- [ ] Span types specified (`AGENT`, `TOOL`, `LLM`, etc.)
- [ ] Trace storage configured in Unity Catalog
- [ ] Traces visible in MLflow Experiment UI

### Trace Context (NEW!)
- [ ] Standard metadata fields used (`mlflow.trace.user`, `mlflow.trace.session`)
- [ ] Deployment context from environment variables (not hardcoded)
- [ ] Safe trace update helper for evaluation compatibility
- [ ] User ID and session ID returned in `custom_outputs`
- [ ] Tags used for mutable context, metadata for immutable
- [ ] Environment type set (`PRODUCTION`, `STAGING`, `DEV`)

### Multi-Agent
- [ ] LangGraph workflow defined
- [ ] Conditional routing based on intent
- [ ] Domain workers execute in parallel
- [ ] Synthesizer combines results
- [ ] No LLM fallback for Genie failures

### OBO Authentication
- [ ] `DATABRICKS_USE_IDENTITY_PASSTHROUGH=true` in serving config
- [ ] User identity logged to traces
- [ ] Genie queries use caller's permissions

### Visualization
- [ ] Visualization hints in `custom_outputs`
- [ ] Chart type and axes specified
- [ ] Format metadata included

---

## References

### Official Documentation - Core Patterns
- [ResponsesAgent](https://mlflow.org/docs/latest/genai/serving/responses-agent) - **Primary reference**
- [MLflow Tracing](https://mlflow.org/docs/latest/llms/tracing/index.html)
- [Unity Catalog Traces](https://docs.databricks.com/aws/en/mlflow3/genai/tracing/storage)
- [LangGraph](https://langchain-ai.github.io/langgraph/)
- [Model Serving OBO](https://docs.databricks.com/en/machine-learning/model-serving/create-manage-serving-endpoints.html)

### Official Documentation - Trace Context (NEW!)
- [Add Context to Traces](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/tracing/add-context-to-traces) - **Primary reference for metadata/tags**
- [Attach Custom Tags and Metadata](https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/tracing/attach-tags/) - Tags vs metadata
- [Standard Metadata Fields](https://mlflow.org/docs/latest/genai/tracing/track-environments-context/#reserved-standard-tags) - MLflow conventions
- [Search Traces Programmatically](https://mlflow.org/docs/latest/genai/tracing/search-traces/) - Query patterns

### Related Rules
- [30-mlflow-genai-evaluation.mdc](30-mlflow-genai-evaluation.mdc) - Evaluation patterns
- [31-lakebase-memory-patterns.mdc](31-lakebase-memory-patterns.mdc) - Memory patterns
- [32-prompt-registry-patterns.mdc](32-prompt-registry-patterns.mdc) - Prompt management
- [35-production-monitoring-patterns.mdc](35-production-monitoring-patterns.mdc) - Production monitoring

### Implementation Reference
- `src/agents/setup/log_agent_model.py` - Complete ResponsesAgent implementation with trace context
- `src/agents/orchestrator/graph.py` - LangGraph multi-agent workflow
- `src/agents/setup/configure_trace_storage.py` - Trace configuration
- `resources/agents/agent_serving_endpoint.yml` - OBO serving config
