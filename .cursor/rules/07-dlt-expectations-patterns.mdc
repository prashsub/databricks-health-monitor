---
description: Standard patterns for Delta Live Tables expectations, data quality checks, and quarantine tables in Silver layer
globs: src/altria_silver/**/*.py
alwaysApply: true
---

# Delta Live Tables Expectations Patterns

## Pattern Recognition
All Silver layer tables use consistent DLT expectations for data quality enforcement. This rule standardizes these patterns to ensure comprehensive data validation.

**Key Patterns:**
1. **Centralized Rules in `data_quality_rules.py`** - All rules defined in one place
2. **`@dlt.expect_all_or_drop()` Decorator** - Strict enforcement pattern
3. **Direct Publishing Mode** - Always use fully qualified table names with `get_source_table()` helper
4. **Pure Python Module** - `data_quality_rules.py` is importable (not a notebook)

## âš ï¸ DLT Direct Publishing Mode (Modern Pattern)

**DEPRECATED Patterns (Do NOT use):**
- âŒ `LIVE.` prefix for table references (e.g., `LIVE.bronze_transactions`)
- âŒ `target:` field in DLT pipeline configuration

**MODERN Pattern (Always use):**
- âœ… **Fully qualified table names**: `{catalog}.{schema}.{table_name}`
- âœ… **`schema:` field** in DLT pipeline configuration (not `target`)
- âœ… **Helper function** to build table names from configuration

### DLT Direct Publishing Mode Benefits
- Publish to multiple schemas and catalogs
- Better cross-schema data lineage
- Explicit catalog/schema control
- Forward-compatible with Unity Catalog

### Implementation Pattern

```python
# At the top of your DLT notebook
from pyspark.sql import SparkSession

def get_source_table(table_name, source_schema_key="bronze_schema"):
    """
    Helper function to get fully qualified table name from DLT configuration.
    
    Args:
        table_name: Name of the table (e.g., "bronze_transactions")
        source_schema_key: Configuration key for the schema (default: "bronze_schema")
    
    Returns:
        Fully qualified table name: "{catalog}.{schema}.{table_name}"
    """
    spark = SparkSession.getActiveSession()
    catalog = spark.conf.get("catalog")
    schema = spark.conf.get(source_schema_key)
    return f"{catalog}.{schema}.{table_name}"

# In your DLT table definition
import dlt
from pyspark.sql.functions import col, lit, coalesce, when, current_timestamp

@dlt.table(...)
def silver_transactions():
    return (
        dlt.read_stream(get_source_table("bronze_transactions"))
        .withColumn(...)
    )
```

### DLT Pipeline Configuration (YAML)

```yaml
resources:
  pipelines:
    silver_dlt_pipeline:
      name: "[${bundle.target}] Silver Layer Pipeline"
      
      # âœ… CORRECT: Use 'schema' (Direct Publishing Mode)
      catalog: ${var.catalog}
      schema: ${var.silver_schema}
      
      # âŒ WRONG: Don't use 'target' (deprecated)
      # target: ${var.catalog}.${var.silver_schema}
      
      # Pass configuration to notebooks
      configuration:
        catalog: ${var.catalog}
        bronze_schema: ${var.bronze_schema}
        silver_schema: ${var.silver_schema}
      
      serverless: true
      edition: ADVANCED
```

## ðŸŽ¯ Centralized DQ Rules (PREFERRED for All Tables)

**Use centralized rule configuration with `@dlt.expect_all_or_drop()` decorator.**

### Benefits of Centralization
- âœ… Single source of truth for all DQ rules
- âœ… Zero duplication across tables
- âœ… Easy to audit and update
- âœ… Consistent rule formatting
- âœ… Better documentation
- âœ… Importable as pure Python module

### Implementation Pattern

**1. Define all rules in `data_quality_rules.py` (pure Python, not notebook):**

```python
# src/altria_silver/data_quality_rules.py
def get_all_rules():
    """Returns all data quality rules with table name, rule name, and constraint."""
    return [
        {
            "table": "silver_store_dim",
            "name": "valid_store_number",
            "constraint": "store_number IS NOT NULL AND LENGTH(store_number) > 0"
        },
        {
            "table": "silver_store_dim",
            "name": "valid_store_name",
            "constraint": "store_name IS NOT NULL AND LENGTH(store_name) > 0"
        },
        {
            "table": "silver_transactions",
            "name": "valid_transaction_id",
            "constraint": "transaction_id IS NOT NULL AND LENGTH(transaction_id) > 0"
        },
        {
            "table": "silver_transactions",
            "name": "non_zero_quantity",
            "constraint": "quantity_sold != 0"
        },
        # ... more rules
    ]

def get_all_rules_for_table(table_name):
    """
    Get all DQ rules for a specific table in DLT @expect_all_or_drop format.
    
    Returns:
        dict: Dictionary mapping rule names to SQL constraints
    """
    all_rules = get_all_rules()
    table_rules = [rule for rule in all_rules if rule["table"] == table_name]
    return {rule["name"]: rule["constraint"] for rule in table_rules}
```

**2. Apply in DLT notebook:**

```python
# src/altria_silver/silver_store_dim.py
from data_quality_rules import get_all_rules_for_table

@dlt.table(
    name="silver_store_dim",
    comment="Silver store dimension with comprehensive data quality rules",
    table_properties={...},
    cluster_by_auto=True
)
@dlt.expect_all_or_drop(get_all_rules_for_table("silver_store_dim"))
def silver_store_dim():
    """
    Data Quality Rules (centralized in data_quality_rules.py):
    - All required fields must be present and non-empty
    - State must be valid 2-letter code
    - Coordinates must be within valid ranges
    """
    return dlt.read_stream(get_source_table("bronze_store_dim"))
```

**3. Modify rules centrally:**

```python
# Update in ONE place in data_quality_rules.py, applies everywhere
{
    "table": "silver_transactions",
    "name": "reasonable_quantity",
    "constraint": "quantity_sold BETWEEN -50 AND 100"  # Updated threshold
}
```

### Project Implementation

**Rules File:** `src/altria_silver/data_quality_rules.py`
- Pure Python module (not a notebook) for DLT imports
- Contains 65+ rules across 8 Silver tables
- Provides `get_critical_rules_for_table()` and `get_warning_rules_for_table()` functions
- Rules have explicit `severity` field (`critical` or `warning`)

## Expectation Strategy: Critical vs. Warning

**This project uses critical vs. warning separation for all data quality rules.**

### Why Critical vs. Warning?
- âœ… **Critical rules**: Must pass or record is dropped/quarantined (fail-fast for data integrity)
- âœ… **Warning rules**: Logged but record still passes (observability for business logic)
- âœ… **Centralized management**: All rules defined in one place (`data_quality_rules.py`)
- âœ… **Explicit severity**: Clear distinction between data integrity and business reasonableness
- âœ… **Clean Silver layer**: Only valid records make it through critical rules
- âœ… **Better observability**: Warning violations are tracked without data loss

### Severity Classification Guidelines

**CRITICAL (use `@dlt.expect_all_or_fail()`):**
- Primary key fields (must be present and non-empty)
- Foreign key fields (must be present for referential integrity)
- Required date fields (must be present and >= minimum valid date)
- Data type integrity (numeric fields must be numeric, etc.)
- Non-nullable business fields (quantity != 0, price > 0, etc.)

**WARNING (use `@dlt.expect_all()`):**
- Reasonableness checks (quantity between 1 and 10000)
- Recency checks (date within last 90 days)
- Format preferences (UPC length between 12 and 14)
- Coordinate ranges (latitude/longitude within valid bounds)
- Date logic checks (close date >= open date)

### Rule Definition Pattern

```python
# In data_quality_rules.py - all rules have severity field
def get_all_rules():
    return [
        # CRITICAL: Data integrity rules
        {
            "table": "silver_transactions",
            "name": "valid_transaction_id",
            "constraint": "transaction_id IS NOT NULL AND LENGTH(transaction_id) > 0",
            "severity": "critical"
        },
        {
            "table": "silver_transactions",
            "name": "non_zero_quantity",
            "constraint": "quantity_sold != 0",
            "severity": "critical"
        },
        
        # WARNING: Business logic rules
        {
            "table": "silver_transactions",
            "name": "reasonable_quantity",
            "constraint": "quantity_sold BETWEEN -20 AND 50",
            "severity": "warning"
        },
        {
            "table": "silver_transactions",
            "name": "recent_transaction",
            "constraint": "transaction_date >= CURRENT_DATE() - INTERVAL 365 DAYS",
            "severity": "warning"
        }
    ]

def get_critical_rules_for_table(table_name):
    """Get all critical DQ rules for a specific table."""
    all_rules = get_all_rules()
    table_rules = [rule for rule in all_rules if rule["table"] == table_name and rule.get("severity") == "critical"]
    return {rule["name"]: rule["constraint"] for rule in table_rules}

def get_warning_rules_for_table(table_name):
    """Get all warning DQ rules for a specific table."""
    all_rules = get_all_rules()
    table_rules = [rule for rule in all_rules if rule["table"] == table_name and rule.get("severity") == "warning"]
    return {rule["name"]: rule["constraint"] for rule in table_rules}
```

### Application Pattern

```python
from data_quality_rules import (
    get_critical_rules_for_table, 
    get_warning_rules_for_table
)

@dlt.table(...)
@dlt.expect_all_or_fail(get_critical_rules_for_table("silver_transactions"))
@dlt.expect_all(get_warning_rules_for_table("silver_transactions"))
def silver_transactions():
    """
    Data Quality Rules (centralized in data_quality_rules.py):
    
    CRITICAL (Record dropped if ANY fail):
    - Transaction ID must be present and non-empty
    - Store number must be present
    - Quantity cannot be zero
    - Price must be positive
    
    WARNING (Logged but record still passes):
    - Quantity should be reasonable (-20 to 50 units)
    - Price should be reasonable ($0.01 to $500)
    - Transaction should be recent (within 1 year)
    """
    return dlt.read_stream(get_source_table("bronze_transactions"))
```

## Standard Expectation Patterns by Data Type

### ID/Key Fields
```python
@dlt.expect_or_fail("valid_<field>_id", "<field>_id IS NOT NULL AND LENGTH(<field>_id) > 0")
@dlt.expect_or_fail("unique_<field>_id", "COUNT(*) OVER (PARTITION BY <field>_id) = 1")  # If needed
```

### Dates and Timestamps
```python
@dlt.expect_or_fail("valid_<field>_date", "<field>_date IS NOT NULL AND <field>_date >= '2020-01-01'")
@dlt.expect("reasonable_<field>_date", "<field>_date BETWEEN CURRENT_DATE() - INTERVAL 30 DAYS AND CURRENT_DATE() + INTERVAL 90 DAYS")
@dlt.expect("date_sequence", "end_date IS NULL OR end_date >= start_date")
```

### Numeric Fields
```python
@dlt.expect_or_fail("positive_<field>", "<field> > 0")
@dlt.expect_or_fail("non_negative_<field>", "<field> >= 0")
@dlt.expect("reasonable_<field>_range", "<field> BETWEEN <min> AND <max>")
```

### String Fields
```python
@dlt.expect_or_fail("valid_<field>", "<field> IS NOT NULL AND LENGTH(<field>) >= <min_length>")
@dlt.expect("valid_<field>_format", "<field> RLIKE '<regex_pattern>'")
```

### Enum/Category Fields
```python
@dlt.expect_all({
    "valid_<field>": "<field> IN ('Value1', 'Value2', 'Value3') OR <field> IS NULL"
})
```

### Referential Integrity
```python
@dlt.expect("has_<dimension>_reference", "<fk_field> IS NOT NULL")
# Note: Actual foreign key validation happens in dq_referential_integrity view
```

## Quarantine Table Pattern

**ALWAYS create a quarantine table** for each fact table with critical expectations.

```python
@dlt.table(
    name="<table_name>_quarantine",
    comment="""LLM: Quarantine table for <entity> that failed critical data quality checks. 
    Records here require manual review and remediation before promotion to Silver.""",
    table_properties={
        "quality": "quarantine",
        "delta.enableChangeDataFeed": "true",
        "delta.enableRowTracking": "true",
        "delta.enableDeletionVectors": "true",
        "delta.autoOptimize.autoCompact": "true",
        "delta.autoOptimize.optimizeWrite": "true",
        "delta.tuneFileSizesForRewrites": "true",
        "layer": "silver",
        "source_table": "<bronze_table_name>",
        "domain": "<domain>",
        "entity_type": "quarantine",
        "contains_pii": "<inherit_from_main_table>",
        "data_classification": "<inherit_from_main_table>"
    },
    cluster_by_auto=True
)
def <table_name>_quarantine():
    """
    Quarantine table for records that fail critical validations.
    
    This table captures records that:
    - Have invalid or missing critical fields
    - Have impossible business values
    - Fail referential integrity checks
    
    These records should be reviewed, corrected at source, and re-ingested.
    """
    return (
        dlt.read_stream("LIVE.<bronze_table_name>")
        .filter("""
            <field> IS NULL OR
            <field> < <invalid_value> OR
            <other_failure_conditions>
        """)
        .withColumn("quarantine_reason",
            when(col("<field>").isNull(), "<Description of failure>")
            .when(col("<field>") < <value>, "<Description>")
            .otherwise("Unknown validation failure"))
        .withColumn("quarantine_timestamp", current_timestamp())
    )
```

## Derived Field Pattern

Always calculate derived fields in the main table function:

```python
def silver_<entity>():
    return (
        dlt.read_stream(get_source_table("bronze_<entity>"))
        # Calculate derived fields
        .withColumn("total_<field>",
                   coalesce(col("field1"), lit(0)) + coalesce(col("field2"), lit(0)))
        .withColumn("is_<flag>", 
                   when(col("field") > <threshold>, True).otherwise(False))
        .withColumn("processed_timestamp", current_timestamp())
        .withColumn("<entity>_business_key",
                   sha2(concat_ws("||", col("key1"), col("key2")), 256))
    )
```

## Complete Example: Centralized Rules with expect_all_or_drop

**Step 1: Define rules in `data_quality_rules.py`:**

```python
# src/altria_silver/data_quality_rules.py
def get_all_rules():
    return [
        # Silver Transactions Rules
        {
            "table": "silver_transactions",
            "name": "valid_transaction_id",
            "constraint": "transaction_id IS NOT NULL AND LENGTH(transaction_id) > 0"
        },
        {
            "table": "silver_transactions",
            "name": "valid_store_number",
            "constraint": "store_number IS NOT NULL"
        },
        {
            "table": "silver_transactions",
            "name": "valid_transaction_date",
            "constraint": "transaction_date IS NOT NULL AND transaction_date >= '2020-01-01'"
        },
        {
            "table": "silver_transactions",
            "name": "non_zero_quantity",
            "constraint": "quantity_sold != 0"
        },
        {
            "table": "silver_transactions",
            "name": "valid_final_price",
            "constraint": "final_sales_price > 0"
        },
        {
            "table": "silver_transactions",
            "name": "reasonable_quantity",
            "constraint": "quantity_sold BETWEEN -20 AND 50"
        },
        {
            "table": "silver_transactions",
            "name": "reasonable_price",
            "constraint": "final_sales_price BETWEEN 0.01 AND 500.00"
        },
        # ... more rules for other tables
    ]

def get_all_rules_for_table(table_name):
    """Get all DQ rules for a specific table."""
    all_rules = get_all_rules()
    table_rules = [rule for rule in all_rules if rule["table"] == table_name]
    return {rule["name"]: rule["constraint"] for rule in table_rules}
```

**Step 2: Apply in DLT notebook:**

```python
# src/altria_silver/silver_transactions.py
from data_quality_rules import get_all_rules_for_table

@dlt.table(
    name="silver_transactions",
    comment="""LLM: Silver layer streaming fact table with comprehensive data quality rules""",
    table_properties={
        "quality": "silver",
        "delta.enableChangeDataFeed": "true",
        "delta.enableRowTracking": "true",
        "delta.enableDeletionVectors": "true",
        "delta.autoOptimize.autoCompact": "true",
        "delta.autoOptimize.optimizeWrite": "true",
        "layer": "silver",
        "source_table": "bronze_transactions",
        "domain": "sales"
    },
    cluster_by_auto=True
)
@dlt.expect_all_or_drop(get_all_rules_for_table("silver_transactions"))
def silver_transactions():
    """
    Data Quality Rules (centralized in data_quality_rules.py):
    - Transaction ID, store number, date must be present
    - Quantity cannot be zero, price must be positive
    - Reasonableness checks for quantity and price ranges
    """
    return (
        dlt.read_stream(get_source_table("bronze_transactions"))
        .withColumn("total_discount", 
                   coalesce(col("multi_unit_discount"), lit(0)) + 
                   coalesce(col("coupon_discount"), lit(0)))
        .withColumn("is_return", when(col("quantity_sold") < 0, True).otherwise(False))
        .withColumn("processed_timestamp", current_timestamp())
    )
```

**Benefits:**
- âœ… One decorator vs many inline rules
- âœ… Update rules in ONE file, applies everywhere
- âœ… Easy to audit all rules at once
- âœ… No duplication across notebooks
- âœ… Strict enforcement with `expect_all_or_drop`

## Data Quality Monitoring Views

For each set of Silver tables, create monitoring views:

```python
@dlt.view(
    name="dq_<entity>_metrics",
    comment="""LLM: Real-time data quality metrics for <entity> showing record counts, 
    validation pass rates, quarantine rates, and business rule compliance"""
)
def dq_<entity>_metrics():
    """Calculate DQ metrics"""
    silver = dlt.read("silver_<entity>")
    quarantine = dlt.read("silver_<entity>_quarantine")
    
    return (
        silver.agg(count("*").alias("silver_count"))
        .crossJoin(quarantine.agg(count("*").alias("quarantine_count")))
        .withColumn("total_records", col("silver_count") + col("quarantine_count"))
        .withColumn("pass_rate", (col("silver_count") / col("total_records")) * 100)
        .withColumn("metric_timestamp", current_timestamp())
    )
```

## Validation Checklist

When creating Silver tables with expectations:

### Rule Definition
- [ ] All rules defined in `data_quality_rules.py`
- [ ] Each rule has table name, rule name, and constraint
- [ ] Rule names are clear and descriptive (not "check1", "rule_a")
- [ ] Constraints use proper SQL syntax

### Data Quality Rules
- [ ] All primary keys have NOT NULL and LENGTH validations
- [ ] All required fields have NOT NULL checks
- [ ] Numeric fields have range validations (positive, non-negative, or reasonable ranges)
- [ ] Dates have minimum date checks (e.g., >= '2020-01-01')
- [ ] Business logic has reasonableness checks (quantity ranges, price ranges)

### DLT Notebook Implementation
- [ ] Import statement added: `from data_quality_rules import get_all_rules_for_table`
- [ ] Decorator applied: `@dlt.expect_all_or_drop(get_all_rules_for_table("table_name"))`
- [ ] Table properties include all required metadata
- [ ] `cluster_by_auto=True` is set
- [ ] Helper function `get_source_table()` used for Bronze references

### Quarantine & Monitoring
- [ ] Consider if quarantine table is needed (optional, for specific failure tracking)
- [ ] DQ monitoring view created for metrics tracking

## Common Mistakes to Avoid

### âŒ Mistake 1: No expectations at all
```python
@dlt.table(name="silver_data")
def silver_data():
    return dlt.read_stream(get_source_table("bronze_data"))
```

### âŒ Mistake 2: Unclear rule names
```python
# In data_quality_rules.py
{
    "table": "silver_data",
    "name": "check1",  # âŒ Unclear!
    "constraint": "field IS NOT NULL"
},
{
    "table": "silver_data",
    "name": "rule_a",  # âŒ Unclear!
    "constraint": "amount > 0"
}
```

### âŒ Mistake 3: Inline decorators instead of centralized rules
```python
# âŒ DON'T: Duplicates logic, hard to maintain
@dlt.table(name="silver_data", ...)
@dlt.expect_or_fail("valid_id", "id IS NOT NULL")
@dlt.expect_or_fail("valid_store", "store_number IS NOT NULL")
@dlt.expect_or_fail("valid_amount", "amount > 0")
@dlt.expect("reasonable_amount", "amount BETWEEN 0 AND 1000000")
def silver_data():
    return dlt.read_stream(get_source_table("bronze_data"))
```

### âŒ Mistake 4: Not importing the helper function
```python
# âŒ Missing import
@dlt.table(name="silver_data", ...)
@dlt.expect_all_or_drop(get_all_rules_for_table("silver_data"))  # NameError!
def silver_data():
    return dlt.read_stream(get_source_table("bronze_data"))
```

### âœ… CORRECT: Centralized rules with expect_all_or_drop
```python
# In data_quality_rules.py (define once, use everywhere)
{
    "table": "silver_data",
    "name": "valid_primary_key",  # âœ… Clear name
    "constraint": "id IS NOT NULL AND LENGTH(id) > 0"
},
{
    "table": "silver_data",
    "name": "valid_store_number",  # âœ… Clear name
    "constraint": "store_number IS NOT NULL"
},
{
    "table": "silver_data",
    "name": "valid_amount",
    "constraint": "amount > 0"
},

# In silver_data.py (clean and simple)
from data_quality_rules import get_all_rules_for_table  # âœ… Import first!

@dlt.table(name="silver_data", ...)
@dlt.expect_all_or_drop(get_all_rules_for_table("silver_data"))  # âœ… All rules applied!
def silver_data():
    return dlt.read_stream(get_source_table("bronze_data"))
```

## References

### Official Databricks Documentation
- [DLT Expectations](https://docs.databricks.com/aws/en/dlt/expectations)
- [DLT Expectation Patterns](https://docs.databricks.com/aws/en/dlt/expectation-patterns)
- [Data Quality Monitoring](https://docs.databricks.com/aws/en/dlt/observability)

### Project Files
- [DQ Rules File](../src/altria_silver/data_quality_rules.py) - Centralized rules for all Silver tables
- [Silver Dimensions Notebook](../src/altria_silver/silver_dimensions.py) - Example of centralized rules usage
- [Silver Transactions Notebook](../src/altria_silver/silver_transactions.py) - Example of centralized rules usage
