# Bronze Layer Streaming Pipeline — Delta Sinks (Serverless)
# Reference: https://github.com/prashsub/databricks-system-tables-archival
#
# Uses pyspark.pipelines (SDP) with dp.create_sink + dp.append_flow to write
# to persistent Delta sink tables. Full refresh is SAFE — re-appends data
# without deleting existing rows; a separate dedup step cleans up duplicates.
#
# Ingests 24 streaming-capable system tables from 7 domains:
# - access (6): audit, clean_room_events, column_lineage, inbound_network, outbound_network, table_lineage
# - billing (1): usage
# - compute (4): clusters, node_timeline, warehouse_events, warehouses
# - lakeflow (6): job_run_timeline, job_task_run_timeline, job_tasks, jobs, pipelines, pipeline_update_timeline
# - marketplace (2): listing_funnel_events, listing_access_events
# - mlflow (3): experiments_latest, runs_latest, run_metrics_history
# - serving (2): served_entities, endpoint_usage
#
# Previously disabled tables (inbound_network, pipeline_update_timeline) are
# now enabled via responseFormat=delta to handle Deletion Vectors.

resources:
  pipelines:
    bronze_streaming_pipeline:
      name: "[${bundle.target}] Health Monitor - Bronze Streaming Pipeline (Delta Sinks)"
      
      # Pipeline root folder (Lakeflow Pipelines Editor best practice)
      root_path: ../../../src/pipelines/bronze/streaming
      
      # DLT Direct Publishing Mode (Modern Pattern)
      catalog: ${var.catalog}
      schema: ${var.system_bronze_schema}
      
      # Single Python file — defines all 24 sinks + flows
      libraries:
        - file:
            path: ../../../src/pipelines/bronze/streaming/streaming_archive.py
      
      # Pipeline Configuration (read by streaming_archive.py via spark.conf.get)
      configuration:
        catalog: ${var.catalog}
        bronze_schema: ${var.system_bronze_schema}
        pipelines.enableTrackHistory: "true"
      
      # Serverless Compute
      serverless: true
      
      # Photon Engine
      photon: true
      
      # Channel (CURRENT = latest features)
      channel: CURRENT
      
      # Continuous vs Triggered
      continuous: false
      
      # Development Mode (faster iteration, auto-recovery)
      development: true
      
      # Edition (ADVANCED for sinks, expectations, SCD, etc.)
      edition: ADVANCED
      
      # Notifications
      notifications:
        - alerts:
            - on-update-failure
            - on-update-fatal-failure
            - on-flow-failure
          email_recipients:
            - data-engineering@company.com
      
      # Tags
      tags:
        environment: ${bundle.target}
        project: databricks_health_monitor
        layer: bronze
        pipeline_type: delta_sinks
        compute_type: serverless
        domains: "7"
        streaming_tables: "24"
      
      # Permissions
      permissions:
        - level: CAN_VIEW
          group_name: users
