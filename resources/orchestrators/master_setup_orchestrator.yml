# Master Setup Orchestrator (Layer 3)
# ====================================
#
# TOP-LEVEL ORCHESTRATOR: Complete infrastructure initialization
# 
# Architecture:
#   Layer 3 (This job): Master Orchestrator
#   Layer 2 (Composite): Semantic, Monitoring, ML Layer jobs
#   Layer 1 (Atomic): Individual notebook jobs (TVF, Metric View, etc.)
#
# Pattern: Each layer references the layer below via run_job_task
# NO notebooks are called directly - only job references
# This enables atomic testing at each layer
#
# Execution Flow:
#   1. Bronze Setup (atomic) - Non-streaming tables + DQ rules
#   2. Gold Setup (atomic) - 39 tables + constraints
#   3. Semantic Layer Setup (composite) → TVF + Metric View jobs
#   4. Monitoring Layer Setup (atomic) → Lakehouse Monitoring job
#   5. Wait for Monitor Tables (20 min) → Allow async table creation
#   6. Monitoring Documentation (atomic) → Add Genie-friendly descriptions
#   7. Alerting Layer Setup (composite) → Alert tables + deployment
#   8. ML Feature Tables (atomic) → Create empty feature tables
#
# NOTE: ML Training runs separately (weekly schedule) after data exists.
#       Setup only creates feature table infrastructure.
#
# Total Infrastructure:
#   - Bronze: 8 non-streaming tables + DQ rules
#   - Gold: 39 domain tables + constraints
#   - Semantic: 60 TVFs + 10 Metric Views
#   - Monitoring: 8 Lakehouse Monitors + Genie-friendly descriptions
#   - Alerting: 56 SQL Alerts
#   - ML: Feature tables (training runs separately)
#
# Usage:
#   databricks bundle run -t dev master_setup_orchestrator
#
# Expected Duration: 2-3 hours
# Run Once: Initial deployment only

resources:
  jobs:
    master_setup_orchestrator:
      name: "[${bundle.target}] Health Monitor - Master Setup Orchestrator"
      description: "Complete setup: Bronze → Gold → Semantic → Monitoring → Alerting → ML (references composite jobs)"
      
      # Environment for wait task (only task that needs notebook_task)
      environments:
        - environment_key: default
          spec:
            environment_version: "4"
      
      tasks:
        # ================================================================
        # PHASE 1: DATA LAYER SETUP
        # ================================================================
        
        # Step 1: Bronze Setup (atomic job)
        - task_key: bronze_setup
          run_job_task:
            job_id: ${resources.jobs.bronze_setup_job.id}
        
        # Step 2: Gold Setup (atomic job)
        - task_key: gold_setup
          depends_on:
            - task_key: bronze_setup
          run_job_task:
            job_id: ${resources.jobs.gold_setup_job.id}
        
        # ================================================================
        # PHASE 2: SEMANTIC LAYER SETUP (requires Gold tables)
        # ================================================================
        
        # Step 3: Semantic Layer (composite job → TVF + Metric View jobs)
        - task_key: semantic_layer_setup
          depends_on:
            - task_key: gold_setup
          run_job_task:
            job_id: ${resources.jobs.semantic_layer_setup_job.id}
        
        # ================================================================
        # PHASE 3: MONITORING LAYER SETUP (requires Gold tables)
        # ================================================================
        
        # Step 4: Lakehouse Monitoring Setup (atomic job - creates 8 monitors)
        - task_key: monitoring_layer_setup
          depends_on:
            - task_key: gold_setup
          run_job_task:
            job_id: ${resources.jobs.lakehouse_monitoring_setup_job.id}
        
        # Step 5: Wait for Monitor Tables (20 min conservative delay)
        # Lakehouse Monitors create output tables asynchronously
        # Documentation job needs these tables to exist before adding descriptions
        - task_key: wait_for_monitor_tables
          depends_on:
            - task_key: monitoring_layer_setup
          environment_key: default
          notebook_task:
            notebook_path: ../../src/monitoring/wait_for_monitors.py
            base_parameters:
              wait_minutes: "20"
        
        # Step 6: Document Monitoring Tables (atomic job - adds Genie descriptions)
        - task_key: monitoring_documentation
          depends_on:
            - task_key: wait_for_monitor_tables
          run_job_task:
            job_id: ${resources.jobs.lakehouse_monitoring_document_job.id}

        # ================================================================
        # PHASE 4: ALERTING LAYER SETUP (requires Gold tables)
        # ================================================================

        # Step 7: Alerting Layer (composite job → Alert tables + deployment)
        - task_key: alerting_layer_setup
          depends_on:
            - task_key: gold_setup
          run_job_task:
            job_id: ${resources.jobs.alerting_layer_setup_job.id}
        
        # ================================================================
        # PHASE 5: ML FEATURE TABLES (infrastructure only)
        # ================================================================
        
        # Step 8: ML Feature Tables (atomic job - creates empty feature tables)
        # NOTE: ML Training runs separately on weekly schedule after data exists
        - task_key: ml_feature_tables
          depends_on:
            - task_key: gold_setup
          run_job_task:
            job_id: ${resources.jobs.ml_feature_pipeline.id}
      
      # Timeout: 4 hours for complete setup
      timeout_seconds: 14400
      
      # Email notifications
      email_notifications:
        on_start:
          - data-engineering@company.com
        on_failure:
          - data-engineering@company.com
        on_success:
          - data-engineering@company.com
      
      # Tags
      tags:
        environment: ${bundle.target}
        project: databricks_health_monitor
        layer: all
        job_type: setup
        job_level: orchestrator
        orchestrator: "master"
        compute_type: serverless
        execution_type: "one_time"
