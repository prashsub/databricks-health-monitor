# Agent Framework Setup Job
# ===========================================================================
# Setup pipeline for the Health Monitor Agent:
#   1. Create infrastructure (schemas, tables, volumes)
#   2. Register prompts to MLflow Prompt Registry
#   2b. Register scorers for production monitoring
#   2c. Create evaluation datasets (synthetic + manual)
#   3. Log agent model to Unity Catalog
#   4. Run initial evaluation (sanity check)
#
# NOTE: Serving endpoint is NOT created here!
# Endpoint creation is handled by agent_deployment_job which implements
# the proper MLflow 3.0 Deployment Job pattern:
#   - Evaluate → Check Thresholds → Create Endpoint (gated)
#
# Reference: https://docs.databricks.com/aws/en/mlflow/deployment-job
#
# Single Schema (avoids sprawl):
#   Dev: prashanth_subrahmanyam_catalog.dev_<user>_system_gold_agent
#   Prod: main.system_gold_agent
#
# Contains: Models, Tables (structured), Volumes (unstructured)
# ===========================================================================

resources:
  jobs:
    agent_setup_job:
      name: "[${bundle.target}] Health Monitor - Agent Setup"
      description: >
        Agent setup pipeline: Creates infrastructure, registers prompts and scorers,
        logs model to Unity Catalog, runs initial evaluation. Serving endpoint
        creation is handled by agent_deployment_job (gated by evaluation).
      
      # Different environments for different task requirements
      environments:
        # Minimal env for SQL-only tasks
        - environment_key: sql_only
          spec:
            environment_version: "4"
        
        # MLflow 3.0+ env for prompt registry and scorers
        # REQUIRES: MLflow 3.0+ for mlflow.genai module
        # This is a HARD requirement - job WILL FAIL without it
        - environment_key: mlflow_env
          spec:
            environment_version: "4"
            dependencies:
              - "mlflow>=3.0.0"  # REQUIRED for mlflow.genai (Prompt Registry, Scorers)
        
        # Full env for agent model logging
        # REQUIRES: MLflow 3.0+ for mlflow.genai
        # Note: langchain-databricks is pre-installed on Databricks runtime
        - environment_key: agent_env
          spec:
            environment_version: "4"
            dependencies:
              - "mlflow>=3.0.0"  # REQUIRED for mlflow.genai
              - langchain
              - langchain-core
              - langgraph
              - databricks-sdk
        
        # Databricks Agents env for synthetic dataset generation
        - environment_key: agents_env
          spec:
            environment_version: "4"
            dependencies:
              - databricks-agents
              - "mlflow>=3.0.0"  # Required for mlflow.genai
      
      tasks:
        # ==================================================================
        # Task 1: Create Infrastructure (schemas, tables, volumes)
        # ==================================================================
        - task_key: create_agent_infrastructure
          environment_key: sql_only
          notebook_task:
            notebook_path: ../../src/agents/setup/create_schemas.py
            base_parameters:
              catalog: ${var.catalog}
              agent_schema: ${var.agent_schema}
        
        # ==================================================================
        # Task 2: Register Prompts to MLflow Prompt Registry
        # ==================================================================
        - task_key: register_prompts
          depends_on:
            - task_key: create_agent_infrastructure
          environment_key: mlflow_env
          notebook_task:
            notebook_path: ../../src/agents/setup/register_prompts.py
            base_parameters:
              catalog: ${var.catalog}
              agent_schema: ${var.agent_schema}
        
        # ==================================================================
        # Task 2b: Register Scorers for Production Monitoring
        # Registers built-in (Relevance, Safety) and custom scorers
        # These run automatically in background on sampled traces
        # ==================================================================
        - task_key: register_scorers
          depends_on:
            - task_key: create_agent_infrastructure
          environment_key: mlflow_env
          notebook_task:
            notebook_path: ../../src/agents/setup/register_scorers.py
            base_parameters:
              catalog: ${var.catalog}
              agent_schema: ${var.agent_schema}
        
        # ==================================================================
        # Task 2c: Create Evaluation Datasets (Synthetic + Manual)
        # Uses generate_evals_df from databricks-agents to create datasets
        # that appear in MLflow UI under Datasets tab
        # Reference: https://learn.microsoft.com/en-us/azure/databricks/generative-ai/agent-evaluation/synthesize-evaluation-set
        # ==================================================================
        - task_key: create_evaluation_datasets
          depends_on:
            - task_key: create_agent_infrastructure
          environment_key: agents_env
          notebook_task:
            notebook_path: ../../src/agents/setup/create_evaluation_dataset.py
            base_parameters:
              catalog: ${var.catalog}
              agent_schema: ${var.agent_schema}
              num_evals: "50"
        
        # ==================================================================
        # Task 3: Log Agent Model to Unity Catalog
        # ==================================================================
        - task_key: log_agent_model
          depends_on:
            - task_key: register_prompts
            - task_key: register_scorers
            - task_key: create_evaluation_datasets
          environment_key: agent_env
          notebook_task:
            notebook_path: ../../src/agents/setup/log_agent_model.py
            base_parameters:
              catalog: ${var.catalog}
              agent_schema: ${var.agent_schema}
        
        # ==================================================================
        # Task 4: Run Initial Evaluation (Pre-deployment validation)
        # 
        # This is a quick sanity check. Full evaluation with gated
        # endpoint creation happens in agent_deployment_job.
        # ==================================================================
        - task_key: run_evaluation
          depends_on:
            - task_key: log_agent_model
          environment_key: mlflow_env
          notebook_task:
            notebook_path: ../../src/agents/setup/run_evaluation.py
            base_parameters:
              catalog: ${var.catalog}
              agent_schema: ${var.agent_schema}
        
        # ==================================================================
        # NOTE: Serving endpoint is NOT created here!
        # 
        # Endpoint creation is handled by agent_deployment_job which:
        #   1. Runs comprehensive evaluation
        #   2. Checks quality thresholds
        #   3. ONLY creates/updates endpoint IF evaluation passes
        #
        # This is the proper MLflow 3.0 Deployment Job pattern.
        # Reference: https://docs.databricks.com/aws/en/mlflow/deployment-job
        # ==================================================================
      
      # Timeout for model logging and initial evaluation
      timeout_seconds: 1800  # 30 minutes (no endpoint creation here)
      
      email_notifications:
        on_failure:
          - data-engineering@company.com
      
      tags:
        environment: ${bundle.target}
        project: databricks_health_monitor
        layer: agent
        job_level: atomic
        job_type: setup
