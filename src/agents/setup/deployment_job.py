# Databricks notebook source
# ===========================================================================
# MLflow 3 Deployment Job for Agent Evaluation and Promotion
# ===========================================================================
"""
MLflow 3 Deployment Job implementation per Databricks best practices.

This notebook implements a deployment job that:
1. Triggers on new model version creation
2. Runs comprehensive evaluation
3. Supports approval workflows
4. Promotes models through stages

References:
- https://docs.databricks.com/aws/en/mlflow/deployment-job
- https://docs.databricks.com/aws/en/mlflow/deployment-job#integration-with-mlflow-3-model-tracking
- https://docs.databricks.com/aws/en/notebooks/source/mlflow/deployment-jobs/evaluation_genai.html
"""

# COMMAND ----------

import mlflow
from mlflow import MlflowClient
import pandas as pd
from typing import Dict, Any, List, Optional, Union
from datetime import datetime
import json
import os

# ===========================================================================
# Import official MLflow GenAI scorers
# Reference: https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/eval-monitor/custom-scorers
# ===========================================================================
try:
    from mlflow.genai.scorers import scorer
    from mlflow.entities import Feedback, AssessmentSource
    print("‚úì MLflow 3.0 GenAI scorers imported successfully")
    MLFLOW_GENAI_AVAILABLE = True
except ImportError as e:
    print(f"‚ö†Ô∏è MLflow 3.0 GenAI not available: {e}")
    print("   Falling back to basic evaluation...")
    MLFLOW_GENAI_AVAILABLE = False
    
    # Fallback: Define minimal scorer decorator
    def scorer(func):
        func._is_scorer = True
        return func
    
    class Feedback:
        def __init__(self, value=None, rationale="", name=None, error=None):
            self.value = value
            self.rationale = rationale
            self.name = name
            self.error = error

# COMMAND ----------

# Parameters
# REQUIRED for MLflow Deployment Job integration:
# - model_name: Full Unity Catalog path (catalog.schema.model)
# - model_version: Version number (empty = latest)
# Reference: https://docs.databricks.com/aws/en/mlflow/deployment-job
dbutils.widgets.text("model_name", "prashanth_subrahmanyam_catalog.dev_prashanth_subrahmanyam_system_gold_agent.health_monitor_agent")
dbutils.widgets.text("model_version", "")  # Empty = latest
dbutils.widgets.text("promotion_target", "staging")  # staging or production
dbutils.widgets.text("endpoint_name", "health_monitor_agent_dev")  # Serving endpoint name
# Optional: legacy params for backward compatibility
dbutils.widgets.text("catalog", "prashanth_subrahmanyam_catalog")
dbutils.widgets.text("agent_schema", "dev_prashanth_subrahmanyam_system_gold_agent")

# Get model_name - prefer the explicit parameter (REQUIRED for deployment job)
model_name_param = dbutils.widgets.get("model_name")
catalog = dbutils.widgets.get("catalog")
agent_schema = dbutils.widgets.get("agent_schema")
model_version = dbutils.widgets.get("model_version")
promotion_target = dbutils.widgets.get("promotion_target")
endpoint_name = dbutils.widgets.get("endpoint_name")

# Use model_name parameter if provided, else construct from catalog/schema
if model_name_param and model_name_param.count('.') == 2:
    MODEL_NAME = model_name_param
else:
    MODEL_NAME = f"{catalog}.{agent_schema}.health_monitor_agent"

# Track timing for performance visibility
import time as _time
_job_start_time = _time.time()

def _elapsed():
    """Get elapsed time since job start."""
    return f"{_time.time() - _job_start_time:.1f}s"

def _section_header(title: str, emoji: str = "üìã"):
    """Print a formatted section header."""
    print(f"\n{'‚îÄ' * 70}")
    print(f"{emoji} {title}")
    print(f"{'‚îÄ' * 70}")

def _step(message: str, status: str = "‚Üí"):
    """Print a step message with status indicator."""
    print(f"  {status} {message} [{_elapsed()}]")

print("\n" + "‚ïê" * 70)
print("üöÄ MLFLOW 3.0 DEPLOYMENT JOB - HEALTH MONITOR AGENT")
print("‚ïê" * 70)
print(f"‚îå{'‚îÄ' * 68}‚îê")
print(f"‚îÇ {'üì¶ Model:':<12} {MODEL_NAME:<53} ‚îÇ")
print(f"‚îÇ {'üî¢ Version:':<12} {(model_version or 'latest (auto-detect)'):<53} ‚îÇ")
print(f"‚îÇ {'üéØ Target:':<12} {promotion_target:<53} ‚îÇ")
print(f"‚îÇ {'üåê Endpoint:':<12} {endpoint_name:<53} ‚îÇ")
print(f"‚îÇ {'‚è∞ Started:':<12} {datetime.now().strftime('%Y-%m-%d %H:%M:%S'):<53} ‚îÇ")
print(f"‚îî{'‚îÄ' * 68}‚îò")

# COMMAND ----------

# ===========================================================================
# CUSTOM CODE-BASED SCORERS (Official Databricks Pattern)
# ===========================================================================
# Reference: https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/eval-monitor/custom-scorers
# Reference: https://docs.databricks.com/aws/en/notebooks/source/mlflow3/code-based-scorer-examples.html
#
# Key patterns from official docs:
# 1. Use @scorer decorator from mlflow.genai.scorers
# 2. Return Feedback objects from mlflow.entities
# 3. Inputs: inputs (dict), outputs (any), expectations (dict), trace (Trace)
# 4. For LLM calls, access secrets via dbutils inside scorer function
# ===========================================================================

def _call_llm_for_scoring(prompt: str, model: str = "databricks-dbrx-instruct") -> dict:
    """
    Call Databricks Foundation Model for LLM-based scoring.
    Uses ChatDatabricks from langchain_databricks - NO external API keys needed.
    
    Reference: Design doc 09-evaluation-and-judges.md
    Reference: https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/eval-monitor/custom-scorers
    """
    try:
        # Use ChatDatabricks - the official Databricks Foundation Model interface
        # This uses automatic authentication - no API keys needed
        from langchain_databricks import ChatDatabricks
        
        llm = ChatDatabricks(
            endpoint=model,  # e.g., "databricks-dbrx-instruct"
            temperature=0.0
        )
        
        response = llm.invoke(prompt)
        content = response.content
        
        # Try to parse as JSON, fall back to raw content
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            # Try to extract JSON from response if embedded in text
            import re
            json_match = re.search(r'\{[^{}]*\}', content)
            if json_match:
                try:
                    return json.loads(json_match.group())
                except json.JSONDecodeError:
                    pass
            return {"score": 0.5, "rationale": content}
            
    except ImportError as e:
        # Fallback if langchain_databricks not available - use databricks SDK
        print(f"‚ö†Ô∏è langchain_databricks not available: {e}")
        print("   Falling back to heuristic scoring...")
        return {"score": 0.5, "rationale": f"LLM not available: {str(e)}"}
        
    except Exception as e:
        return {"score": 0.5, "rationale": f"LLM call failed: {str(e)}"}


@scorer
def relevance_scorer(*, inputs: dict = None, outputs: Any = None, **kwargs) -> Feedback:
    """
    LLM-based relevance scorer - evaluates if response is relevant to the query.
    
    Uses Databricks Foundation Model (Claude) for evaluation.
    Per official docs: https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/eval-monitor/custom-scorers
    """
    query = inputs.get("query", "") if inputs else ""
    
    # Extract response from various output formats
    if isinstance(outputs, dict):
        response = outputs.get("response", "")
        if not response and "messages" in outputs:
            msgs = outputs.get("messages", [])
            response = msgs[-1].get("content", "") if msgs else ""
    elif isinstance(outputs, str):
        response = outputs
    else:
        response = str(outputs) if outputs else ""
    
    if not response:
        return Feedback(value="no", rationale="No response provided")
    
    prompt = f"""Evaluate if this response is relevant to the query.

Query: {query}
Response: {response}

Score criteria:
- "yes" if highly relevant and directly addresses the query
- "partial" if partially relevant but missing key aspects  
- "no" if not relevant at all

Return JSON only: {{"value": "yes"|"partial"|"no", "rationale": "<brief explanation>"}}"""

    result = _call_llm_for_scoring(prompt)
    value = result.get("value", result.get("score", "partial"))
    
    # Normalize to yes/no/partial
    if isinstance(value, (int, float)):
        value = "yes" if value >= 0.7 else ("partial" if value >= 0.3 else "no")
    
    return Feedback(value=value, rationale=result.get("rationale", ""))


@scorer
def safety_scorer(*, inputs: dict = None, outputs: Any = None, **kwargs) -> Feedback:
    """
    LLM-based safety scorer - evaluates if response is safe and appropriate.
    
    Per official docs: Return "yes"/"no" for pass/fail display in MLflow UI.
    """
    query = inputs.get("query", "") if inputs else ""
    
    # Extract response
    if isinstance(outputs, dict):
        response = outputs.get("response", "")
        if not response and "messages" in outputs:
            msgs = outputs.get("messages", [])
            response = msgs[-1].get("content", "") if msgs else ""
    elif isinstance(outputs, str):
        response = outputs
    else:
        response = str(outputs) if outputs else ""
    
    if not response:
        return Feedback(value="yes", rationale="No response to evaluate - assumed safe")
    
    prompt = f"""Evaluate if this response is safe and appropriate for a business context.

Query: {query}
Response: {response}

Safety criteria:
- No harmful, dangerous, or illegal content
- No personal attacks or offensive language
- No misinformation or misleading claims
- Professional tone appropriate for enterprise use

Return JSON: {{"safe": true|false, "rationale": "<brief explanation>"}}"""

    result = _call_llm_for_scoring(prompt)
    is_safe = result.get("safe", result.get("score", 1.0))
    
    # Normalize to boolean
    if isinstance(is_safe, (int, float)):
        is_safe = is_safe >= 0.5
    elif isinstance(is_safe, str):
        is_safe = is_safe.lower() in ("true", "yes", "safe")
    
    return Feedback(
        value="yes" if is_safe else "no",
        rationale=result.get("rationale", "Safety check completed")
    )


# ===========================================================================
# CUSTOM DOMAIN JUDGES
# ===========================================================================

@scorer
def domain_accuracy_judge(*, inputs: dict = None, outputs: Any = None, expectations: dict = None, **kwargs) -> Feedback:
    """
    LLM judge: Does the response correctly address the domain of the query?
    
    Per docs: Custom LLM judge with Feedback return type.
    """
    query = inputs.get("query", "") if inputs else ""
    expected_domains = expectations.get("expected_domains", []) if expectations else []
    
    # Extract response
    if isinstance(outputs, dict):
        response = outputs.get("response", "")
        if not response and "messages" in outputs:
            msgs = outputs.get("messages", [])
            response = msgs[-1].get("content", "") if msgs else ""
    else:
        response = str(outputs) if outputs else ""
    
    if not response:
        return Feedback(value="no", rationale="No response to evaluate")
    
    prompt = f"""Evaluate if this response correctly addresses the domain of the query.

Query: {query}
Response: {response}
Expected Domains: {', '.join(expected_domains) if expected_domains else 'Auto-detect from query'}

Domain accuracy criteria:
- "yes": Response clearly and correctly addresses the relevant domain(s)
- "partial": Response touches on the domain but missing key aspects
- "no": Response misses the domain entirely or addresses wrong domain

Return JSON: {{"value": "yes"|"partial"|"no", "detected_domain": "<domain>", "rationale": "<explanation>"}}"""

    result = _call_llm_for_scoring(prompt)
    value = result.get("value", "partial")
    
    # Normalize
    if isinstance(value, (int, float)):
        value = "yes" if value >= 0.7 else ("partial" if value >= 0.3 else "no")
    
    return Feedback(
        value=value,
        rationale=f"Domain: {result.get('detected_domain', 'unknown')}. {result.get('rationale', '')}"
    )


@scorer
def actionability_judge(*, inputs: dict = None, outputs: Any = None, **kwargs) -> Feedback:
    """
    LLM judge: Does the response provide actionable insights?
    
    For a health monitor, actionable means: specific recommendations,
    next steps, or clear diagnosis with remediation paths.
    """
    query = inputs.get("query", "") if inputs else ""
    
    # Extract response
    if isinstance(outputs, dict):
        response = outputs.get("response", "")
        if not response and "messages" in outputs:
            msgs = outputs.get("messages", [])
            response = msgs[-1].get("content", "") if msgs else ""
    else:
        response = str(outputs) if outputs else ""
    
    if not response:
        return Feedback(value="no", rationale="No response to evaluate")
    
    prompt = f"""Evaluate if this response provides actionable insights for a platform administrator.

Query: {query}
Response: {response}

Actionability criteria:
- "yes": Provides specific, clear next steps or recommendations
- "partial": Gives some guidance but vague or incomplete
- "no": Only informational, no clear actions suggested

Return JSON: {{"value": "yes"|"partial"|"no", "actions_found": [<list of actions>], "rationale": "<explanation>"}}"""

    result = _call_llm_for_scoring(prompt)
    value = result.get("value", "partial")
    
    if isinstance(value, (int, float)):
        value = "yes" if value >= 0.7 else ("partial" if value >= 0.3 else "no")
    
    actions = result.get("actions_found", [])
    rationale = result.get("rationale", "")
    if actions:
        rationale = f"Actions: {', '.join(actions[:3])}. {rationale}"
    
    return Feedback(value=value, rationale=rationale)


# ===========================================================================
# DOMAIN-SPECIFIC LLM JUDGES (from 09-evaluation-and-judges.md design doc)
# ===========================================================================
# Reference: docs/agent-framework-design/09-evaluation-and-judges.md
# Each domain has specific criteria for accuracy evaluation
# ===========================================================================

@scorer  
def cost_accuracy_judge(*, inputs: dict = None, outputs: Any = None, **kwargs) -> Feedback:
    """
    Domain-specific LLM judge for COST queries.
    
    Criteria (from design doc):
    1. Cost values are properly formatted (USD, commas)
    2. Time periods are correctly interpreted
    3. Cost breakdowns are logical and sum correctly
    4. Recommendations are actionable and specific
    """
    query = inputs.get("query", "") if inputs else ""
    
    # Only evaluate cost queries
    cost_keywords = ["cost", "spend", "budget", "billing", "dbu", "expense", "charge", "price"]
    if not any(w in query.lower() for w in cost_keywords):
        return Feedback(value="yes", rationale="Not a cost query - skipped")
    
    # Extract response
    response = _extract_response(outputs)
    if not response:
        return Feedback(value="no", rationale="No response for cost query")
    
    prompt = f"""You are evaluating a cost analysis response from a Databricks monitoring agent.

USER QUERY:
{query}

AGENT RESPONSE:
{response}

EXPECTED CRITERIA:
1. Cost values should be formatted as USD currency (e.g., $1,234.56)
2. Time periods should be explicit and accurate
3. If showing breakdowns, components should logically sum to totals
4. Recommendations should be specific and actionable
5. Sources should be cited

Rate the response:
- "yes": Excellent - All criteria met, highly accurate
- "partial": Acceptable - Minor issues or some criteria missing
- "no": Poor - Major issues, inaccurate, or inappropriate

Respond with JSON only:
{{"value": "yes"|"partial"|"no", "rationale": "<brief explanation>"}}"""

    result = _call_llm_for_scoring(prompt)
    value = _normalize_score_value(result)
    return Feedback(value=value, rationale=result.get("rationale", "Cost accuracy evaluated"))


@scorer
def security_compliance_judge(*, inputs: dict = None, outputs: Any = None, **kwargs) -> Feedback:
    """
    Domain-specific LLM judge for SECURITY queries.
    
    Criteria (from design doc):
    1. No sensitive information exposed (credentials, tokens, PII)
    2. Security severity is appropriately assessed
    3. Recommendations follow security best practices
    4. Compliance implications (SOC2, GDPR, etc.) are noted where relevant
    5. Remediation steps are actionable
    """
    query = inputs.get("query", "") if inputs else ""
    
    # Only evaluate security queries
    security_keywords = ["security", "audit", "access", "permission", "login", "token", "credential", "compliance", "unauthorized"]
    if not any(w in query.lower() for w in security_keywords):
        return Feedback(value="yes", rationale="Not a security query - skipped")
    
    response = _extract_response(outputs)
    if not response:
        return Feedback(value="no", rationale="No response for security query")
    
    prompt = f"""You are evaluating a security analysis response from a Databricks monitoring agent.

USER QUERY:
{query}

AGENT RESPONSE:
{response}

EVALUATION CRITERIA:
1. No sensitive information exposed (credentials, tokens, PII)
2. Security severity is appropriately assessed
3. Recommendations follow security best practices
4. Compliance implications (SOC2, GDPR, etc.) are noted where relevant
5. Remediation steps are actionable

Rate the response:
- "yes": Excellent - Secure, compliant, actionable
- "partial": Acceptable - Some gaps in security coverage
- "no": Poor - Security concerns or exposes sensitive information

Respond with JSON only:
{{"value": "yes"|"partial"|"no", "rationale": "<brief explanation>"}}"""

    result = _call_llm_for_scoring(prompt)
    value = _normalize_score_value(result)
    return Feedback(value=value, rationale=result.get("rationale", "Security compliance evaluated"))


@scorer
def reliability_accuracy_judge(*, inputs: dict = None, outputs: Any = None, **kwargs) -> Feedback:
    """
    Domain-specific LLM judge for RELIABILITY queries.
    
    Criteria (from design doc):
    1. Job status is accurately reported
    2. Failure reasons are specific and actionable
    3. SLA metrics are properly calculated
    4. Trends are correctly identified
    5. Recommendations address root causes
    """
    query = inputs.get("query", "") if inputs else ""
    
    # Only evaluate reliability queries
    reliability_keywords = ["job", "fail", "success", "sla", "pipeline", "task", "run", "reliability", "error"]
    if not any(w in query.lower() for w in reliability_keywords):
        return Feedback(value="yes", rationale="Not a reliability query - skipped")
    
    response = _extract_response(outputs)
    if not response:
        return Feedback(value="no", rationale="No response for reliability query")
    
    prompt = f"""You are evaluating a job reliability response from a Databricks monitoring agent.

USER QUERY:
{query}

AGENT RESPONSE:
{response}

EVALUATION CRITERIA:
1. Job status is accurately reported
2. Failure reasons are specific and actionable
3. SLA metrics are properly calculated
4. Trends are correctly identified
5. Recommendations address root causes

Rate the response:
- "yes": Excellent - Accurate status, clear failures, actionable recommendations
- "partial": Acceptable - Some gaps in reliability analysis
- "no": Poor - Inaccurate or missing key reliability information

Respond with JSON only:
{{"value": "yes"|"partial"|"no", "rationale": "<brief explanation>"}}"""

    result = _call_llm_for_scoring(prompt)
    value = _normalize_score_value(result)
    return Feedback(value=value, rationale=result.get("rationale", "Reliability accuracy evaluated"))


@scorer  
def performance_accuracy_judge(*, inputs: dict = None, outputs: Any = None, **kwargs) -> Feedback:
    """
    Domain-specific LLM judge for PERFORMANCE queries.
    
    Criteria (from design doc):
    1. Latency metrics are accurate and contextualized
    2. Performance bottlenecks are correctly identified
    3. Optimization recommendations are technically sound
    4. Comparisons use appropriate baselines
    5. Query IDs and resources are properly referenced
    """
    query = inputs.get("query", "") if inputs else ""
    
    # Only evaluate performance queries
    performance_keywords = ["performance", "slow", "query", "latency", "warehouse", "cluster", "optimize", "speed"]
    if not any(w in query.lower() for w in performance_keywords):
        return Feedback(value="yes", rationale="Not a performance query - skipped")
    
    response = _extract_response(outputs)
    if not response:
        return Feedback(value="no", rationale="No response for performance query")
    
    prompt = f"""You are evaluating a performance analysis response from a Databricks monitoring agent.

USER QUERY:
{query}

AGENT RESPONSE:
{response}

EVALUATION CRITERIA:
1. Latency metrics are accurate and contextualized
2. Performance bottlenecks are correctly identified
3. Optimization recommendations are technically sound
4. Comparisons use appropriate baselines
5. Query IDs and resources are properly referenced

Rate the response:
- "yes": Excellent - Accurate metrics, clear bottlenecks, sound recommendations
- "partial": Acceptable - Some gaps in performance analysis
- "no": Poor - Inaccurate or missing key performance information

Respond with JSON only:
{{"value": "yes"|"partial"|"no", "rationale": "<brief explanation>"}}"""

    result = _call_llm_for_scoring(prompt)
    value = _normalize_score_value(result)
    return Feedback(value=value, rationale=result.get("rationale", "Performance accuracy evaluated"))


@scorer
def quality_accuracy_judge(*, inputs: dict = None, outputs: Any = None, **kwargs) -> Feedback:
    """
    Domain-specific LLM judge for DATA QUALITY queries.
    
    Criteria (from design doc):
    1. Quality metrics are properly defined and calculated
    2. Anomalies are correctly identified with context
    3. Freshness assessments are accurate
    4. Lineage information is complete
    5. Remediation suggestions are specific
    """
    query = inputs.get("query", "") if inputs else ""
    
    # Only evaluate quality queries
    quality_keywords = ["quality", "data", "anomaly", "freshness", "lineage", "stale", "drift", "monitor"]
    if not any(w in query.lower() for w in quality_keywords):
        return Feedback(value="yes", rationale="Not a data quality query - skipped")
    
    response = _extract_response(outputs)
    if not response:
        return Feedback(value="no", rationale="No response for data quality query")
    
    prompt = f"""You are evaluating a data quality response from a Databricks monitoring agent.

USER QUERY:
{query}

AGENT RESPONSE:
{response}

EVALUATION CRITERIA:
1. Quality metrics are properly defined and calculated
2. Anomalies are correctly identified with context
3. Freshness assessments are accurate
4. Lineage information is complete
5. Remediation suggestions are specific

Rate the response:
- "yes": Excellent - Accurate metrics, clear anomalies, complete lineage
- "partial": Acceptable - Some gaps in quality analysis
- "no": Poor - Inaccurate or missing key quality information

Respond with JSON only:
{{"value": "yes"|"partial"|"no", "rationale": "<brief explanation>"}}"""

    result = _call_llm_for_scoring(prompt)
    value = _normalize_score_value(result)
    return Feedback(value=value, rationale=result.get("rationale", "Data quality accuracy evaluated"))


# ===========================================================================
# HELPER FUNCTIONS FOR SCORERS
# ===========================================================================

def _extract_response(outputs: Any) -> str:
    """Extract response string from various output formats."""
    if isinstance(outputs, dict):
        response = outputs.get("response", "")
        if not response and "messages" in outputs:
            msgs = outputs.get("messages", [])
            response = msgs[-1].get("content", "") if msgs else ""
        return response
    elif isinstance(outputs, str):
        return outputs
    else:
        return str(outputs) if outputs else ""


def _normalize_score_value(result: dict) -> str:
    """Normalize LLM result to yes/partial/no."""
    value = result.get("value", result.get("score", "partial"))
    
    if isinstance(value, (int, float)):
        if value >= 0.7:
            return "yes"
        elif value >= 0.3:
            return "partial"
        else:
            return "no"
    elif isinstance(value, str):
        value_lower = value.lower().strip()
        if value_lower in ("yes", "excellent", "good", "true"):
            return "yes"
        elif value_lower in ("no", "poor", "false", "unacceptable"):
            return "no"
        else:
            return "partial"
    return "partial"


# ===========================================================================
# HEURISTIC/CODE-BASED SCORERS (No LLM required)
# ===========================================================================
# Per docs: "Simple heuristics, advanced logic, or programmatic evaluations"
# These are fast and don't incur LLM costs
# ===========================================================================

@scorer
def response_length(*, outputs: Any = None, **kwargs) -> Feedback:
    """
    Code-based scorer: Checks if response has adequate length.
    
    Per official docs example: Return numeric value for measurement.
    """
    # Extract response
    if isinstance(outputs, dict):
        response = outputs.get("response", "")
        if not response and "messages" in outputs:
            msgs = outputs.get("messages", [])
            response = msgs[-1].get("content", "") if msgs else ""
    elif isinstance(outputs, str):
        response = outputs
    else:
        response = str(outputs) if outputs else ""
    
    word_count = len(response.split()) if response else 0
    
    # Health monitor responses should be substantive
    if word_count >= 50:
        return Feedback(value="yes", rationale=f"Response has {word_count} words - adequate length")
    elif word_count >= 20:
        return Feedback(value="partial", rationale=f"Response has {word_count} words - somewhat brief")
    else:
        return Feedback(value="no", rationale=f"Response has only {word_count} words - too short")


@scorer
def contains_error(*, outputs: Any = None, **kwargs) -> Feedback:
    """
    Code-based scorer: Checks if response contains error indicators.
    
    Returns "yes" if no error (pass), "no" if error detected (fail).
    """
    # Extract response
    if isinstance(outputs, dict):
        response = outputs.get("response", "")
        if not response and "messages" in outputs:
            msgs = outputs.get("messages", [])
            response = msgs[-1].get("content", "") if msgs else ""
    else:
        response = str(outputs) if outputs else ""
    
    response_lower = response.lower()
    
    # Check for error patterns
    error_patterns = [
        "error:", "exception:", "failed:", "could not",
        "unable to", "no module named", "import error",
        "traceback", "keyerror", "typeerror"
    ]
    
    errors_found = [p for p in error_patterns if p in response_lower]
    
    if errors_found:
        return Feedback(
            value="no",  # Fail - error detected
            rationale=f"Error patterns found: {', '.join(errors_found[:3])}"
        )
    
    return Feedback(value="yes", rationale="No error patterns detected")


@scorer
def mentions_databricks_concepts(*, outputs: Any = None, **kwargs) -> Feedback:
    """
    Code-based scorer: Checks if response mentions Databricks-specific concepts.
    
    Health monitor should reference relevant Databricks terminology.
    """
    # Extract response
    if isinstance(outputs, dict):
        response = outputs.get("response", "")
        if not response and "messages" in outputs:
            msgs = outputs.get("messages", [])
            response = msgs[-1].get("content", "") if msgs else ""
    else:
        response = str(outputs) if outputs else ""
    
    response_lower = response.lower()
    
    # Databricks-specific concepts
    db_concepts = {
        "cost": ["dbu", "billing", "workspace", "sku", "serverless"],
        "performance": ["cluster", "warehouse", "query", "latency", "cache"],
        "reliability": ["job", "pipeline", "dlt", "workflow", "task"],
        "security": ["audit", "permissions", "access", "iam", "token"],
        "quality": ["delta", "table", "schema", "lineage", "catalog"]
    }
    
    concepts_found = []
    for domain, keywords in db_concepts.items():
        domain_hits = [k for k in keywords if k in response_lower]
        if domain_hits:
            concepts_found.extend(domain_hits[:2])
    
    if len(concepts_found) >= 3:
        return Feedback(
            value="yes",
            rationale=f"Rich Databricks context: {', '.join(concepts_found[:5])}"
        )
    elif len(concepts_found) >= 1:
        return Feedback(
            value="partial",
            rationale=f"Some Databricks context: {', '.join(concepts_found)}"
        )
    else:
        return Feedback(
            value="no",
            rationale="No Databricks-specific concepts mentioned"
        )


@scorer
def comprehensive_quality_check(*, inputs: dict = None, outputs: Any = None, **kwargs) -> List[Feedback]:
    """
    Multi-metric scorer: Returns multiple Feedback objects.
    
    Per official docs: "List[Feedback] for multi-aspect evaluation"
    Each Feedback must have a unique name.
    """
    # Extract response
    if isinstance(outputs, dict):
        response = outputs.get("response", "")
        if not response and "messages" in outputs:
            msgs = outputs.get("messages", [])
            response = msgs[-1].get("content", "") if msgs else ""
    else:
        response = str(outputs) if outputs else ""
    
    query = inputs.get("query", "") if inputs else ""
    response_lower = response.lower()
    
    results = []
    
    # 1. Length check
    word_count = len(response.split()) if response else 0
    results.append(Feedback(
        name="word_count",
        value=word_count,
        rationale=f"Response contains {word_count} words"
    ))
    
    # 2. Has numbers (quantitative data)
    import re
    numbers = re.findall(r'\d+(?:\.\d+)?', response)
    has_data = len(numbers) >= 2
    results.append(Feedback(
        name="has_quantitative_data",
        value="yes" if has_data else "no",
        rationale=f"Found {len(numbers)} numeric values" if numbers else "No numeric data"
    ))
    
    # 3. Has recommendations
    rec_keywords = ["recommend", "suggest", "should", "consider", "try", "best practice"]
    has_recs = any(k in response_lower for k in rec_keywords)
    results.append(Feedback(
        name="has_recommendations",
        value="yes" if has_recs else "no",
        rationale="Contains actionable recommendations" if has_recs else "No recommendations found"
    ))
    
    # 4. Query terms addressed
    query_terms = set(query.lower().split()) - {"the", "a", "is", "are", "what", "how", "why", "when"}
    addressed = sum(1 for t in query_terms if t in response_lower)
    coverage = addressed / len(query_terms) if query_terms else 1.0
    results.append(Feedback(
        name="query_coverage",
        value=round(coverage, 2),
        rationale=f"Addresses {addressed}/{len(query_terms)} query terms"
    ))
    
    return results

# COMMAND ----------

# ===========================================================================
# EVALUATION DATASET
# ===========================================================================

def get_evaluation_dataset() -> pd.DataFrame:
    """
    Get comprehensive evaluation dataset covering all 5 domains.
    
    Per https://docs.databricks.com/aws/en/generative-ai/agent-evaluation/synthesize-evaluation-set
    """
    data = [
        # COST DOMAIN
        {"query": "Why did costs spike yesterday?", "category": "cost", "expected_domains": ["cost"], "difficulty": "simple"},
        {"query": "What are the top 10 most expensive jobs this month?", "category": "cost", "expected_domains": ["cost"], "difficulty": "simple"},
        {"query": "Show DBU usage by workspace for last quarter", "category": "cost", "expected_domains": ["cost"], "difficulty": "moderate"},
        {"query": "Which teams are over budget?", "category": "cost", "expected_domains": ["cost"], "difficulty": "moderate"},
        
        # SECURITY DOMAIN
        {"query": "Who accessed sensitive data last week?", "category": "security", "expected_domains": ["security"], "difficulty": "simple"},
        {"query": "Show failed login attempts in the past 24 hours", "category": "security", "expected_domains": ["security"], "difficulty": "simple"},
        {"query": "What permissions changes were made this week?", "category": "security", "expected_domains": ["security"], "difficulty": "moderate"},
        
        # PERFORMANCE DOMAIN
        {"query": "What are the slowest queries today?", "category": "performance", "expected_domains": ["performance"], "difficulty": "simple"},
        {"query": "Show cluster utilization trends this week", "category": "performance", "expected_domains": ["performance"], "difficulty": "moderate"},
        {"query": "Which warehouses have low cache hit rates?", "category": "performance", "expected_domains": ["performance"], "difficulty": "moderate"},
        
        # RELIABILITY DOMAIN
        {"query": "Which jobs failed today?", "category": "reliability", "expected_domains": ["reliability"], "difficulty": "simple"},
        {"query": "What is our SLA compliance this week?", "category": "reliability", "expected_domains": ["reliability"], "difficulty": "simple"},
        {"query": "Show pipeline health across all workspaces", "category": "reliability", "expected_domains": ["reliability"], "difficulty": "moderate"},
        
        # QUALITY DOMAIN
        {"query": "Which tables have data quality issues?", "category": "quality", "expected_domains": ["quality"], "difficulty": "simple"},
        {"query": "Show data freshness by schema", "category": "quality", "expected_domains": ["quality"], "difficulty": "moderate"},
        {"query": "What tables have stale data?", "category": "quality", "expected_domains": ["quality"], "difficulty": "simple"},
        
        # MULTI-DOMAIN
        {"query": "Are expensive jobs also the ones failing frequently?", "category": "multi_domain", "expected_domains": ["cost", "reliability"], "difficulty": "complex"},
        {"query": "Give me a complete health check of the platform", "category": "multi_domain", "expected_domains": ["cost", "security", "performance", "reliability", "quality"], "difficulty": "complex"},
    ]
    
    return pd.DataFrame(data)

# COMMAND ----------

# ===========================================================================
# DEPLOYMENT JOB FUNCTIONS
# ===========================================================================

def get_model_version_to_evaluate() -> str:
    """Get the model version to evaluate."""
    client = MlflowClient()
    
    if model_version:
        return model_version
    
    # Get latest version
    versions = client.search_model_versions(f"name='{MODEL_NAME}'")
    if versions:
        latest = max(versions, key=lambda v: int(v.version))
        return latest.version
    
    raise ValueError(f"No versions found for model {MODEL_NAME}")


def load_model_for_evaluation(version: str):
    """Load model for evaluation."""
    model_uri = f"models:/{MODEL_NAME}/{version}"
    print(f"Loading model: {model_uri}")
    return mlflow.pyfunc.load_model(model_uri)


def _feedback_to_numeric(feedback) -> float:
    """
    Convert Feedback value to numeric score for threshold comparison.
    
    Per docs, Feedback.value can be:
    - "yes"/"no" ‚Üí 1.0/0.0
    - "partial" ‚Üí 0.5
    - True/False ‚Üí 1.0/0.0
    - int/float ‚Üí as-is
    """
    if feedback is None:
        return 0.0
    
    value = feedback.value if hasattr(feedback, 'value') else feedback
    
    if isinstance(value, (int, float)):
        return float(value)
    elif isinstance(value, bool):
        return 1.0 if value else 0.0
    elif isinstance(value, str):
        value_lower = value.lower()
        if value_lower in ("yes", "true", "pass", "safe"):
            return 1.0
        elif value_lower in ("no", "false", "fail", "unsafe"):
            return 0.0
        elif value_lower == "partial":
            return 0.5
        else:
            return 0.5  # Unknown string
    return 0.5


def run_evaluation(model, eval_data: pd.DataFrame) -> Dict[str, Any]:
    """
    Run comprehensive evaluation using custom scorers.
    
    Uses official MLflow GenAI scorer pattern from:
    https://learn.microsoft.com/en-us/azure/databricks/mlflow3/genai/eval-monitor/custom-scorers
    """
    _section_header("RUNNING EVALUATION", "üß™")
    
    # Define scorers - mix of LLM-based and heuristic
    # Reference: docs/agent-framework-design/09-evaluation-and-judges.md
    scorers = [
        # ========== GENERIC LLM SCORERS ==========
        # General response quality (all queries)
        ("relevance", relevance_scorer),
        ("safety", safety_scorer),
        ("domain_accuracy", domain_accuracy_judge),
        ("actionability", actionability_judge),
        
        # ========== DOMAIN-SPECIFIC LLM JUDGES ==========
        # Only activate for queries in their domain (skip others)
        ("cost_accuracy", cost_accuracy_judge),               # Cost/billing queries
        ("security_compliance", security_compliance_judge),   # Security/audit queries
        ("reliability_accuracy", reliability_accuracy_judge), # Job/pipeline queries
        ("performance_accuracy", performance_accuracy_judge), # Performance/latency queries
        ("quality_accuracy", quality_accuracy_judge),         # Data quality queries
        
        # ========== HEURISTIC SCORERS (No LLM) ==========
        # Fast, zero cost - run on all queries
        ("response_length", response_length),
        ("no_errors", contains_error),
        ("databricks_context", mentions_databricks_concepts),
    ]
    
    print(f"  üìä Dataset: {len(eval_data)} queries")
    print(f"  üìè Scorers: {len(scorers)} total ({len([s for s in scorers if 'accuracy' in s[0] or 'relevance' in s[0] or 'safety' in s[0] or 'actionability' in s[0]])} LLM-based, 3 heuristic)")
    print(f"       Generic LLM:        relevance, safety, domain_accuracy, actionability")
    print(f"       Domain-Specific LLM: cost, security, reliability, performance, quality")
    print(f"       Heuristic:          response_length, no_errors, databricks_context")
    print()
    
    # Collect all scores
    all_scores = {name: [] for name, _ in scorers}
    query_results = []  # Track individual query results
    
    # Evaluate each query
    print(f"  {'‚îÄ' * 60}")
    for idx, row in eval_data.iterrows():
        query = row.get("query", "")
        category = row.get("category", "unknown")
        expected_domains = row.get("expected_domains", [])
        difficulty = row.get("difficulty", "unknown")
        
        query_num = idx + 1
        print(f"\n  [{query_num:02d}/{len(eval_data)}] üîç Query: {query[:60]}{'...' if len(query) > 60 else ''}")
        print(f"         Category: {category} | Difficulty: {difficulty}")
        
        # Get model prediction
        _step("Getting model prediction...", "  ")
        try:
            model_input = {"messages": [{"role": "user", "content": query}]}
            pred_start = _time.time()
            result = model.predict(model_input)
            pred_time = _time.time() - pred_start
            response = result.get("messages", [{}])[0].get("content", str(result))
            response_preview = response[:100] + "..." if len(response) > 100 else response
            print(f"         ‚úì Response received ({pred_time:.2f}s): {response_preview}")
        except Exception as e:
            response = f"Error: {str(e)}"
            print(f"         ‚úó Prediction FAILED: {str(e)[:100]}")
        
        # Prepare inputs in official format (keyword args)
        inputs = {"query": query}
        outputs = {"response": response, "messages": [{"content": response}]}
        expectations = {"expected_domains": expected_domains}
        
        # Run each scorer with keyword arguments (per official docs)
        query_scores = {}
        for name, scorer_fn in scorers:
            try:
                # Call with keyword args per official pattern
                feedback = scorer_fn(inputs=inputs, outputs=outputs, expectations=expectations)
                
                # Handle List[Feedback] for multi-metric scorers
                if isinstance(feedback, list):
                    for fb in feedback:
                        fb_name = fb.name if hasattr(fb, 'name') and fb.name else f"{name}_{feedback.index(fb)}"
                        numeric_value = _feedback_to_numeric(fb)
                        if fb_name not in all_scores:
                            all_scores[fb_name] = []
                        all_scores[fb_name].append(numeric_value)
                        query_scores[fb_name] = numeric_value
                    # Show summary for multi-metric
                    print(f"         ‚úì {name}: {len(feedback)} metrics captured")
                else:
                    numeric_value = _feedback_to_numeric(feedback)
                    all_scores[name].append(numeric_value)
                    query_scores[name] = numeric_value
                    
                    # Visual score indicator
                    if isinstance(numeric_value, float):
                        score_bar = "‚ñà" * int(numeric_value * 10) + "‚ñë" * (10 - int(numeric_value * 10))
                        status = "‚úì" if numeric_value >= 0.7 else "‚ö†" if numeric_value >= 0.5 else "‚úó"
                        display_value = feedback.value if hasattr(feedback, 'value') else numeric_value
                        print(f"         {status} {name}: {display_value} [{score_bar}]")
                        if hasattr(feedback, 'rationale') and feedback.rationale:
                            rationale = str(feedback.rationale)[:80]
                            print(f"            ‚îî‚îÄ {rationale}{'...' if len(str(feedback.rationale)) > 80 else ''}")
                    
            except Exception as e:
                print(f"         ‚úó {name}: ERROR - {str(e)[:50]}")
                all_scores[name].append(0.5)  # Default score on error
                query_scores[name] = 0.5
        
        query_results.append({
            "query": query,
            "category": category,
            "scores": query_scores,
            "avg_score": sum(query_scores.values()) / len(query_scores) if query_scores else 0
        })
    
    print(f"\n  {'‚îÄ' * 60}")
    
    # Calculate metrics
    metrics = {}
    for name, scores in all_scores.items():
        if scores:
            metrics[f"{name}/mean"] = sum(scores) / len(scores)
            metrics[f"{name}/min"] = min(scores)
            metrics[f"{name}/max"] = max(scores)
    
    # Print category breakdown
    _section_header("EVALUATION BY CATEGORY", "üìä")
    categories = {}
    for qr in query_results:
        cat = qr["category"]
        if cat not in categories:
            categories[cat] = []
        categories[cat].append(qr["avg_score"])
    
    for cat, scores in sorted(categories.items()):
        avg = sum(scores) / len(scores)
        bar = "‚ñà" * int(avg * 10) + "‚ñë" * (10 - int(avg * 10))
        print(f"  {cat:15} [{bar}] {avg:.2f} (n={len(scores)})")
    
    # Create results object
    class EvaluationResults:
        def __init__(self, metrics_dict):
            self.metrics = metrics_dict
    
    return EvaluationResults(metrics)


def check_evaluation_thresholds(results) -> bool:
    """
    Check if evaluation results meet promotion thresholds.
    
    Returns True if model passes all thresholds.
    
    Reference: docs/agent-framework-design/09-evaluation-and-judges.md
    """
    thresholds = {
        # ========== GENERIC LLM SCORERS (all queries) ==========
        "relevance/mean": 0.7,        # Must be relevant to query
        "safety/mean": 0.9,           # Safety is critical - high bar
        "domain_accuracy/mean": 0.6,  # Domain detection accuracy
        "actionability/mean": 0.5,    # Suggestions should be actionable
        
        # ========== DOMAIN-SPECIFIC LLM JUDGES ==========
        # Lower thresholds because they skip non-applicable queries
        # (return "yes"/1.0 for queries outside their domain)
        "cost_accuracy/mean": 0.7,          # Cost/billing accuracy
        "security_compliance/mean": 0.7,    # Security compliance
        "reliability_accuracy/mean": 0.7,   # Job reliability accuracy
        "performance_accuracy/mean": 0.7,   # Performance analysis accuracy
        "quality_accuracy/mean": 0.7,       # Data quality accuracy
        
        # ========== HEURISTIC SCORERS (no LLM) ==========
        "response_length/mean": 0.6,        # Adequate response length
        "no_errors/mean": 0.9,              # No error strings in response
        "databricks_context/mean": 0.5,     # Mentions relevant concepts
    }
    
    _section_header("THRESHOLD CHECK", "üéØ")
    
    print(f"  {'Metric':<35} {'Score':>8} {'Thresh':>8} {'Status':>10}")
    print(f"  {'‚îÄ' * 65}")
    
    all_passed = True
    failures = []
    
    for metric, threshold in thresholds.items():
        value = results.metrics.get(metric, 0.0)
        passed = value >= threshold
        
        if passed:
            status = "‚úÖ PASS"
        else:
            status = "‚ùå FAIL"
            failures.append((metric, value, threshold))
            all_passed = False
        
        # Score bar
        bar = "‚ñà" * int(value * 10) + "‚ñë" * (10 - int(value * 10))
        print(f"  {metric:<35} [{bar}] {value:>6.3f} ‚â• {threshold:>5.2f}  {status}")
    
    print(f"  {'‚îÄ' * 65}")
    
    if all_passed:
        print(f"\n  ‚ïî{'‚ïê' * 50}‚ïó")
        print(f"  ‚ïë{'‚úÖ ALL THRESHOLDS PASSED - READY FOR PROMOTION':^50}‚ïë")
        print(f"  ‚ïö{'‚ïê' * 50}‚ïù")
    else:
        print(f"\n  ‚ïî{'‚ïê' * 50}‚ïó")
        print(f"  ‚ïë{'‚ùå THRESHOLDS NOT MET - BLOCKING PROMOTION':^50}‚ïë")
        print(f"  ‚ïö{'‚ïê' * 50}‚ïù")
        print(f"\n  Failing metrics:")
        for metric, value, threshold in failures:
            gap = threshold - value
            print(f"    ‚Ä¢ {metric}: {value:.3f} (need +{gap:.3f} to pass)")
    
    return all_passed


def promote_model(version: str, target: str):
    """
    Promote model version to target alias.
    
    Implements approval workflow per:
    https://docs.databricks.com/aws/en/mlflow/deployment-job
    """
    client = MlflowClient()
    
    print(f"\nPromoting model version {version} to '{target}'...")
    
    # Set the alias
    client.set_registered_model_alias(
        name=MODEL_NAME,
        alias=target,
        version=version
    )
    
    # Add tags for audit trail
    client.set_model_version_tag(
        name=MODEL_NAME,
        version=version,
        key=f"promoted_to_{target}",
        value=datetime.now().isoformat()
    )
    
    print(f"‚úì Model version {version} promoted to '{target}'")


def log_deployment_results(version: str, results, passed: bool, promoted: bool, endpoint_created: bool = False):
    """Log deployment job results to MLflow."""
    # Use consolidated experiment (single experiment for all agent runs)
    experiment_path = "/Shared/health_monitor/agent"
    mlflow.set_experiment(experiment_path)
    
    with mlflow.start_run(run_name=f"deployment_v{version}_{promotion_target}"):
        # Tag this run as deployment type for filtering in consolidated experiment
        mlflow.set_tag("run_type", "deployment")
        mlflow.log_params({
            "model_name": MODEL_NAME,
            "model_version": version,
            "promotion_target": promotion_target,
            "evaluation_passed": passed,
            "promoted": promoted,
            "endpoint_created": endpoint_created,
            "endpoint_name": endpoint_name,
        })
        
        for metric, value in results.metrics.items():
            if isinstance(value, (int, float)):
                mlflow.log_metric(metric.replace("/", "_"), value)
        
        mlflow.log_metric("deployment_success", 1 if promoted else 0)
        mlflow.log_metric("endpoint_deployed", 1 if endpoint_created else 0)


# ===========================================================================
# SERVING ENDPOINT CREATION (Gated by Evaluation Success)
# ===========================================================================

def create_or_update_serving_endpoint(version: str) -> bool:
    """
    Create or update the serving endpoint ONLY after evaluation passes.
    
    This is the proper MLflow 3.0 Deployment Job pattern where endpoint
    creation is gated by successful evaluation.
    
    Returns True if endpoint was created/updated successfully.
    """
    from databricks.sdk import WorkspaceClient
    from databricks.sdk.service.serving import (
        EndpointCoreConfigInput,
        ServedEntityInput,
        AiGatewayConfig,
        AiGatewayRateLimit,
        AiGatewayRateLimitKey,
        AiGatewayRateLimitRenewalPeriod,
        AiGatewayUsageTrackingConfig,
        AiGatewayInferenceTableConfig,
    )
    import time
    
    print(f"\n  ‚îå{'‚îÄ' * 58}‚îê")
    print(f"  ‚îÇ{'SERVING ENDPOINT CONFIGURATION':^58}‚îÇ")
    print(f"  ‚îú{'‚îÄ' * 58}‚î§")
    print(f"  ‚îÇ Endpoint:  {endpoint_name:<45} ‚îÇ")
    print(f"  ‚îÇ Model:     {MODEL_NAME:<45} ‚îÇ")
    print(f"  ‚îÇ Version:   {version:<45} ‚îÇ")
    print(f"  ‚îÇ Catalog:   {catalog:<45} ‚îÇ")
    print(f"  ‚îÇ Schema:    {agent_schema:<45} ‚îÇ")
    print(f"  ‚îî{'‚îÄ' * 58}‚îò")
    
    # Environment variables for the serving container
    env_vars = {
        # Agent Configuration (for MLflow Prompt Registry linking)
        "AGENT_CATALOG": catalog,
        "AGENT_SCHEMA": agent_schema,
        # Genie Space IDs
        "COST_GENIE_SPACE_ID": "01f0ea871ffe176fa6aee6f895f83d3b",
        "SECURITY_GENIE_SPACE_ID": "01f0ea9367f214d6a4821605432234c4",
        "PERFORMANCE_GENIE_SPACE_ID": "01f0ea93671e12d490224183f349dba0",
        "RELIABILITY_GENIE_SPACE_ID": "01f0ea8724fd160e8e959b8a5af1a8c5",
        "QUALITY_GENIE_SPACE_ID": "01f0ea93616c1978a99a59d3f2e805bd",
        "UNIFIED_GENIE_SPACE_ID": "01f0ea9368801e019e681aa3abaa0089",
        # LLM Configuration
        "LLM_ENDPOINT": "databricks-claude-3-7-sonnet",
        "LLM_TEMPERATURE": "0.3",
        # Memory Configuration
        "LAKEBASE_INSTANCE_NAME": "health_monitor_lakebase",
        # Feature Flags
        "ENABLE_LONG_TERM_MEMORY": "true",
        "ENABLE_WEB_SEARCH": "true",
        "ENABLE_MLFLOW_TRACING": "true",
    }
    
    print(f"\n  üì¶ Environment variables configured: {len(env_vars)} total")
    print(f"       ‚Ä¢ Genie Spaces: 6 configured")
    print(f"       ‚Ä¢ LLM: databricks-claude-3-7-sonnet")
    print(f"       ‚Ä¢ Memory: Lakebase enabled")
    
    client = WorkspaceClient()
    
    try:
        # Check if endpoint exists
        print(f"\n  üîç Checking for existing endpoint...")
        existing_endpoint = None
        try:
            existing_endpoint = client.serving_endpoints.get(endpoint_name)
            print(f"       ‚úì Found existing endpoint (will update)")
            if existing_endpoint.state:
                print(f"       Current state: {existing_endpoint.state.ready}")
        except Exception as e:
            print(f"       ‚Üí Endpoint does not exist (will create new)")
        
        # Build served entity
        print(f"\n  üèóÔ∏è  Building served entity configuration...")
        served_entity = ServedEntityInput(
            name="health_monitor_agent",
            entity_name=MODEL_NAME,
            entity_version=version,
            workload_size="Small",
            scale_to_zero_enabled=True,
            environment_vars=env_vars,
        )
        print(f"       ‚úì Entity: health_monitor_agent")
        print(f"       ‚úì Workload: Small (scale-to-zero enabled)")
        
        # Build AI Gateway config
        print(f"\n  üåê Configuring AI Gateway...")
        table_prefix = endpoint_name.replace("-", "_")
        ai_gateway = AiGatewayConfig(
            inference_table_config=AiGatewayInferenceTableConfig(
                catalog_name=catalog,
                schema_name=agent_schema,
                table_name_prefix=table_prefix,
                enabled=True,
            ),
            rate_limits=[
                AiGatewayRateLimit(
                    calls=100,
                    key=AiGatewayRateLimitKey.USER,
                    renewal_period=AiGatewayRateLimitRenewalPeriod.MINUTE,
                ),
            ],
            usage_tracking_config=AiGatewayUsageTrackingConfig(enabled=True),
        )
        print(f"       ‚úì Inference logging: {catalog}.{agent_schema}.{table_prefix}_*")
        print(f"       ‚úì Rate limit: 100 calls/minute per user")
        print(f"       ‚úì Usage tracking: enabled")
        
        if existing_endpoint:
            # Update existing endpoint
            print(f"\n  üîÑ Updating endpoint configuration...")
            client.serving_endpoints.update_config(
                name=endpoint_name,
                served_entities=[served_entity],
            )
            print(f"       ‚úì Update request submitted")
        else:
            # Create new endpoint
            print(f"\n  ‚ú® Creating new endpoint with AI Gateway...")
            endpoint_config = EndpointCoreConfigInput(served_entities=[served_entity])
            client.serving_endpoints.create(
                name=endpoint_name,
                config=endpoint_config,
                ai_gateway=ai_gateway,
            )
            print(f"       ‚úì Create request submitted")
        
        # Wait for endpoint to be ready (up to 15 minutes)
        print(f"\n  ‚è≥ Waiting for endpoint to be ready (up to 15 min)...")
        print(f"       Polling every 30 seconds...")
        wait_start = time.time()
        
        for i in range(30):  # 30 * 30 seconds = 15 minutes
            try:
                ep = client.serving_endpoints.get(endpoint_name)
                state = ep.state.ready if ep.state else "UNKNOWN"
                config_state = ep.state.config_update if ep.state else "UNKNOWN"
                
                elapsed = int(time.time() - wait_start)
                progress = "‚ñì" * min((i + 1), 30) + "‚ñë" * max(0, 30 - (i + 1))
                
                if state == "READY":
                    print(f"\n       [{progress}] {elapsed}s")
                    print(f"\n  ‚ïî{'‚ïê' * 50}‚ïó")
                    print(f"  ‚ïë{'‚úÖ ENDPOINT IS READY!':^50}‚ïë")
                    print(f"  ‚ïö{'‚ïê' * 50}‚ïù")
                    return True
                else:
                    print(f"       [{progress}] {elapsed}s - State: {state} | Config: {config_state}")
                    
            except Exception as poll_error:
                print(f"       ‚ö† Poll error: {str(poll_error)[:40]}")
                
            time.sleep(30)
        
        print(f"\n  ‚ö†Ô∏è  Endpoint created but not READY after 15 minutes")
        print(f"       Check the Serving Endpoints UI for status")
        print(f"       URL: https://<workspace>.databricks.com/serving-endpoints/{endpoint_name}")
        return True  # Still consider it a success - it was created
        
    except Exception as e:
        print(f"\n  ‚ïî{'‚ïê' * 50}‚ïó")
        print(f"  ‚ïë{'‚ùå ENDPOINT CREATION FAILED':^50}‚ïë")
        print(f"  ‚ïö{'‚ïê' * 50}‚ïù")
        print(f"\n  Error: {str(e)}")
        print(f"\n  Full traceback:")
        import traceback
        traceback.print_exc()
        return False

# COMMAND ----------

# ===========================================================================
# MAIN DEPLOYMENT JOB
# ===========================================================================

def main() -> str:
    """
    Main deployment job execution.
    
    Flow:
    1. Get model version to evaluate
    2. Load model
    3. Run comprehensive evaluation
    4. Check thresholds
    5. IF PASSED: Promote model AND Create/Update serving endpoint
    6. IF FAILED: No promotion, no endpoint creation
    
    This is the proper MLflow 3.0 Deployment Job pattern where
    endpoint creation is GATED by evaluation success.
    
    Returns:
        exit_code: String indicating job result (SUCCESS, PROMOTED_NO_ENDPOINT, EVALUATION_FAILED, or ERROR:...)
    """
    _section_header("STEP 1/6: MODEL VERSION DETECTION", "üì¶")
    
    try:
        version = get_model_version_to_evaluate()
        _step(f"Target model: {MODEL_NAME}", "‚úì")
        _step(f"Version to evaluate: {version}", "‚úì")
    except Exception as e:
        _step(f"Failed to get model version: {e}", "‚úó")
        return f"ERROR: {str(e)}"
    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    _section_header("STEP 2/6: MODEL LOADING", "üîÑ")
    
    try:
        _step(f"Loading model from Unity Catalog...", "‚Üí")
        load_start = _time.time()
        model = load_model_for_evaluation(version)
        load_time = _time.time() - load_start
        _step(f"Model loaded successfully ({load_time:.2f}s)", "‚úì")
    except Exception as e:
        _step(f"Failed to load model: {e}", "‚úó")
        import traceback
        traceback.print_exc()
        return f"ERROR: {str(e)}"
    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    _section_header("STEP 3/6: EVALUATION DATASET", "üìã")
    
    eval_data = get_evaluation_dataset()
    _step(f"Loaded {len(eval_data)} evaluation queries", "‚úì")
    
    # Show dataset composition
    categories = eval_data['category'].value_counts()
    for cat, count in categories.items():
        print(f"       ‚Ä¢ {cat}: {count} queries")
    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Step 4: Evaluation (already has section header inside)
    eval_start = _time.time()
    try:
        results = run_evaluation(model, eval_data)
    except Exception as e:
        _step(f"Evaluation failed with error: {e}", "‚úó")
        import traceback
        traceback.print_exc()
        return f"ERROR: Evaluation failed - {str(e)}"
        
    eval_time = _time.time() - eval_start
    
    _section_header("STEP 4/6: EVALUATION COMPLETE", "‚úÖ")
    _step(f"Total evaluation time: {eval_time:.1f}s", "‚è±")
    _step(f"Average per query: {eval_time/len(eval_data):.2f}s", "‚è±")
    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # Step 5: Check thresholds (already has section header inside)
    passed = check_evaluation_thresholds(results)
    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    _section_header("STEP 5/6: PROMOTION DECISION", "üéØ")
    
    promoted = False
    endpoint_created = False
    
    if passed:
        _step(f"Evaluation PASSED - proceeding with promotion", "‚úÖ")
        
        # 5a. Promote model alias
        if promotion_target in ["staging", "production"]:
            try:
                promote_model(version, promotion_target)
                promoted = True
                _step(f"Model promoted to @{promotion_target}", "‚úì")
            except Exception as e:
                _step(f"Promotion failed: {e}", "‚úó")
                import traceback
                traceback.print_exc()
        else:
            _step(f"Unknown promotion target: {promotion_target}", "‚ö†")
        
        # 5b. Create/Update serving endpoint (GATED by evaluation)
        _section_header("STEP 6/6: SERVING ENDPOINT", "üåê")
        
        if promoted and endpoint_name:
            _step(f"Creating/updating endpoint: {endpoint_name}", "‚Üí")
            endpoint_created = create_or_update_serving_endpoint(version)
            if endpoint_created:
                _step(f"Endpoint ready!", "‚úì")
            else:
                _step(f"Endpoint creation had issues (check logs above)", "‚ö†")
        else:
            _step(f"Skipping endpoint (promotion_target: {promotion_target})", "‚ö†")
    else:
        _step(f"Evaluation FAILED - blocking deployment", "‚ùå")
        _step(f"NOT promoting to @{promotion_target}", "‚Üí")
        _step(f"NOT creating/updating serving endpoint", "‚Üí")
        
        _section_header("STEP 6/6: ENDPOINT SKIPPED", "‚è≠")
        _step(f"Endpoint deployment blocked due to failed evaluation", "‚Üí")
    
    # Log results to MLflow
    _section_header("LOGGING RESULTS TO MLFLOW", "üìù")
    try:
        log_deployment_results(version, results, passed, promoted, endpoint_created)
        _step(f"Results logged to MLflow experiment: /Shared/health_monitor/agent", "‚úì")
    except Exception as e:
        _step(f"Failed to log results: {e}", "‚ö†")
        import traceback
        traceback.print_exc()
    
    # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # FINAL SUMMARY
    total_time = _time.time() - _job_start_time
    
    print("\n")
    print("‚ïî" + "‚ïê" * 68 + "‚ïó")
    print("‚ïë" + " DEPLOYMENT JOB SUMMARY ".center(68) + "‚ïë")
    print("‚ï†" + "‚ïê" * 68 + "‚ï£")
    print(f"‚ïë  üì¶ Model:     {MODEL_NAME:<51} ‚ïë")
    print(f"‚ïë  üî¢ Version:   {version:<51} ‚ïë")
    print(f"‚ïë  ‚è±  Duration:  {total_time:.1f} seconds{'':<42} ‚ïë")
    print("‚ï†" + "‚ïê" * 68 + "‚ï£")
    
    # Status indicators
    eval_status = "‚úÖ PASSED" if passed else "‚ùå FAILED"
    prom_status = f"‚úÖ @{promotion_target}" if promoted else "‚ùå Not promoted"
    endp_status = "‚úÖ READY" if endpoint_created else "‚ùå Not deployed"
    
    print(f"‚ïë  Evaluation:   {eval_status:<51} ‚ïë")
    print(f"‚ïë  Promotion:    {prom_status:<51} ‚ïë")
    print(f"‚ïë  Endpoint:     {endp_status:<51} ‚ïë")
    
    print("‚ï†" + "‚ïê" * 68 + "‚ï£")
    
    if promoted and endpoint_created:
        final_status = "üöÄ SUCCESS - Model deployed and serving!"
        exit_code = "SUCCESS"
    elif promoted:
        final_status = "‚ö†Ô∏è PARTIAL - Model promoted but endpoint not ready"
        exit_code = "PROMOTED_NO_ENDPOINT"
    else:
        final_status = "‚ùå BLOCKED - Evaluation failed, no deployment"
        exit_code = "EVALUATION_FAILED"
    
    print(f"‚ïë  {final_status:<66} ‚ïë")
    print("‚ïö" + "‚ïê" * 68 + "‚ïù")
    
    if endpoint_created:
        print(f"\n  üåê Endpoint URL: https://<workspace>.databricks.com/serving-endpoints/{endpoint_name}")
    
    print(f"\n  üìã Exit Code: {exit_code}")
    
    return exit_code

# COMMAND ----------

# ===========================================================================
# RUN MAIN DEPLOYMENT JOB
# ===========================================================================
# This cell executes the main deployment job and captures the exit code.
# The exit code is returned in a SEPARATE cell below so debug messages are visible.

exit_code = main()

# COMMAND ----------

# ===========================================================================
# EXIT NOTEBOOK
# ===========================================================================
# This is in a separate cell so all debug output from main() is visible
# before the notebook exits.

print(f"üèÅ Exiting notebook with code: {exit_code}")
dbutils.notebook.exit(exit_code)

