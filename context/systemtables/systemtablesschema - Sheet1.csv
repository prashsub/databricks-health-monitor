Audit log system table schema,,,,
system.access.audit,https://docs.databricks.com/aws/en/admin/system-tables/audit-logs,,,
Column name,Data type,Description,Example,
account_id,string,ID of the account,23e22ba4-87b9-4cc2-9770-d10b894bxx,
workspace_id,string,ID of the workspace,1234567890123456,
version,string,Audit log schema version,2,
event_time,timestamp,Timestamp of the event. Timezone information is recorded at the end of the value with +00:00 representing UTC timezone.,2023-01-01T01:01:01.123+00:00,
event_date,date,Calendar date the action took place,2023-01-01,
source_ip_address,string,IP address where the request originated,10.30.0.242,
user_agent,string,Origination of request,Apache-HttpClient/4.5.13 (Java/1.8.0_345),
session_id,string,ID of the session where the request came from,123456789,
user_identity,struct,Identity of user initiating request,"{""email"": ""user@domain.com"", ""subjectName"": null}",
service_name,string,Service name initiating request,unityCatalog,
action_name,string,Category of the event captured in audit log,getTable,
request_id,string,ID of request,ServiceMain-4529754264,
request_params,map,Map of key values containing all the request parameters. Depends on request type,"[[""full_name_arg"", ""user.chat.messages""], [""workspace_id"", ""123456789""], [""metastore_id"", ""123456789""]]",
response,struct,Struct of response return values,"{""statusCode"": 200, ""errorMessage"": null, ""result"": null}",
audit_level,string,Workspace or account level event,ACCOUNT_LEVEL,
event_id,string,ID of the event,34ac703c772f3549dcc8671f654950f0,
identity_metadata,struct,"Identities involved in the action, including run_by and run_as. See Auditing group dedicated compute activity.",{run_by: example@email.com; run_as: example@email.com;,
Table lineage schema,,,,
system.access.table_lineage,https://docs.databricks.com/aws/en/admin/system-tables/lineage,,,
Column name,Data type,Description,Example,
account_id,string,The ID of the Databricks account.,7af234db-66d7-4db3-bbf0-956098224879,
metastore_id,string,The ID of the Unity Catalog metastore.,5a31ba44-bbf4-4174-bf33-e1fa078e6765,
workspace_id,string,The ID of the workspace,123456789012345,
entity_type,string,"The type of entity associated with the lineage record, if any. The value is NOTEBOOK, JOB, PIPELINE, DASHBOARD_V3, DBSQL_DASHBOARD (legacy dashboard), DBSQL_QUERY, or NULL.
Note: please see entity_metadata column for information on all entities involved in the lineage record.",JOB,
entity_id,string,"The ID of the entity associated with the lineage record, or NULL.
Note: please see entity_metadata column for information on all entities involved in the lineage record.",417306252667357,
entity_run_id,string,"The unique run ID of the entity associated with the lineage record, or NULL.
Note: please see entity_metadata column for information on all entities involved in the lineage record.",688612062233399,
source_table_full_name,string,Three-part name to identify the source table.,catalog.schema.table,
source_table_catalog,string,The catalog of the source table.,catalog,
source_table_schema,string,The schema of the source table.,schema,
source_table_name,string,The name of the source table.,table,
source_path,string,"Location in cloud storage of the source table, or the path if it's reading from cloud storage directly.",s3://mybucket/table1,
source_type,string,"The type of the source. The value is TABLE, PATH, VIEW, MATERIALIZED_VIEW, METRIC_VIEW, or STREAMING_TABLE.",TABLE,
target_table_full_name,string,Three-part name to identify the target table.,catalog.schema.table,
target_table_catalog,string,The catalog of the target table.,catalog,
target_table_schema,string,The schema of the target table.,schema,
target_table_name,string,The name of the target table.,table,
target_path,string,Location in cloud storage of the target table.,s3://mybucket/table1,
target_type,string,"The type of the target. The value is TABLE, PATH, VIEW, MATERIALIZED_VIEW, METRIC_VIEW, or STREAMING_TABLE.",TABLE,
created_by,string,"The user who generated this lineage. This can be a Databricks username, a Databricks service principal ID, “System-User”, or NULL if the user information cannot be captured.",crampton.rods@email.com,
event_time,timestamp,The timestamp when the lineage was generated. Timezone information is recorded at the end of the value with +00:00 representing UTC.,2025-04-20T19:47:21.194+00:00,
event_date,date,The date when the lineage was generated. This is a partitioned column.,2025-04-20,
record_id,string,The unique ID of the lineage record. This value is auto-generated and cannot be joined with any tables.,3c5c8eed-87bb-3aa6-8a86-80d00d48299e,
event_id,string,The unique ID for a single lineage event. Multiple rows may share the same event_id if they were generated by the same event. This value is auto-generated and cannot be joined with any tables.,ca123ff3-f3f8-332b-a832-0154a6327353,
statement_id,string,The unique ID of a query statement that generated the lineage event. This is a foreign key to join with query history system table. This value is only set when the query is run from a SQL warehouse.,1234526f-a6ac-475c-8601-e8637f8ee039,
entity_metadata,struct,Metadata about the entity responsible for the lineage event.,"See Entity metadata: job_info:
  job_id: ""string""
  job_run_id: ""string""
dashboard_id: ""string""
legacy_dashboard_id: ""string""
notebook_id: ""string""
sql_query_id: ""string""
dlt_pipeline_info:
  dlt_pipeline_id: ""string""
  dlt_update_id: ""string""",
Billable usage table schema,,,,
system.billing.usage,https://docs.databricks.com/aws/en/admin/system-tables/billing,,,
Column name,Data type,Description,Example,
record_id,string,Unique ID for this usage record,11e22ba4-87b9-4cc2-9770-d10b894b7118,
account_id,string,ID of the account this report was generated for,23e22ba4-87b9-4cc2-9770-d10b894b7118,
workspace_id,string,ID of the workspace this usage was associated with,1234567890123456,
sku_name,string,Name of the SKU,STANDARD_ALL_PURPOSE_COMPUTE,
cloud,string,"Cloud associated with this usage. Possible values are AWS, AZURE, and GCP.","AWS, AZURE, or GCP",
usage_start_time,timestamp,The start time relevant to this usage record. Timezone information is recorded at the end of the value with +00:00 representing UTC timezone.,2023-01-09 10:00:00.000+00:00,
usage_end_time,timestamp,The end time relevant to this usage record. Timezone information is recorded at the end of the value with +00:00 representing UTC timezone.,2023-01-09 11:00:00.000+00:00,
usage_date,date,"Date of the usage record, this field can be used for faster aggregation by date",2023-01-01,
custom_tags,map,Custom tags associated with the usage record,{ “env”: “production” },
usage_unit,string,Unit this usage is measured in,DBU,
usage_quantity,decimal,Number of units consumed for this record,259.2958,
usage_metadata,struct,"System-provided metadata about the usage, including IDs for compute resources and jobs (if applicable). See Usage Metadata.",See Usage metadata,
identity_metadata,struct,System-provided metadata about the identities involved in the usage. See Identity Metadata.,See Identity metadata,
record_type,string,"Whether the record is original, a retraction, or a restatement. The value is ORIGINAL unless the record is related to a correction. See Record Type.",ORIGINAL,
ingestion_date,date,Date the record was ingested into the usage table,2024-01-01,
billing_origin_product,string,"The product that originated the usage. Some products can be billed as different SKUs. For possible values, see Product.",JOBS,
product_features,struct,Details about the specific product features used. See Product features.,See Product features,
usage_type,string,"The type of usage attributed to the product or workload for billing purposes. Possible values are COMPUTE_TIME, STORAGE_SPACE, NETWORK_BYTE, NETWORK_HOUR, API_OPERATION, TOKEN, or GPU_TIME.",STORAGE_SPACE,
Usage metadata reference Value,Description,Populated for (otherwise null),,
cluster_id,ID of the cluster associated with the usage record,"Non-serverless compute usage, including notebooks, jobs, Lakeflow Spark Declarative Pipelines, and legacy model serving",,
job_id,ID of the job associated with the usage record,Serverless jobs and jobs run on job compute (does not populate for jobs run on all-purpose compute),,
warehouse_id,ID of the SQL warehouse associated with the usage record,Workloads run on a SQL warehouse,,
instance_pool_id,ID of the instance pool associated with the usage record,"Non-serverless compute usage from pools, including notebooks, jobs, Lakeflow Spark Declarative Pipelines, and legacy model serving",,
node_type,The instance type of the compute resource,"Non-serverless compute usage, including notebooks, jobs, Lakeflow Spark Declarative Pipelines, and all SQL warehouses",,
job_run_id,ID of the job run associated with the usage record,Serverless jobs and jobs run on job compute (does not populate for jobs run on all-purpose compute),,
notebook_id,ID of the notebook associated with the usage,Serverless notebooks,,
dlt_pipeline_id,ID of the declarative pipeline associated with the usage record,"Lakeflow Spark Declarative Pipelines and features that use Lakeflow Spark Declarative Pipelines, such as materialized views, online tables, vector search indexing, and Lakeflow Connect",,
endpoint_name,The name of the model serving endpoint or vector search endpoint associated with the usage record,Model serving and Vector Search,,
endpoint_id,ID of the model serving endpoint or vector search endpoint associated with the usage record,Model serving and Vector Search,,
dlt_update_id,ID of the pipeline update associated with the usage record,"Lakeflow Spark Declarative Pipelines and features that use Lakeflow Spark Declarative Pipelines, such as materialized views, online tables, vector search indexing, and Lakeflow Connect",,
dlt_maintenance_id,ID of the pipeline maintenance tasks associated with the usage record,"Lakeflow Spark Declarative Pipelines and features that use Lakeflow Spark Declarative Pipelines, such as materialized views, online tables, vector search indexing, and Lakeflow Connect",,
metastore_id,ID of the metastore associated with the default storage,Default storage,,
run_name,Unique user-facing name of the Foundation Model Fine-tuning run associated with the usage record,Foundation Model Fine-tuning,,
job_name,User-given name of the job associated with the usage record,Serverless jobs and jobs run on job compute (populated for job compute since September 2025). Not populated for all-purpose compute.,,
notebook_path,Workspace storage path of the notebook associated with the usage,Notebooks run on serverless compute,,
central_clean_room_id,ID of the central clean room associated with the usage record,Clean Rooms,,
source_region,Region where billed traffic originated. Only returns a value for serverless networking-related usage.,Serverless networking,,
destination_region,Region where billed traffic was received. Only returns a value for serverless networking-related usage.,Serverless networking,,
app_id,ID of the app associated with the usage record,Databricks Apps,,
app_name,User-given name of the app associated with the usage record,Databricks Apps,,
private_endpoint_name,This value is not populated in Databricks on AWS,Always null on Databricks on AWS,,
budget_policy_id,ID of the serverless budget policy attached to the workload,"Serverless compute usage, including notebooks, jobs, Lakeflow Spark Declarative Pipelines, and model serving endpoints",,
storage_api_type,"The type of operation performed on default storage. Possible values are TIER_1 (PUT, COPY, POST, LIST) and TIER_2 (other operations)",Default storage,,
ai_runtime_workload_id,ID of the serverless GPU workload associated with the usage record,Serverless GPU workloads,,
uc_table_catalog,The Unity Catalog catalog name associated with the usage record,Materialized views,,
uc_table_schema,The Unity Catalog schema name associated with the usage record,Materialized views,,
uc_table_name,The Unity Catalog table name associated with the usage record,Materialized views,,
database_instance_id,ID of the database instance associated with the usage record,Lakebase database instances,,
sharing_materialization_id,ID of the sharing materialization associated with the usage record,"View sharing, materialized views, and streaming tables using Delta Sharing",,
usage_policy_id,ID of the usage policy associated with the usage record,Usage policies,,
agent_bricks_id,ID of the agent bricks workload associated with the usage record,Agent Bricks workloads,,
base_environment_id,ID of the base environment associated with the usage,Usage from building or refreshing a workspace's serverless base environment. Populated when billing_origin_product is BASE_ENVIRONMENTS.,,
Pricing table schema,,,,
system.billing.list_prices,https://docs.databricks.com/aws/en/admin/system-tables/pricing,,,
Column name,Data type,Description,Example,
price_start_time,timestamp,The time this price became effective in UTC,2023-01-01T09:59:59.999Z,
price_end_time,timestamp,The time this price stopped being effective in UTC,2023-01-01T09:59:59.999Z,
account_id,string,ID of the account this report was generated for,1234567890123456,
sku_name,string,Name of the SKU,STANDARD_ALL_PURPOSE_COMPUTE,
cloud,string,"Name of the Cloud this price is applicable to. Possible values are AWS, AZURE, and GCP.","AWS, AZURE, or GCP",
currency_code,string,The currency this price is expressed in,USD,
usage_unit,string,The unit of measurement that is monetized.,DBU,
pricing,struct,"A structured data field that includes pricing info at the published list price rate. The key default will always return a single price that can be used for simple long-term estimates. The key promotional represents a temporary promotional price that all customers get which could be used for cost estimation during the temporary period. The key effective_list resolves list and promotional price, and contains the effective list price used for calculating the cost. Some pricing models might also include additional keys that provide more detail.","{
""default"": ""0.10"",
""promotional"": {""default"": ""0.07""},
""effective_list"": {""default"": ""0.07""}
}",
Cluster table schema,,,,
system.compute.clusters,https://docs.databricks.com/aws/en/admin/system-tables/compute,,,
Column name,Data type,Description,Example,
account_id,string,ID of the account where this cluster was created.,23e22ba4-87b9- 4cc2-9770-d10b894b7118,
workspace_id,string,ID of the workspace where this cluster was created.,1234567890123456,
cluster_id,string,ID of the cluster for which this record is associated.,0000-123456-xxxxxxxx,
cluster_name,string,User defined name for the cluster.,My cluster,
owned_by,string,"Username of the cluster owner. Defaults to the cluster creator, but can be changed through the Clusters API.",sample_user@email.com,
create_time,timestamp,Timestamp of the change to this compute definition.,2023-01-09 11:00:00,
delete_time,timestamp,Timestamp of when the cluster was deleted. The value is null if the cluster is not deleted.,2023-01-09 11:00:00,
driver_node_type,string,Driver node type name. This matches the instance type name from the cloud provider.,i3.xlarge,
worker_node_type,string,Worker node type name. This matches the instance type name from the cloud provider.,i3.xlarge,
worker_count,bigint,Number of workers. Defined for fixed-size clusters only.,4,
min_autoscale_workers,bigint,The set minimum number of workers. This field is valid only for autoscaling clusters.,1,
max_autoscale_workers,bigint,The set maximum number of workers. This field is valid only for autoscaling clusters.,1,
auto_termination_minutes,bigint,The configured autotermination duration.,120,
enable_elastic_disk,boolean,Autoscaling disk enablement status.,TRUE,
tags,map,User-defined tags for the cluster (does not include default tags).,"{""ResourceClass"":""SingleNode""}",
cluster_source,string,Source of the cluster. The UI or API values only apply to all-purpose compute. All job compute is logged as JOB. Pipelines are PIPELINE or PIPELINE_MAINTENANCE.,UI,
init_scripts,array,Set of paths for init scripts.,"""/Users/example@email.com /files/scripts/install-python-pacakges.sh""",
aws_attributes,struct,AWS specific settings.,"{ ""ebs_volume_count"": null, ""availability"": ""SPOT_WITH_FALLBACK"", ""first_on_demand"": ""0"", ""spot_bid_price_percent"": ""100"" }",
azure_attributes,struct,Azure specific settings.,null,
gcp_attributes,struct,GCP specific settings. This field will be empty.,null,
driver_instance_pool_id,string,Instance pool ID if the driver is configured on top of an instance pool.,1107-555555-crhod16-pool-DIdnjazB,
worker_instance_pool_id,string,Instance Pool ID if the worker is configured on top of an instance pool.,1107-555555-crhod16-pool-DIdnjazB,
dbr_version,string,The Databricks Runtime of the cluster.,14.x-snapshot-scala2.12,
change_time,timestamp,Timestamp of change to the compute definition.,2023-01-09 11:00:00,
change_date,date,Change date. Used for retention.,2023-01-09,
data_security_mode,string,The access mode of the compute resource. See Access mode reference.,USER_ISOLATION,
policy_id,string,"ID of the cluster's compute policy, if applicable.",1234F35636110A5B,
Warehouses table schema,,,,
system.compute.warehouses,https://docs.databricks.com/aws/en/admin/system-tables/warehouses,,,
Column name,Data type,Description,Example,
warehouse_id,string,The ID of the SQL warehouse.,123456789012345,
workspace_id,string,The ID of the workspace where the warehouse is deployed.,123456789012345,
account_id,string,The ID of the Databricks account.,7af234db-66d7-4db3-bbf0-956098224879,
warehouse_name,string,The name of the SQL warehouse.,My Serverless Warehouse,
warehouse_type,string,"The type of SQL warehouse. Possible values are CLASSIC, PRO, and SERVERLESS.",SERVERLESS,
warehouse_channel,string,The channel of the SQL warehouse. Possible values are CURRENT and PREVIEW.,CURRENT,
warehouse_size,string,"The cluster size of the SQL warehouse. Possible values are 2X_SMALL, X_SMALL, SMALL, MEDIUM, LARGE, X_LARGE, 2X_LARGE, 3X_LARGE, and 4X_LARGE.",MEDIUM,
min_clusters,int,The minimum number of clusters permitted.,1,
max_clusters,int,The maximum number of clusters permitted.,5,
auto_stop_minutes,int,The number of minutes before the SQL warehouse auto-stops due to inactivity.,35,
tags,map,Tags for the SQL warehouse.,"{""budget"":""research""}",
change_time,timestamp,Timestamp of change to the SQL warehouse definition.,2023-07-20T19:13:09.504Z,
delete_time,timestamp,Timestamp of when the SQL warehouse was deleted. The value is null if the SQL warehouse is not deleted.,2023-07-20T19:13:09.504Z,
Warehouse events schema,,,,
system.compute.warehouse_events,https://docs.databricks.com/aws/en/admin/system-tables/warehouse-events,,,
Column name,Data type,Description,Example,
account_id,string,The ID of the Databricks account.,7af234db-66d7-4db3-bbf0-956098224879,
workspace_id,string,The ID of the workspace where the warehouse is deployed.,123456789012345,
warehouse_id,string,The ID of SQL warehouse the event is related to.,123456789012345,
event_type,string,"The type of warehouse event. Possible values are SCALED_UP, SCALED_DOWN, STOPPING, RUNNING, STARTING, and STOPPED.",SCALED_UP,
cluster_count,integer,The number of clusters that are actively running.,2,
event_time,timestamp,Timestamp of when the event took place in UTC.,2023-07-20T19:13:09.504Z,
Jobs system table reference,,,,
system.lakeflow schema. ,https://docs.databricks.com/aws/en/admin/system-tables/jobs,,,
Column name,Data type,Description,Notes,
account_id,string,The ID of the account this job belongs to,,
workspace_id,string,The ID of the workspace this job belongs to,,
job_id,string,The ID of the job,Only unique within a single workspace,
name,string,The user-supplied name of the job,,
description,string,The user-supplied description of the job,This field is empty if you have customer-managed keys configured.,
creator_id,string,The ID of the principal who created the job,,
tags,map,The user-supplied custom tags associated with this job,,
change_time,timestamp,The time when the job was last modified,Timezone recorded as +00:00 (UTC),
delete_time,timestamp,The time when the job was deleted by the user,Timezone recorded as +00:00 (UTC),
run_as,string,The ID of the user or service principal whose permissions are used for the job run,,
Job task table schema,,,,
Column name,Data type,Description,Notes,
account_id,string,The ID of the account this job belongs to,,
workspace_id,string,The ID of the workspace this job belongs to,,
job_id,string,The ID of the job,Only unique within a single workspace,
task_key,string,The reference key for a task in a job,Only unique within a single job,
depends_on_keys,array,The task keys of all upstream dependencies of this task,,
change_time,timestamp,The time when the task was last modified,Timezone recorded as +00:00 (UTC),
delete_time,timestamp,The time when a task was deleted by the user,Timezone recorded as +00:00 (UTC),
Job run timeline table schema,,,,
Column name,Data type,Description,Notes,
account_id,string,The ID of the account this job belongs to,,
workspace_id,string,The ID of the workspace this job belongs to,,
job_id,string,The ID of the job,This key is only unique within a single workspace,
run_id,string,The ID of the job run,,
period_start_time,timestamp,The start time for the run or for the time period,"Timezone information is recorded at the end of the value with +00:00 representing UTC. For details on how Databricks slices long runs into hourly intervals, see timeline slicing logic.",
period_end_time,timestamp,The end time for the run or for the time period,"Timezone information is recorded at the end of the value with +00:00 representing UTC. For details on how Databricks slices long runs into hourly intervals, see timeline slicing logic.",
trigger_type,string,The type of trigger that can fire a run,"For possible values, see Trigger type values",
run_type,string,The type of job run,"For possible values, see Run type values",
run_name,string,The user-supplied run name associated with this job run,,
compute_ids,array,Array containing the job compute IDs for the parent job run,"Use for identifying the job cluster used by WORKFLOW_RUN run types. For other compute information, refer to the job_task_run_timeline table.",
result_state,string,The outcome of the job run,"For runs longer than one hour that are split across multiple rows, this column is populated only in the row that represents the end of the run. For possible values, see Result state values.",
termination_code,string,The termination code of the job run,"For runs longer than one hour that are split across multiple rows, this column is populated only in the row that represents the end of the run. For possible values, see Termination code values.",
job_parameters,map,The job-level parameters used in the job run,The deprecated notebook_params settings are not included in this field.,
Job task run timeline table schema,,,,
Column name,Data type,Description,Notes,
account_id,string,The ID of the account this job belongs to,,
workspace_id,string,The ID of the workspace this job belongs to,,
job_id,string,The ID of the job,Only unique within a single workspace,
run_id,string,The ID of the task run,,
job_run_id,string,The ID of the job run,,
parent_run_id,string,The ID of the parent run,,
period_start_time,timestamp,The start time for the task or for the time period,"Timezone information is recorded at the end of the value with +00:00 representing UTC. For details on how Databricks slices long runs into hourly intervals, see timeline slicing logic.",
period_end_time,timestamp,The end time for the task or for the time period,"Timezone information is recorded at the end of the value with +00:00 representing UTC. For details on how Databricks slices long runs into hourly intervals, see timeline slicing logic.",
task_key,string,The reference key for a task in a job,This key is only unique within a single job,
compute_ids,array,"The compute_ids array contains IDs of job clusters, interactive clusters, and SQL warehouses used by the job task",,
result_state,string,The outcome of the job task run,"For task runs longer than one hour that are split across multiple rows, this column is populated only in the row that represents the end of the run. For possible values, see Result state values.",
termination_code,string,The termination code of the task run,"For task runs longer than one hour that are split across multiple rows, this column is populated only in the row that represents the end of the run. For possible values, see Termination code values.",
Pipelines table schema,,,,
Column name,Data type,Description,Notes,
account_id,string,The ID of the account this pipeline belongs to,,
workspace_id,string,The ID of the workspace this pipeline belongs to,,
pipeline_id,string,The ID of the pipeline,Only unique within a single workspace,
pipeline_type,string,The type of the pipeline,"For possible values, see Pipeline type values",
name,string,The user-supplied name of the pipeline,,
created_by,string,The email of the user or the ID of the service principal that created the pipeline,,
run_as,string,The email of the user or ID of the service principal whose permissions are used for the pipeline run,,
tags,map,The user-supplied custom tags associated with this job,,
settings,struct,The settings of the pipeline,See Pipeline settings,
configuration,map,The user-supplied configuration of the pipeline,,
change_time,timestamp,The time when the pipeline was last modified,Timezone recorded as +00:00 (UTC),
delete_time,timestamp,The time when the pipeline was deleted by the user,Timezone recorded as +00:00 (UTC),
Pipeline update timeline table schema,,,,
Column name,Data type,Description,Notes,
account_id,string,The ID of the account this pipeline belongs to,,
workspace_id,string,The ID of the workspace this pipeline belongs to,,
pipeline_id,string,The ID of the pipeline,Only unique within a single workspace,
update_id,string,The ID of the pipeline update,Only unique within a single workspace,
update_type,string,The type of the pipeline update,"For possible values, see Pipeline update type values",
request_id,string,The ID of the request. Helps to understand how many times an update had to be retried/restarted,,
run_as_user_name,string,The email/ID of the service principal whose permissions are used for the pipeline update,,
trigger_type,string,What triggered this update,"For possible values, see Pipeline trigger type values",
trigger_details,struct,The details of the pipeline's trigger,"For possible values, see Pipeline trigger type details",
result_state,string,The outcome of the pipeline update,"For updates running across more than 1 hour hours that are split across multiple rows, this column is populated only in the row that represents the end of the update. For possible values, see Pipeline result reference.",
compute,struct,Details about the compute resource used in the pipeline update,,
period_start_time,timestamp,The start time for the pipeline update or for the hour. The value is stored as a UTC timestamp.,"Timezone information is recorded at the end of the value with +00:00 representing UTC. For details on how Databricks slices long runs into hourly intervals, see timeline slicing logic.",
period_end_time,timestamp,The end time for the pipeline update or for the hour. The value is stored as a UTC timestamp.,"Timezone information is recorded at the end of the value with +00:00 representing UTC. For details on how Databricks slices long runs into hourly intervals, see timeline slicing logic.",
refresh_selection,array,A list of tables to update without fullRefresh,,
full_refresh_selection,array,A list of tables to update with fullRefresh,,
reset_checkpoint_selection,array,A list of streaming flows to clear the checkpoints for,,
Query history system table schema,,,,
system.query.history,https://docs.databricks.com/aws/en/admin/system-tables/query-history,,,
Column name,Data type,Description,Example,
account_id,string,ID of the account.,"11e22ba4-87b9-4cc2
-9770-d10b894b7118",
workspace_id,string,The ID of the workspace where the query was run.,1234567890123456,
statement_id,string,The ID that uniquely identifies the execution of the statement. You can use this ID to find the statement execution in the Query History UI.,"7a99b43c-b46c-432b
-b0a7-814217701909",
session_id,string,The Spark session ID.,"01234567-cr06-a2mp
-t0nd-a14ecfb5a9c2",
execution_status,string,"The statement termination state. Possible values are:
- FINISHED: execution was successful
- FAILED: execution failed with the reason for failure described in the accompanying error message
- CANCELED: execution was canceled",FINISHED,
compute,struct,A struct that represents the type of compute resource used to run the statement and the ID of the resource where applicable. The type value will be either WAREHOUSE or SERVERLESS_COMPUTE.,"{
type: WAREHOUSE,
cluster_id: NULL,
warehouse_id: ec58ee3772e8d305
}",
executed_by_user_id,string,The ID of the user who ran the statement.,2967555311742259,
executed_by,string,The email address or username of the user who ran the statement.,example@databricks.com,
statement_text,string,"Text of the SQL statement. If you have configured customer-managed keys, statement_text is empty. Due to storage limitations, longer statement text values are compressed. Even with compression, you may reach a character limit.",SELECT 1,
statement_type,string,"The statement type. For example: ALTER, COPY, and INSERT.",SELECT,
error_message,string,"Message describing the error condition. If you have configured customer-managed keys, error_message is empty.","[INSUFFICIENT_PERMISSIONS]
Insufficient privileges:
User does not have
permission SELECT on table
'default.nyctaxi_trips'.",
client_application,string,"Client application that ran the statement. For example: Databricks SQL Editor, Tableau, and Power BI. This field is derived from information provided by client applications. While values are expected to remain static over time, this cannot be guaranteed.",Databricks SQL Editor,
client_driver,string,"The connector used to connect to Databricks to run the statement. For example: Databricks SQL Driver for Go, Databricks ODBC Driver, Databricks JDBC Driver.",Databricks JDBC Driver,
cache_origin_statement_id,string,"For query results fetched from cache, this field contains the statement ID of the query that originally inserted the result into the cache. If the query result is not fetched from cache, this field contains the query's own statement ID.","01f034de-5e17-162d
-a176-1f319b12707b",
total_duration_ms,bigint,Total execution time of the statement in milliseconds (excluding result fetch time).,1,
waiting_for_compute_duration_ms,bigint,Time spent waiting for compute resources to be provisioned in milliseconds.,1,
waiting_at_capacity_duration_ms,bigint,Time spent waiting in queue for available compute capacity in milliseconds.,1,
execution_duration_ms,bigint,Time spent executing the statement in milliseconds.,1,
compilation_duration_ms,bigint,Time spent loading metadata and optimizing the statement in milliseconds.,1,
total_task_duration_ms,bigint,The sum of all task durations in milliseconds. This time represents the combined time it took to run the query across all cores of all nodes. It can be significantly longer than the wall-clock duration if multiple tasks are executed in parallel. It can be shorter than the wall-clock duration if tasks wait for available nodes.,1,
result_fetch_duration_ms,bigint,"Time spent, in milliseconds, fetching the statement results after the execution finished.",1,
start_time,timestamp,The time when Databricks received the request. Timezone information is recorded at the end of the value with +00:00 representing UTC.,2022-12-05T00:00:00.000+0000,
end_time,timestamp,"The time the statement execution ended, excluding result fetch time. Timezone information is recorded at the end of the value with +00:00 representing UTC.",2022-12-05T00:00:00.000+00:00,
update_time,timestamp,The time the statement last received a progress update. Timezone information is recorded at the end of the value with +00:00 representing UTC.,2022-12-05T00:00:00.000+00:00,
read_partitions,bigint,The number of partitions read after pruning.,1,
pruned_files,bigint,The number of pruned files.,1,
read_files,bigint,The number of files read after pruning.,1,
read_rows,bigint,Total number of rows read by the statement.,1,
produced_rows,bigint,Total number of rows returned by the statement.,1,
read_bytes,bigint,Total size of data read by the statement in bytes.,1,
read_io_cache_percent,int,The percentage of bytes of persistent data read from the IO cache.,50,
from_result_cache,boolean,TRUE indicates that the statement result was fetched from the cache.,TRUE,
spilled_local_bytes,bigint,"Size of data, in bytes, temporarily written to disk while executing the statement.",1,
written_bytes,bigint,The size in bytes of persistent data written to cloud object storage.,1,
written_rows,bigint,The number of rows of persistent data written to cloud object storage.,1,
written_files,bigint,Number of files of persistent data written to cloud object storage.,1,
shuffle_read_bytes,bigint,The total amount of data in bytes sent over the network.,1,
query_source,struct,"A struct that contains key-value pairs representing Databricks entities that were involved in the execution of this statement, such as jobs, notebooks, or dashboards. This field only records Databricks entities.","{
alert_id: 81191d77-184f-4c4e-9998-b6a4b5f4cef1,
sql_query_id: null,
dashboard_id: null,
notebook_id: null,
job_info: {
job_id: 12781233243479,
job_run_id: null,
job_task_run_id: 110373910199121
},
legacy_dashboard_id: null,
genie_space_id: null
}",
query_parameters,struct,A struct containing named and positional parameters used in parameterized queries. Named parameters are represented as key-value pairs mapping parameter names to values. Positional parameters are represented as a list where the index indicates the parameter position. Only one type (named or positional) can be present at a time.,"{
named_parameters: {
""param-1"": 1,
""param-2"": ""hello""
},
pos_parameters: null,
is_truncated: false
}",
executed_as,string,The name of the user or service principal whose privilege was used to run the statement.,example@databricks.com,
executed_as_user_id,string,The ID of the user or service principal whose privilege was used to run the statement.,2967555311742259,
Listing access events table,,,,
system.marketplace.listing_access_events,https://docs.databricks.com/aws/en/admin/system-tables/marketplace,,,
Column name,Data type,Description,,
account_id,string,The account ID that hosts the listing.,,
metastore_id,string,The metastore ID that hosts the listing.,,
metastore_cloud,string,The cloud provider of the metastore that hosts the listing.,,
metastore_region,string,The region of the metastore that hosts the listing.,,
provider_id,string,The provider profile ID.,,
provider_name,string,The provider profile name.,,
listing_id,string,The listing ID.,,
listing_name,string,The listing name.,,
consumer_delta_sharing_recipient_name,string,The underlying Delta Sharing recipient name for the consumer. The value is null when the event_type is REQUEST_DATA.,,
consumer_delta_sharing_recipient_type,string,Whether the consumer is on a Databricks account or not. Values will be either OPEN or DATABRICKS.,,
consumer_cloud,string,The consumer's cloud. Nullable if consumer_delta_sharing_recipient_type is OPEN.,,
consumer_region,string,The consumer's region. Nullable if consumer_delta_sharing_recipient_type is OPEN.,,
consumer_metastore_id,string,The consumer's metastore ID. Nullable if consumer_delta_sharing_recipient_type is OPEN.,,
consumer_email,string,The consumer's email address.,,
consumer_name,string,The consumer's name.,,
consumer_company,string,The consumer's company.,,
consumer_intended_use,string,The consumer's intended use of the listing.,,
consumer_comments,string,Any additional comment the consumer left.,,
event_type,string,The type of access. The value can be either REQUEST_DATA or GET_DATA.,,
event_date,date,The UTC date the event happened.,,
event_time,timestamp,The exact UTC timestamp when the event happened. Timezone information is recorded at the end of the value with +00:00 representing UTC.,,
Predictive optimization table schema,,,,
system.storage.predictive_optimization_operations_history,https://docs.databricks.com/aws/en/admin/system-tables/predictive-optimization,,,
Column name,Data type,Description,Example,
account_id,string,ID of the account.,11e22ba4-87b9-4cc2-9770-d10b894b7118,
workspace_id,string,The ID of the workspace in which predictive optimization ran the operation.,1234567890123456,
start_time,timestamp,The time at which the operation started. Timezone information is recorded at the end of the value with +00:00 representing UTC.,2023-01-09 10:00:00.000+00:00,
end_time,timestamp,The time at which the operation ended. Timezone information is recorded at the end of the value with +00:00 representing UTC.,2023-01-09 11:00:00.000+00:00,
metastore_name,string,The name of the metastore to which the optimized table belongs.,metastore,
metastore_id,string,The ID of the metastore to which the optimized table belongs.,5a31ba44-bbf4-4174-bf33-e1fa078e6765,
catalog_name,string,The name of the catalog to which the optimized table belongs.,catalog,
schema_name,string,The name of the schema to which the optimized table belongs.,schema,
table_id,string,The ID of the optimized table.,138ebb4b-3757-41bb-9e18-52b38d3d2836,
table_name,string,The name of the optimized table.,table1,
operation_type,string,"The optimization operation which was performed. The value will be COMPACTION, VACUUM, ANALYZE, or CLUSTERING.",COMPACTION,
operation_id,string,The ID for the optimization operation.,4dad1136-6a8f-418f-8234-6855cfaff18f,
operation_status,string,The status of the optimization operation. The value will be SUCCESSFUL or FAILED: INTERNAL_ERROR.,SUCCESSFUL,
operation_metrics,"map[string, string]",Additional details about the specific optimization that was performed. See Operation metrics.,"{""number_of_output_files"":""100"",""number_of_compacted_files"":""1000"",""amount_of_output_data_bytes"":""4000"",""amount_of_data_compacted_bytes"":""10000""}",
usage_unit,string,The unit of usage that this operation incurred. Can only be one value: ESTIMATED_DBU.,ESTIMATED_DBU,
usage_quantity,decimal,The amount of the usage unit that was used by this operation.,2.12,
Assistant events schema,,,,
system.access.assistant_events,https://docs.databricks.com/aws/en/admin/system-tables/assistant,,,
Event,Data type,Description,Example,
event_id,string,A unique ID for this event.,80899c5dfe4b6b9505f3ea5a90e9c97f7ab9fa8f,
account_id,string,ID of the account.,ca886134-876c-4671-a38b-332edf48c602,
workspace_id,string,ID of the workspace.,2548836972759138,
event_time,timestamp,Time that the event happened. Timezone information is recorded at the end of the value with +00:00 representing UTC.,2024-01-05T00:00:00.000+00:00,
event_date,string,Date that the event happened.,2024-01-05,
user_agent,string,Origination of request.,"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36",
initiated_by,string,Email of the user initiating the request.,abc@databricks.com,
Clean room events system table schema,,,,
system.access.clean_room_events,https://docs.databricks.com/aws/en/admin/system-tables/clean-rooms,,,
Column name,Data type,Description,Example,
account_id,string,The ID of the Databricks account,"7af234db-66d7-4db3
-bbf0-956098224879",
metastore_id,string,The ID of the UC metastore,"5a31ba44-bbf4-4174
-bf33-e1fa078e6765",
event_id,string,The ID of the clean room event,"db52a413-7a0a-4d49
-b742-7ae5f06bc4b2",
clean_room_name,string,Name of the clean room associated with the event,market-analysis,
central_clean_room_id,string,The ID of central clean room,"e01b6a78-1336-47e1
-b63d-3e49aa5b627",
initiator_global_metastore_id,string,Global metastore ID of the collaborator who initiated the event,"aws:us-west-2:ec22936d
-cd29-4421-a88a-883fb356776a",
event_time,timestamp,Timestamp when the event took place,2023-01-01 1:01:01,
event_type,string,The type of the event. See Logged clean room events.,CLEAN_ROOM_CREATED,
clean_room_created_metadata,struct,The metadata of the event type CLEAN_ROOM_CREATED,"{
""collaborators"": [
{""collaborator_global_metastore_id"":
""aws:us-west-2:ec22936d-cd29-4421-a88a-883fb356776a""},
{""collaborator_global_metastore_id"":
""azure:westus:3be05a26-5a83-478c-8428-9ef9aa67b5d0""}]
}",
clean_room_deleted_metadata,struct,The metadata of the event type CLEAN_ROOM_DELETED,"{
""central_clean_room_id"": ""af0d4563-2267-412c-9d4a
-8a59c4895c18""
}",
run_notebook_started_metadata,struct,The metadata of the event type RUN_NOTEBOOK_STARTED,"{
""notebook_name"": ""Market Analysis"",
""notebook_checksum"": ""7072696E7468656C6C6F776F7264"",
""run_id"": ""TaskRunId-634124444694206"",
""notebook_etag"": ""f2429adc02d548bacc11db6c8891ec2548d38b4798a80810fbc8be784af22931"",
""notebook_update_time"": ""2025-05-04T03:20:35.000Z""
}",
run_notebook_completed_metadata,struct,The metadata of the event type RUN_NOTEBOOK_COMPLETED,See Completed notebook run metadata,
clean_room_assets_updated_metadata,struct,The metadata of the event type CLEAN_ROOM_ASSETS_UPDATED,"{
""added_assets"": [{
""data_object_type"": ""TABLE"",
""name"": ""sales"",
""catalog"": ""demo""}],
""updated_assets"":[],
""removed_assets"": []
}",
asset_review_created_metadata,struct,The metadata of the event type ASSET_REVIEW_CREATED,See Clean room asset review events,
output_schema_deleted_metadata,struct,The metadata of the event type OUTPUT_SCHEMA_DELETED,"{
""name"": ""output_schema_55555"",
""owner_global_metastore_id"": ""aws:us-west-2:555555-55..."",
""action"": ""DELETE"",
""expire_time"": ""2025-01-14T15:28:19.000Z""
}",
initiator_collaborator_alias,string,"Alias of the collaborator who initiated the event. For clean rooms created through the UI, the value is either creator or collaborator. For clean rooms created via API, they are the custom values specified on creation.",creator,
Outbound network access events system table schema,,,,
system.access.outbound_network,https://docs.databricks.com/aws/en/admin/system-tables/network,,,
Column name,Data type,Description,Example,
account_id,string,The ID of the Databricks account,7af234db-66d7-4db3-bbf0-956098224879,
workspace_id,string,The ID of the workspace where the event occurred,1234567890123456,
event_id,string,The ID of the event,db52a413-7a0a-4d49-b742-7ae5f06bc4b2,
destination_type,string,"The type of destination. Possible values are DNS, IP, and STORAGE",DNS,
destination,string,"Details of the blocked destination. Depending on the destination type, the value could be a domain name, IP address, or storage location.",google.com,
dns_event,struct,"Details about the DNS destination. Only populates for DNS destinations, otherwise the field is NULL.","{ ""domain_name"":""google.com"", ""rcode"": 3 }",
storage_event,struct,"Details about the storage destination. Only populates for storage destinations, otherwise the field is NULL.","{ ""hostname"":""s3://some-bucket"", ""path"": ""/some-path"", ""rejection_reason"": ""storage-bucket-path-denied"" }",
event_time,timestamp,Timestamp when the event took place,2024-05-01 1:01:01,
access_type,string,Type of access event that occurred.,DROP,
network_source_type,string,The specific product or service used within the workspace where the event occurred.,"DBSQL, General Compute, MLServing, ML Build, Apps",
Inbound network access events system table schema,,,,
system.access.inbound_network,,,,
Column name,Data type,Description,Example,
account_id,string,The ID of the Databricks account.,7af234db-66d7-4db3-bbf0-956098224879,
workspace_id,string,The ID of the workspace where the event occurred.,1234567890123456,
event_id,string,The ID of the event.,db52a413-7a0a-4d49-b742-7ae5f06bc4b2,
request_path,string,The destination of the request.,/compute,
source,struct,"The source of the request. Contains IP, private link, and related attributes.","{ ""ip"": ""10.0.0.1"", ""private-link"": ""some-pl-id"" }",
authenticated_as,string,"The authenticated identity of the request. Must be one of the following:
<user>@<domain-name>

<sp-application-id>

group_name",user@databricks.com,
policy_id,string,The ID of the ingress policy that evaluated the request.,fbc3a2a1-ef12-43b8-9e88-f024ac219ba5,
event_time,timestamp,Timestamp when the event took place.,2024-05-01 1:01:01,
policy_outcome,string,Type of access event outcome. Possible values are DENY or DENY_DRY_RUN.,DENY,
Shared materialized data history system table schema,,,,
system.sharing.materialization_history,https://docs.databricks.com/aws/en/admin/system-tables/materialization,,,
Column Name,Type,Description,Example Data,Nullable
sharing_materialization_id,string,The unique ID of a data materialization.,da38803f-2a62-4e27-bdb9-29b801c6dd84,FALSE
account_id,string,The ID of the Databricks account where the materialization was created.,,FALSE
workspace_id,string,The ID of the Databricks workspace billed.,6051921418418893,FALSE
recipient_name,string,Name of the recipient using the data materialization.,e2-dogfood,TRUE
provider_name,string,Name of the provider using the data materialization.,aws:us-west-2:19a85dee-54bc-43a2-87ab-023d0ec16013,TRUE
share_name,string,Name of the share used to create data materialization.,my_share,FALSE
schema_name,string,Name of the schema of the shared asset.,my_schema,FALSE
table_name,string,Name of the table used to create data materialization.,stocks,FALSE
created_at,timestamp,Timestamp of when the materialization was created.,2025-01-01 0:00:00,FALSE
Workspaces table schema,,system.access.workspaces_latest,,
system.access.workspaces_latest,https://docs.databricks.com/aws/en/admin/system-tables/workspaces,,,
Column name,Data type,Description,Example,
account_id,string,ID of the Databricks account,0722779a-fd4e-49c1-a7a6-8417a97cf9ea,
workspace_id,string,ID of the Databricks workspace,2274721051152826',
workspace_name,string,The human-readable name of the workspace,dough-re-mi,
workspace_url,string,URL of the workspace,https://dough-re-mi-pizza.cloud.databricks.com/,
create_time,timestamp,Timestamp of when the workspace was created (second precision),2025-03-05 15:47,
status,enum,"The status of the workspace. For workspace creation, it is set to PROVISIONING initially. Continue to check the status until the status is RUNNING.","NOT_PROVISIONED, PROVISIONING, RUNNING, FAILED, BANNED",
system.mlflow.experiments_latest,,,,
Column name,Data type,Description,Example,Nullable
account_id,string,The ID of the account containing the MLflow experiment,"""bd59efba-4444-4444-443f-44444449203""",No
update_time,timestamp,The system time when the experiment was last updated,2024-06-27T00:58:57.000+00:00,No
delete_time,timestamp,The system time when the MLflow experiment was soft-deleted by the user,2024-07-02T12:42:59.000+00:00,Yes
experiment_id,string,The ID of the MLflow experiment,"""2667956459304720""",No
workspace_id,string,The ID of the workspace containing the MLflow experiment,"""6051921418418893""",No
name,string,User-provided name of the experiment,"""/Users/first.last@databricks.com/myexperiment""",No
create_time,timestamp,The system time when the experiment was created,2024-06-27T00:58:57.000+00:00,No
system.mlflow.runs_latest,,,,
Column name,Data type,Description,Example,Nullable
account_id,string,The ID of the account containing the MLflow run,"""bd59efba-4444-4444-443f-44444449203""",No
update_time,timestamp,The system time when the run was last updated,2024-06-27T00:58:57.000+00:00,No
delete_time,timestamp,The system time when the MLflow run was soft-deleted by the user,2024-07-02T12:42:59.000+00:00,Yes
workspace_id,string,The ID of the workspace containing the MLflow run,"""6051921418418893""",No
run_id,string,The ID of the MLflow run,"""7716d750d279487c95f64a75bff2ad56""",No
experiment_id,string,The ID of the MLflow experiment containing the MLflow run,"""2667956459304720""",No
created_by,string,The name of the Databricks principal or user that created the MLflow run,"""<user>@<domain-name>""",Yes
start_time,timestamp,The user-specified time when the MLflow run started,2024-06-27T00:58:57.000+00:00,No
end_time,timestamp,The user-specified time when the MLflow run ended,2024-07-02T12:42:59.000+00:00,Yes
run_name,string,The name of the MLflow run,"""wistful-deer-932"", ""my-xgboost-training-run""",No
status,string,The execution status of the MLflow run,"""FINISHED""",No
params,"map<string, string>",Key-value parameters of the MLflow run,"{""n_layers"": ""5"", ""batch_size"": ""64"", ""optimizer"": ""Adam""}",No
tags,"map<string, string>",Key-value tags set on the MLflow run,"{""ready_for_review"": ""true""}",No
aggregated_metrics,"list<struct<string, double, double, double>>",An aggregated view summarizing the metrics in the run_metrics_history,"[{""metric_name"": ""training_accuracy"", ""latest_value"": 0.97, ""min_value"": 0.8, ""max_value"": 1.0}, ...]",No
aggregated_metrics.metric_name,string,The user-specified name of the metric,"""training_accuracy""",No
aggregated_metrics.latest_value,double,"The latest value of the metric_name in the time series of this (run, metric_name) combination in run_metrics_history",0.97,No
aggregated_metrics.max_value,double,"The maximum value of the metric_name in the time series of this (run, metric_name) combination in run_metrics_history. If any NaN value was recorded for a metric, the value will be NaN",1,No
aggregated_metrics.min_value,double,"The minimum value of the metric_name in the time series of this (run, metric_name) combination in run_metrics_history. If any NaN value was recorded for a metric, the value will be NaN",0.8,No
system.mlflow.run_metrics_history,,,,
Column name,Data type,Description,Example,Nullable
account_id,string,The ID of the account containing the MLflow run to which the metric was logged,"""bd59efba-4444-4444-443f-44444449203""",No
insert_time,timestamp,The system time when the metric was inserted,2024-06-27T00:58:57.000+00:00,No
record_id,string,A unique identifier of the metric to distinguish between identical values,"""Ae1mDT5gFMSUwb+UUTuXMQ==""",No
workspace_id,string,The ID of the workspace containing the MLflow run to which the metric was logged,"""6051921418418893""",No
experiment_id,string,The ID of the MLflow experiment containing the MLflow run to which the metric was logged,"""2667956459304720""",No
run_id,string,The ID of the MLflow run to which the metric was logged,"""7716d750d279487c95f64a75bff2ad56""",No
metric_name,string,The metric name,"""training_accuracy""",No
metric_time,timestamp,The user-specified time when the metric was computed,2024-06-27T00:55:54.1231+00:00,No
metric_step,bigint,"The step (for example, epoch) of model training or agent development at which the metric was logged",10,No
metric_value,double,The metric value,0.97,No
Data classification results table schema,,,,
Column name,Data type,Description,Example,
latest_detected_time,timestamp,As-of time when the column was most recently scanned.,2025-06-27 12:34,
first_detected_time,timestamp,Time when the column detection was first recorded.,2025-06-27 12:34,
catalog_id,string,ID of the catalog.,3f1a7d6e-9c59-...,
table_id,string,ID of the table.,3f1a7d6e-9c59-...,
catalog_name,string,Catalog name.,main_catalog,
schema_name,string,Schema name.,public,
table_name,string,Table name.,sales_data,
column_name,string,Column name.,customer_email,
data_type,string,Data type of the column. Complex types include full struct definitions.,"struct<name:string, age:int>",
class_tag,string,Tag for the detected entity or tag key and optional value.,class.us_ssn or pii: confidential,
samples,array<string>,Up to five sample values that matched the detection.,"[""a@b.com"", ...]",
confidence,string,Confidence of detection. Either HIGH or LOW,HIGH,
frequency,float,Estimation of the proportion of matching rows in the sample. Between 0 and 1.,0.87,
Data quality monitoring results table schema,,,,
system.data_quality_monitoring.table_results,,,,
Column name,Contents (for struct data type),Data type,Description,Example data
event_time,,timestamp,Time when the row was generated.,2025-06-27 12:00:00
catalog_name,,string,Name of the catalog. Used to identify the table.,main
schema_name,,string,Name of the schema. Used to identify the table.,default
table_name,,string,Name of the table. Used to identify the table.,events
catalog_id,,string,Stable ID for the catalog.,3f1a7d6e-9c59-4b76-8c32-8d4c74e289fe
schema_id,,string,Stable ID for the schema.,3f1a7d6e-9c59-4b76-8c32-8d4c74e289fe
table_id,,string,Stable ID for the table.,3f1a7d6e-9c59-4b76-8c32-8d4c74e289fe
status,,string,"Consolidated health status at the table level. ""Unhealthy"" if any check or group is unhealthy.","Healthy, Unhealthy, Unknown"
freshness,,struct,Freshness checks.,
,status,string,Overall freshness status.,Unhealthy
,commit_freshness,struct,Commit freshness check results.,
completeness,,struct,Completeness check results.,
,status,string,Status of completeness check.,Unhealthy
,total_row_count,struct,Total number of rows in the table over time.,
,daily_row_count,struct,Number of rows added each day.,
downstream_impact,,struct,Summary of downstream impact based on dependency graph.,
,impact_level,int,"Severity indicator (0 = none, 1 = low, 2 = medium, 3 = high, 4 = very high).",2
,num_downstream_tables,int,Number of downstream tables affected.,5
,num_queries_on_affected_tables,int,Number of queries run on affected downstream tables over the last 30 days.,120
root_cause_analysis,,struct,Information about upstream jobs contributing to the issue.,
,upstream_jobs,array,Metadata for each upstream job.,
commit_freshness array structure,,,,
The commit_freshness struct contains the following:,,,,
,,,,
Item name,Data type,Description,Example data,
status,string,Status of commit freshness check.,Unhealthy,
error_code,string,Error message encountered during check.,FAILED_TO_FIT_MODEL,
last_value,timestamp,Last commit timestamp.,2025-06-27 11:30:00,
predicted_value,timestamp,Predicted time by which the table should have been updated.,2025-06-27 11:45:00,
total_row_count and daily_row_count array structure,,,,
The total_row_count and daily_row_count structs contain the following:,,,,
Item name,Data type,Description,Example data,
status,string,Status of the check.,Unhealthy,
error_code,string,Error message encountered during check.,FAILED_TO_FIT_MODEL,
last_value,int,Number of rows observed in the last 24 hours.,500,
min_predicted_value,int,Minimum expected number of rows in the last 24 hours.,10,
max_predicted_value,int,Maximum expected number of rows in the last 24 hours.,1000,
upstream_jobs array structure,,,,
The structure of the array shown in the upstream_jobs column is shown in the following table:,,,,
Item name,Data type,Description,Example data,
job_id,string,Job ID.,12345,
workspace_id,string,Workspace ID.,6051921418418893,
job_name,string,Job display name.,daily_refresh,
last_run_status,string,Status of the most recent run.,SUCCESS,
run_page_url,string,URL of Databricks job run page.,https://.../runs/123,
Downstream impact information,,,,
"In the logged results table, the column downstream_impact is a struct with the following fields:",,,,
Field,Type,Description,,
impact_level,int,Integer value between 1 and 4 indicating the severity of the data quality issue. Higher values indicate greater disruption.,,
num_downstream_tables,int,Number of downstream tables that might be affected by the identified issue.,,
num_queries_on_affected_tables,int,Total number of queries that have referenced the affected and downstream tables in the past 30 days.,,