# Databricks Health Monitor

Medallion architecture pipeline for Databricks system tables ingestion into Unity Catalog for health monitoring and observability.

## Overview

This project implements a **Bronze â†’ Gold** direct transformation architecture for Databricks system tables, eliminating the Silver layer for simplified observability:

- **ğŸ¥‰ Bronze Layer** (Complete): Raw ingestion of **35 system tables**
  - **8 DLT Streaming Pipelines** for streaming-capable tables (27 tables)
  - **MERGE operations** for non-streaming tables (8 tables)
  - **Orchestrated execution** via single coordinated workflow

- **ğŸ¥‡ Gold Layer** (Planned - 5 Phases): Star schema dimensional model with analytics and ML
  - **Phase 1**: Core infrastructure (catalog, permissions, alert config)
  - **Phase 2**: Gold dimensional model (~38-40 tables from 35 Bronze)
  - **Phase 3**: Analytics & ML layer (20+ metric views, 7 ML models)
  - **Phase 4**: Mosaic AI Agent framework (5 specialized agents, 40+ tools)
  - **Phase 5**: Alert framework & frontend (Databricks App)

> ğŸ“š **See [Implementation Plans](.cursor/plans/)** for actionable phase-by-phase plans  
> ğŸ“ **See [Architecture Documentation](docs/architecture/)** for comprehensive design specifications

### Architecture

```
Databricks System Tables (system.*)
        â†“
    ğŸ¥‰ Bronze Layer (system_bronze schema) âœ… Complete
        â”œâ”€â”€ Streaming Ingestion (8 DLT Pipelines)
        â”‚   â”œâ”€â”€ access (6 tables)
        â”‚   â”œâ”€â”€ billing (2 tables)
        â”‚   â”œâ”€â”€ compute (3 tables)
        â”‚   â”œâ”€â”€ lakeflow (2 tables)
        â”‚   â”œâ”€â”€ marketplace (1 table)
        â”‚   â”œâ”€â”€ mlflow (2 tables)
        â”‚   â”œâ”€â”€ serving (2 tables)
        â”‚   â””â”€â”€ sharing (3 tables)
        â””â”€â”€ Non-Streaming Ingestion (MERGE operations)
            â””â”€â”€ 8 tables (assistant_events, workspaces_latest, etc.)
        â†“
    ğŸ¥‡ Gold Layer (observability.gold schema) ğŸš§ Planned (5 Phases)
        â”œâ”€â”€ Phase 1: Core Infrastructure
        â”‚   â”œâ”€â”€ Unity Catalog setup
        â”‚   â”œâ”€â”€ Permissions model
        â”‚   â””â”€â”€ Alert config table
        â”œâ”€â”€ Phase 2: Dimensional Model (~38-40 tables)
        â”‚   â”œâ”€â”€ 23 dimensions (SCD Type 2)
        â”‚   â””â”€â”€ 12-15 facts (transaction + aggregate)
        â”œâ”€â”€ Phase 3: Analytics & ML
        â”‚   â”œâ”€â”€ 20+ metric views
        â”‚   â””â”€â”€ 7 ML models (UC model registry)
        â”œâ”€â”€ Phase 4: AI Agents
        â”‚   â”œâ”€â”€ Router + 5 specialized agents
        â”‚   â””â”€â”€ 40+ tools (SQL, ML, Action)
        â””â”€â”€ Phase 5: Alerts & Frontend
            â”œâ”€â”€ Lakehouse Monitoring
            â”œâ”€â”€ SQL Alerts (metadata-driven)
            â””â”€â”€ Databricks App (React, 7 pages)
```

## System Tables Ingested

### Streaming Tables (27 tables)

**Access Schema (6 tables):**
- `audit` - All audit events from workspaces
- `clean_room_events` - Clean room lifecycle events
- `column_lineage` - Column-level data lineage
- `inbound_network` - Inbound access denial events
- `outbound_network` - Outbound access denial events
- `table_lineage` - Table-level data lineage

**Billing Schema (1 table):**
- `usage` - Billable usage across account

**Compute Schema (4 tables):**
- `clusters` - Cluster configuration history
- `node_timeline` - Node utilization metrics
- `warehouse_events` - SQL warehouse lifecycle events
- `warehouses` - SQL warehouse configuration history

**Lakeflow Schema (6 tables):**
- `job_run_timeline` - Job run execution timeline
- `job_task_run_timeline` - Task-level execution timeline
- `job_tasks` - Job task definitions
- `jobs` - Job metadata
- `pipeline_update_timeline` - DLT pipeline update timeline
- `pipelines` - DLT pipeline metadata

**Marketplace Schema (2 tables):**
- `listing_funnel_events` - Marketplace listing funnel analytics
- `listing_access_events` - Consumer data access events

**MLflow Schema (3 tables):**
- `experiments_latest` - MLflow experiment metadata
- `runs_latest` - MLflow run metadata
- `run_metrics_history` - MLflow metrics timeseries

**Serving Schema (2 tables):**
- `served_entities` - Model serving endpoint metadata
- `endpoint_usage` - Model serving usage metrics

**Sharing Schema (1 table):**
- `materialization_history` - Delta Sharing materialization events

### Non-Streaming Tables (8 tables)

- `assistant_events` - Databricks Assistant usage
- `workspaces_latest` - Workspace metadata (SCD Type 1)
- `list_prices` - SKU pricing history
- `node_types` - Available node types
- `predictive_optimization_operations_history` - Predictive optimization operations
- `data_classification_results` - Data classification detections
- `data_quality_monitoring_table_results` - Data quality monitoring results
- `query_history` - SQL query execution history

## Project Structure

Organized by **medallion architecture layers** for clear separation:

```
DatabricksHealthMonitor/
â”œâ”€â”€ databricks.yml                   # Main bundle configuration
â”‚
â”œâ”€â”€ src/                             # Source code organized by layer
â”‚   â”œâ”€â”€ bronze_streaming/            # ğŸ¥‰ Bronze Layer
â”‚   â”‚   â”œâ”€â”€ system_access_streaming/
â”‚   â”‚   â”œâ”€â”€ system_billing_streaming/
â”‚   â”‚   â”œâ”€â”€ system_compute_streaming/
â”‚   â”‚   â”œâ”€â”€ system_lakeflow_streaming/
â”‚   â”‚   â”œâ”€â”€ system_marketplace_streaming/
â”‚   â”‚   â”œâ”€â”€ system_mlflow_streaming/
â”‚   â”‚   â”œâ”€â”€ system_serving_streaming/
â”‚   â”‚   â”œâ”€â”€ system_sharing_streaming/
â”‚   â”‚   â”œâ”€â”€ system_nonstreaming_setup/
â”‚   â”‚   â””â”€â”€ system_nonstreaming_merge/
â”‚   â”œâ”€â”€ silver_transform/            # ğŸ¥ˆ Silver Layer (reserved)
â”‚   â””â”€â”€ gold_aggregates/             # ğŸ¥‡ Gold Layer (reserved)
â”‚
â”œâ”€â”€ resources/                       # Asset Bundle resources by layer
â”‚   â”œâ”€â”€ schemas.yml                 # UC schema definitions (all layers)
â”‚   â”œâ”€â”€ system_tables_orchestrator.yml  # Master orchestrator
â”‚   â”œâ”€â”€ bronze/                     # ğŸ¥‰ Bronze resources
â”‚   â”‚   â”œâ”€â”€ *_streaming_pipeline.yml   # 8 DLT pipeline configs
â”‚   â”‚   â”œâ”€â”€ nonstreaming_setup_job.yml
â”‚   â”‚   â””â”€â”€ nonstreaming_merge_job.yml
â”‚   â”œâ”€â”€ silver/                     # ğŸ¥ˆ Silver resources (reserved)
â”‚   â””â”€â”€ gold/                       # ğŸ¥‡ Gold resources (reserved)
â”‚
â”œâ”€â”€ .cursor/plans/                 # Cursor buildable implementation plans
â”‚   â”œâ”€â”€ README.md                  # Plans overview & navigation
â”‚   â”œâ”€â”€ phase1_core_setup.md       # Phase 1: Core infrastructure
â”‚   â”œâ”€â”€ phase2_gold_layer.md       # Phase 2: Dimensional model
â”‚   â”œâ”€â”€ phase3_analytics_ml.md     # Phase 3: Analytics & ML
â”‚   â”œâ”€â”€ phase4_agent_framework.md  # Phase 4: AI agents
â”‚   â””â”€â”€ phase5_alerts_frontend.md  # Phase 5: Alerts & frontend
â”‚
â”œâ”€â”€ docs/                          # Comprehensive design documentation
â”‚   â”œâ”€â”€ architecture/              # Detailed architecture specs
â”‚   â”‚   â”œâ”€â”€ README.md              # Architecture docs index
â”‚   â”‚   â”œâ”€â”€ phase1_core_setup_design.md       # (767 lines)
â”‚   â”‚   â”œâ”€â”€ phase2_gold_layer_design.md       # (1,988 lines)
â”‚   â”‚   â”œâ”€â”€ phase3_analytics_ml_design.md     # (542 lines)
â”‚   â”‚   â”œâ”€â”€ phase4_agent_layer_design.md      # (206 lines)
â”‚   â”‚   â”œâ”€â”€ phase5_alert_framework_design.md  # (1,218 lines)
â”‚   â”‚   â”œâ”€â”€ agents/                # Agent-specific designs
â”‚   â”‚   â”‚   â”œâ”€â”€ cost_agent_design.md
â”‚   â”‚   â”‚   â”œâ”€â”€ security_agent_design.md
â”‚   â”‚   â”‚   â”œâ”€â”€ performance_agent_design.md
â”‚   â”‚   â”‚   â””â”€â”€ reliability_agent_design.md
â”‚   â”‚   â””â”€â”€ frontend/              # Frontend design
â”‚   â”‚       â””â”€â”€ app_design.md
â”‚   â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ operations/
â”‚   â””â”€â”€ reference/
â”‚
â””â”€â”€ context/                       # Reference materials
    â”œâ”€â”€ dashboards/               # Pre-built Lakeview dashboards
    â”œâ”€â”€ prompts/                  # AI agent context
    â””â”€â”€ systemtables/             # System table schemas
```

## Prerequisites

- Unity Catalog enabled workspace
- Account admin and metastore admin privileges (for grant access)
- Databricks CLI installed and configured
- Access to system tables granted via Unity Catalog

## Configuration

### 1. Update `databricks.yml`

Edit catalog and warehouse_id variables:

```yaml
variables:
  catalog:
    default: main  # Change to your catalog
  warehouse_id:
    default: ""  # Add your SQL warehouse ID
```

### 2. Update Email Notifications

Edit all YAML files in `resources/` to replace `data-engineering@company.com` with your team's email.

## Deployment

### Initial Deployment (Dev)

```bash
# Validate bundle configuration
databricks bundle validate

# Deploy to dev environment
databricks bundle deploy -t dev

# Run orchestrator manually (first time)
databricks bundle run -t dev system_tables_orchestrator
```

### Production Deployment

```bash
# Deploy to production
databricks bundle deploy -t prod

# Run orchestrator manually (first time)
databricks bundle run -t prod system_tables_orchestrator

# Enable daily schedule in Databricks UI
# Or update resources/system_tables_orchestrator.yml to set pause_status: UNPAUSED
```

## Orchestrator Workflow

The `system_tables_orchestrator` executes in this sequence:

1. **Setup Phase** (1 task)
   - Create/verify 8 non-streaming table structures

2. **Streaming Phase** (8 tasks - parallel execution)
   - Run all 8 DLT pipelines simultaneously
   - Ingest 27 streaming tables

3. **MERGE Phase** (1 task)
   - Sync 8 non-streaming tables via MERGE operations

**Schedule:** Daily at 2 AM UTC (paused by default in dev)

## Individual Job Execution

For testing or manual runs of specific components:

```bash
# Setup non-streaming tables
databricks bundle run -t dev nonstreaming_setup_job

# Run specific DLT pipeline
databricks pipelines start-update --pipeline-name "[dev] System Access Streaming Pipeline"

# Run MERGE for non-streaming tables
databricks bundle run -t dev nonstreaming_merge_job
```

## Monitoring

### Check Orchestrator Status

```sql
-- Job run history
SELECT * FROM system.lakeflow.job_run_timeline
WHERE job_id = '<orchestrator_job_id>'
ORDER BY period_start_time DESC;

-- Task-level details
SELECT * FROM system.lakeflow.job_task_run_timeline
WHERE job_id = '<orchestrator_job_id>'
ORDER BY period_start_time DESC;
```

### Verify Table Ingestion

```sql
-- Check row counts
SELECT 
    'audit' as table_name, COUNT(*) as row_count 
FROM main.system_bronze.audit
UNION ALL
SELECT 
    'usage' as table_name, COUNT(*) as row_count 
FROM main.system_bronze.usage
-- ... repeat for other tables
;

-- Check latest ingestion timestamp
SELECT 
    'audit' as table_name, 
    MAX(bronze_ingestion_timestamp) as latest_ingestion
FROM main.system_bronze.audit
GROUP BY 1
;
```

## Key Features

- âœ… **Serverless-first**: All compute uses serverless for automatic scaling
- âœ… **Schema evolution**: Streaming tables handle schema changes automatically
- âœ… **DLT Direct Publishing Mode**: Modern Unity Catalog integration
- âœ… **Automatic liquid clustering**: All tables use `CLUSTER BY AUTO`
- âœ… **Comprehensive metadata**: All tables have proper governance tags
- âœ… **Error handling**: MERGE jobs continue on failure and report errors
- âœ… **Orchestrated execution**: Single workflow coordinates all ingestion
- âœ… **Daily schedule**: Automated nightly refresh (configurable)

## Table Properties

All tables include standardized properties:
- `layer: bronze`
- `source_system: databricks_system_tables`
- `domain: <access|billing|compute|etc>`
- `entity_type: <fact|dimension>`
- `contains_pii: <true|false>`
- `data_classification: <confidential|internal>`
- `business_owner: Platform Operations`
- `technical_owner: Data Engineering`

## Troubleshooting

### Issue: Schema doesn't exist

```bash
# Manually create schema
databricks bundle deploy -t dev
# This creates the schema automatically via schemas.yml
```

### Issue: DLT pipeline fails with "skipChangeCommits" error

This is expected for system tables. The pattern is already implemented correctly in all DLT notebooks.

### Issue: MERGE fails for specific table

Check the merge_tables.py output for specific error. Common issues:
- Natural key column name mismatch
- Source table empty or unavailable
- Permissions issue

### Issue: Orchestrator timeout

Default timeout is 4 hours. For initial full refresh, this may need adjustment in `resources/system_tables_orchestrator.yml`:

```yaml
timeout_seconds: 21600  # 6 hours
```

## Cleanup

To remove all resources:

```bash
# WARNING: This deletes all tables and pipelines
databricks bundle destroy -t dev
```

To keep data but remove jobs/pipelines:

```bash
# Manually delete jobs/pipelines via UI
# Tables remain in Unity Catalog
```

## Implementation Roadmap

### Current Status: Phase 0 Complete âœ…

Bronze layer ingestion is operational. Ready to proceed with Gold layer implementation.

### Next Steps: 5-Phase Implementation

Each phase has a **concise actionable plan** in `.cursor/plans/` and **comprehensive design documentation** in `docs/architecture/`:

| Phase | Focus | Duration | Plan | Design Doc |
|-------|-------|----------|------|------------|
| Phase 1 | Core Infrastructure | Weeks 1-4 | [phase1_core_setup.md](.cursor/plans/phase1_core_setup.md) | [phase1_core_setup_design.md](docs/architecture/phase1_core_setup_design.md) |
| Phase 2 | Gold Dimensional Model | Weeks 5-12 | [phase2_gold_layer.md](.cursor/plans/phase2_gold_layer.md) | [phase2_gold_layer_design.md](docs/architecture/phase2_gold_layer_design.md) |
| Phase 3 | Analytics & ML | Weeks 9-12 | [phase3_analytics_ml.md](.cursor/plans/phase3_analytics_ml.md) | [phase3_analytics_ml_design.md](docs/architecture/phase3_analytics_ml_design.md) |
| Phase 4 | AI Agent Framework | Weeks 13-16 | [phase4_agent_framework.md](.cursor/plans/phase4_agent_framework.md) | [phase4_agent_layer_design.md](docs/architecture/phase4_agent_layer_design.md) |
| Phase 5 | Alerts & Frontend | Weeks 13-16 | [phase5_alerts_frontend.md](.cursor/plans/phase5_alerts_frontend.md) | [phase5_alert_framework_design.md](docs/architecture/phase5_alert_framework_design.md) |

**How to Use:**

1. **Review Plan:** Start with the concise plan (e.g., `phase1_core_setup.md`)
2. **Deep Dive:** Reference the comprehensive design doc for full specifications
3. **Build:** Use Cursor's "Build" option on the plan file
4. **Validate:** Complete the validation checklist in each plan
5. **Iterate:** Move to next phase upon completion

**Documentation Structure:**

- **`.cursor/plans/`** - Concise, actionable implementation steps (<200 lines each)
- **`docs/architecture/`** - Comprehensive design specs (700-2,000 lines each)

## References

- [System Tables Documentation](https://docs.databricks.com/aws/en/admin/system-tables/)
- [Streaming System Tables](https://docs.databricks.com/aws/en/admin/system-tables/#considerations-for-streaming-system-tables)
- [DLT Direct Publishing Mode](https://docs.databricks.com/aws/en/delta-live-tables/unity-catalog.html)
- [Asset Bundles](https://docs.databricks.com/aws/en/dev-tools/bundles/)

## Support

For questions or issues:
1. Check Databricks system tables documentation
2. Review DLT pipeline event logs in Databricks UI
3. Check orchestrator job run history
4. Contact: data-engineering@company.com

## License

Internal use only - Platform Operations team.

